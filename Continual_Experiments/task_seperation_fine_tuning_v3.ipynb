{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duygu/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"\")))\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "\n",
    "from dataloaders.dataloader_cifar10 import get_cifar10\n",
    "from dataloaders.dataloader_cifar100 import get_cifar100\n",
    "from utils.eval_metrics import linear_evaluation, get_t_SNE_plot\n",
    "from models.linear_classifer import LinearClassifier\n",
    "from models.ssl import  SimSiam, Siamese, Encoder, Predictor\n",
    "\n",
    "from trainers.train_simsiam import train_simsiam\n",
    "from trainers.train_infomax import train_infomax\n",
    "from trainers.train_barlow import train_barlow\n",
    "\n",
    "from trainers.train_PFR import train_PFR_simsiam\n",
    "from trainers.train_PFR_contrastive import train_PFR_contrastive_simsiam\n",
    "from trainers.train_contrastive import train_contrastive_simsiam\n",
    "from trainers.train_ering import train_ering_simsiam\n",
    "\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from utils.lr_schedulers import LinearWarmupCosineAnnealingLR, SimSiamScheduler\n",
    "from utils.eval_metrics import Knn_Validation_cont\n",
    "from copy import deepcopy\n",
    "from loss import invariance_loss,CovarianceLoss,ErrorCovarianceLoss\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from models.linear_classifer import LinearClassifier\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloaders.dataset import TensorDataset\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[0.1, 2.0]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = torchvision.transforms.functional.gaussian_blur(x,kernel_size=[3,3],sigma=sigma)#kernel size and sigma are open problems but right now seems ok!\n",
    "        return x\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    normalization = 'batch'\n",
    "    weight_standard = False\n",
    "    same_lr = False\n",
    "    pretrain_batch_size = 512\n",
    "    pretrain_warmup_epochs = 10\n",
    "    pretrain_warmup_lr = 3e-3\n",
    "    pretrain_base_lr = 0.03\n",
    "    pretrain_momentum = 0.9\n",
    "    pretrain_weight_decay = 5e-4\n",
    "    min_lr = 0.00\n",
    "    lambdap = 1.0\n",
    "    appr = 'barlow_PFR'\n",
    "    knn_report_freq = 10\n",
    "    cuda_device = 7\n",
    "    num_workers = 8\n",
    "    contrastive_ratio = 0.001\n",
    "    dataset = 'cifar100'\n",
    "    class_split = [20,20,20,20,20]\n",
    "    epochs = [500,500,500,500,500]\n",
    "    cov_loss_weight = 1.0\n",
    "    sim_loss_weight = 250.0\n",
    "    info_loss = 'invariance'\n",
    "    lambda_norm = 1.0\n",
    "    subspace_rate = 0.99\n",
    "    lambda_param = 5e-3\n",
    "    bsize = 32\n",
    "    msize = 150\n",
    "    proj_hidden = 2048\n",
    "    proj_out = 2048 #infomax 64\n",
    "    pred_hidden = 512\n",
    "    pred_out = 2048\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "    get_dataloaders = get_cifar10\n",
    "    num_classes=10\n",
    "elif args.dataset == \"cifar100\":\n",
    "    get_dataloaders = get_cifar100\n",
    "    num_classes=100\n",
    "assert sum(args.class_split) == num_classes\n",
    "assert len(args.class_split) == len(args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:7\n"
     ]
    }
   ],
   "source": [
    "num_worker = args.num_workers\n",
    "#device\n",
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb init\n",
    "wandb.init(project=\"CSSL\",  entity=\"yavuz-team\",\n",
    "            mode=\"disabled\",\n",
    "            config=args,\n",
    "            name= str(args.dataset) + '-algo' + str(args.appr) + \"-e\" + str(args.epochs) + \"-b\" \n",
    "            + str(args.pretrain_batch_size) + \"-lr\" + str(args.pretrain_base_lr)+\"-CS\"+str(args.class_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    transform = T.Compose([\n",
    "            T.RandomResizedCrop(size=32, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)]), p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([GaussianBlur()], p=0.5), \n",
    "            T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])])\n",
    "\n",
    "    transform_prime = T.Compose([\n",
    "            T.RandomResizedCrop(size=32, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)]), p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([GaussianBlur()], p=0.5), \n",
    "            T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataloaders..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Dataloaders\n",
    "print(\"Creating Dataloaders..\")\n",
    "#Class Based\n",
    "train_data_loaders, train_data_loaders_knn, test_data_loaders, _, train_data_loaders_linear, train_data_loaders_pure, train_data_loaders_generic  = get_dataloaders(transform, transform_prime, \\\n",
    "                                    classes=args.class_split, valid_rate = 0.00, batch_size=args.pretrain_batch_size, seed = 0, num_worker= num_worker)\n",
    "_, train_data_loaders_knn_all, test_data_loaders_all, _, train_data_loaders_linear_all, train_data_loaders_pure_all, _ = get_dataloaders(transform, transform_prime, \\\n",
    "                                        classes=[num_classes], valid_rate = 0.00, batch_size=args.pretrain_batch_size, seed = 0, num_worker= num_worker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_top_k(outputs, targets, top_k=(1,5)):\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argsort(outputs, dim=-1, descending=True)\n",
    "        result= []\n",
    "        for k in top_k:\n",
    "            correct_k = torch.sum((prediction[:, 0:k] == targets.unsqueeze(dim=-1)).any(dim=-1).float()).item() \n",
    "            result.append(correct_k)\n",
    "        return result\n",
    "\n",
    "def linear_test(net, data_loader, classifier, epoch, device, task_num):\n",
    "    # evaluate model:\n",
    "    net.eval() # for not update batchnorm\n",
    "    linear_loss = 0.0\n",
    "    num = 0\n",
    "    total_loss, total_correct_1, total_num, test_bar = 0.0, 0.0, 0, tqdm(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for data_tuple in test_bar:\n",
    "            data, target = [t.to(device) for t in data_tuple]\n",
    "            output = net(data)\n",
    "            if classifier is not None:  #else net is already a classifier\n",
    "                output = classifier(output) \n",
    "            linear_loss = F.cross_entropy(output, target)\n",
    "            \n",
    "            # Batchsize for loss and accuracy\n",
    "            num = data.size(0)\n",
    "            total_num += num \n",
    "            total_loss += linear_loss.item() * num \n",
    "            # Accumulating number of correct predictions \n",
    "            correct_top_1 = correct_top_k(output, target, top_k=[1])    \n",
    "            total_correct_1 += correct_top_1[0]\n",
    "            test_bar.set_description('Lin.Test Epoch: [{}] Loss: {:.4f} ACC: {:.2f}% '\n",
    "                                     .format(epoch,  total_loss / total_num,\n",
    "                                             total_correct_1 / total_num * 100\n",
    "                                             ))\n",
    "        acc_1 = total_correct_1/total_num*100\n",
    "        wandb.log({f\" {task_num} Linear Layer Test Loss \": linear_loss / total_num, \"Linear Epoch \": epoch})\n",
    "        wandb.log({f\" {task_num} Linear Layer Test - Acc\": acc_1, \"Linear Epoch \": epoch})\n",
    "    return total_loss / total_num, acc_1  \n",
    "\n",
    "def linear_train(net, data_loader, train_optimizer, classifier, scheduler, epoch, device, task_num):\n",
    "\n",
    "    net.eval() # for not update batchnorm \n",
    "    total_num, train_bar = 0, tqdm(data_loader)\n",
    "    linear_loss = 0.0\n",
    "    total_correct_1 = 0.0\n",
    "    for data_tuple in train_bar:\n",
    "        # Forward prop of the model with single augmented batch\n",
    "        pos_1, target = data_tuple\n",
    "        pos_1 = pos_1.to(device)\n",
    "        feature_1 = net(pos_1)\n",
    "        # Batchsize\n",
    "        batchsize_bc = feature_1.shape[0]\n",
    "        features = feature_1\n",
    "        targets = target.to(device)\n",
    "        logits = classifier(features.detach()) \n",
    "        # Cross Entropy Loss \n",
    "        linear_loss_1 = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # Number of correct predictions\n",
    "        linear_correct_1 = correct_top_k(logits, targets, top_k=[1])\n",
    "    \n",
    "        # Backpropagation part\n",
    "        train_optimizer.zero_grad()\n",
    "        linear_loss_1.backward()\n",
    "        train_optimizer.step()\n",
    "\n",
    "        # Accumulating number of examples, losses and correct predictions\n",
    "        total_num += batchsize_bc\n",
    "        linear_loss += linear_loss_1.item() * batchsize_bc\n",
    "        total_correct_1 += linear_correct_1[0] \n",
    "\n",
    "        acc_1 = total_correct_1/total_num*100\n",
    "        # # This bar is used for live tracking on command line (batch_size -> batchsize_bc: to show current batchsize )\n",
    "        train_bar.set_description('Lin.Train Epoch: [{}] Loss: {:.4f} ACC: {:.2f}'.format(\\\n",
    "                epoch, linear_loss / total_num, acc_1))\n",
    "    scheduler.step()\n",
    "    acc_1 = total_correct_1/total_num*100   \n",
    "    wandb.log({f\" {task_num} Linear Layer Train Loss \": linear_loss / total_num, \"Linear Epoch \": epoch})\n",
    "    wandb.log({f\" {task_num} Linear Layer Train - Acc\": acc_1, \"Linear Epoch \": epoch})\n",
    "        \n",
    "    return linear_loss/total_num, acc_1\n",
    "\n",
    "\n",
    "def linear_evaluation(net, data_loaders,test_data_loaders,train_optimizer,classifier, scheduler, epochs, device, task_num):\n",
    "    train_X = torch.Tensor([])\n",
    "    train_Y = torch.tensor([],dtype=int)\n",
    "    for loader in data_loaders:\n",
    "        train_X = torch.cat((train_X, loader.dataset.train_data), dim=0)\n",
    "        train_Y = torch.cat((train_Y, loader.dataset.label_data), dim=0)\n",
    "    data_loader = DataLoader(TensorDataset(train_X, train_Y,transform=data_loaders[0].dataset.transform), batch_size=256, shuffle=True, num_workers = 5, pin_memory=True)\n",
    "\n",
    "    test_X = torch.Tensor([])\n",
    "    test_Y = torch.tensor([],dtype=int)\n",
    "    for loader in test_data_loaders:\n",
    "        test_X = torch.cat((test_X, loader.dataset.train_data), dim=0)\n",
    "        test_Y = torch.cat((test_Y, loader.dataset.label_data), dim=0)\n",
    "    test_data_loader = DataLoader(TensorDataset(test_X, test_Y,transform=test_data_loaders[0].dataset.transform), batch_size=256, shuffle=True, num_workers = 5, pin_memory=True)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        linear_train(net,data_loader,train_optimizer,classifier,scheduler, epoch, device, task_num)\n",
    "        with torch.no_grad():\n",
    "            # Testing for linear evaluation\n",
    "            test_loss, test_acc1 = linear_test(net, test_data_loader, classifier, epoch, device, task_num)\n",
    "\n",
    "    return test_loss, test_acc1, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    proj_hidden = args.proj_hidden\n",
    "    proj_out = args.proj_out\n",
    "    encoder = Encoder(hidden_dim=proj_hidden, output_dim=proj_out, normalization = args.normalization, weight_standard = args.weight_standard, appr_name = args.appr)\n",
    "    model2 = Siamese(encoder)\n",
    "    model2.to(device) #automatically detects from model\n",
    "#load model here\n",
    "file_name = \"./checkpoints/checkpoint_cifar100-algocassle_barlow-e[500, 500, 500, 500, 500]-b256-lr0.25-CS[20, 20, 20, 20, 20]_task_1_same_lr_True_norm_batch_ws_False.pth.tar\"\n",
    "dict = torch.load(file_name)\n",
    "model2.temporal_projector = nn.Sequential(\n",
    "            nn.Linear(args.proj_out, args.proj_hidden, bias=False),\n",
    "            nn.BatchNorm1d(args.proj_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.proj_hidden, args.proj_out),\n",
    "        ).to(device)\n",
    "model2.load_state_dict(dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin.Train Epoch: [1] Loss: 3.6393 ACC: 42.91: 100%|██████████| 79/79 [00:04<00:00, 16.35it/s]\n",
      "Lin.Test Epoch: [1] Loss: 3.0033 ACC: 51.20% : 100%|██████████| 16/16 [00:01<00:00, 11.42it/s]\n",
      "Lin.Train Epoch: [2] Loss: 3.4512 ACC: 49.25: 100%|██████████| 79/79 [00:03<00:00, 22.13it/s]\n",
      "Lin.Test Epoch: [2] Loss: 3.2797 ACC: 54.17% : 100%|██████████| 16/16 [00:01<00:00, 12.11it/s]\n",
      "Lin.Train Epoch: [3] Loss: 3.3490 ACC: 50.17: 100%|██████████| 79/79 [00:03<00:00, 22.40it/s]\n",
      "Lin.Test Epoch: [3] Loss: 3.3353 ACC: 50.58% : 100%|██████████| 16/16 [00:01<00:00, 11.91it/s]\n",
      "Lin.Train Epoch: [4] Loss: 3.2450 ACC: 51.43: 100%|██████████| 79/79 [00:03<00:00, 22.26it/s]\n",
      "Lin.Test Epoch: [4] Loss: 2.8813 ACC: 53.23% : 100%|██████████| 16/16 [00:01<00:00, 11.77it/s]\n",
      "Lin.Train Epoch: [5] Loss: 3.0826 ACC: 51.90: 100%|██████████| 79/79 [00:03<00:00, 21.74it/s]\n",
      "Lin.Test Epoch: [5] Loss: 2.7467 ACC: 54.12% : 100%|██████████| 16/16 [00:01<00:00, 11.94it/s]\n",
      "Lin.Train Epoch: [6] Loss: 3.0418 ACC: 52.61: 100%|██████████| 79/79 [00:03<00:00, 22.09it/s]\n",
      "Lin.Test Epoch: [6] Loss: 3.0523 ACC: 52.80% : 100%|██████████| 16/16 [00:01<00:00, 11.58it/s]\n",
      "Lin.Train Epoch: [7] Loss: 3.4515 ACC: 51.20: 100%|██████████| 79/79 [00:03<00:00, 22.02it/s]\n",
      "Lin.Test Epoch: [7] Loss: 2.4862 ACC: 58.98% : 100%|██████████| 16/16 [00:01<00:00, 11.80it/s]\n",
      "Lin.Train Epoch: [8] Loss: 2.9858 ACC: 53.67: 100%|██████████| 79/79 [00:03<00:00, 21.73it/s]\n",
      "Lin.Test Epoch: [8] Loss: 3.3996 ACC: 52.30% : 100%|██████████| 16/16 [00:01<00:00, 11.78it/s]\n",
      "Lin.Train Epoch: [9] Loss: 3.1409 ACC: 53.59: 100%|██████████| 79/79 [00:03<00:00, 22.23it/s]\n",
      "Lin.Test Epoch: [9] Loss: 2.4932 ACC: 58.60% : 100%|██████████| 16/16 [00:01<00:00, 11.41it/s]\n",
      "Lin.Train Epoch: [10] Loss: 3.0081 ACC: 54.30: 100%|██████████| 79/79 [00:03<00:00, 22.51it/s]\n",
      "Lin.Test Epoch: [10] Loss: 3.2332 ACC: 53.42% : 100%|██████████| 16/16 [00:01<00:00, 12.85it/s]\n",
      "Lin.Train Epoch: [11] Loss: 3.0016 ACC: 54.06: 100%|██████████| 79/79 [00:03<00:00, 21.87it/s]\n",
      "Lin.Test Epoch: [11] Loss: 2.7357 ACC: 57.12% : 100%|██████████| 16/16 [00:01<00:00, 11.50it/s]\n",
      "Lin.Train Epoch: [12] Loss: 3.0180 ACC: 53.80: 100%|██████████| 79/79 [00:03<00:00, 21.86it/s]\n",
      "Lin.Test Epoch: [12] Loss: 2.9276 ACC: 55.70% : 100%|██████████| 16/16 [00:01<00:00, 11.81it/s]\n",
      "Lin.Train Epoch: [13] Loss: 3.1050 ACC: 54.44: 100%|██████████| 79/79 [00:03<00:00, 22.28it/s]\n",
      "Lin.Test Epoch: [13] Loss: 2.9200 ACC: 54.73% : 100%|██████████| 16/16 [00:01<00:00, 11.63it/s]\n",
      "Lin.Train Epoch: [14] Loss: 3.0342 ACC: 54.21: 100%|██████████| 79/79 [00:03<00:00, 22.39it/s]\n",
      "Lin.Test Epoch: [14] Loss: 3.0903 ACC: 53.12% : 100%|██████████| 16/16 [00:01<00:00, 12.31it/s]\n",
      "Lin.Train Epoch: [15] Loss: 2.9051 ACC: 55.03: 100%|██████████| 79/79 [00:03<00:00, 22.18it/s]\n",
      "Lin.Test Epoch: [15] Loss: 2.7277 ACC: 56.30% : 100%|██████████| 16/16 [00:01<00:00, 11.88it/s]\n",
      "Lin.Train Epoch: [16] Loss: 2.8280 ACC: 55.14: 100%|██████████| 79/79 [00:03<00:00, 22.32it/s]\n",
      "Lin.Test Epoch: [16] Loss: 2.6361 ACC: 56.27% : 100%|██████████| 16/16 [00:01<00:00, 11.67it/s]\n",
      "Lin.Train Epoch: [17] Loss: 2.9575 ACC: 54.62: 100%|██████████| 79/79 [00:03<00:00, 22.66it/s]\n",
      "Lin.Test Epoch: [17] Loss: 3.0713 ACC: 55.00% : 100%|██████████| 16/16 [00:01<00:00, 12.06it/s]\n",
      "Lin.Train Epoch: [18] Loss: 3.0301 ACC: 55.03: 100%|██████████| 79/79 [00:03<00:00, 22.27it/s]\n",
      "Lin.Test Epoch: [18] Loss: 2.4443 ACC: 58.90% : 100%|██████████| 16/16 [00:01<00:00, 11.98it/s]\n",
      "Lin.Train Epoch: [19] Loss: 2.8083 ACC: 55.61: 100%|██████████| 79/79 [00:03<00:00, 22.80it/s]\n",
      "Lin.Test Epoch: [19] Loss: 2.8268 ACC: 54.87% : 100%|██████████| 16/16 [00:01<00:00, 11.93it/s]\n",
      "Lin.Train Epoch: [20] Loss: 2.9507 ACC: 54.43: 100%|██████████| 79/79 [00:03<00:00, 22.06it/s]\n",
      "Lin.Test Epoch: [20] Loss: 2.9219 ACC: 55.55% : 100%|██████████| 16/16 [00:01<00:00, 11.76it/s]\n",
      "Lin.Train Epoch: [21] Loss: 2.8677 ACC: 55.06: 100%|██████████| 79/79 [00:03<00:00, 21.86it/s]\n",
      "Lin.Test Epoch: [21] Loss: 2.4757 ACC: 58.93% : 100%|██████████| 16/16 [00:01<00:00, 11.89it/s]\n",
      "Lin.Train Epoch: [22] Loss: 2.9122 ACC: 54.93: 100%|██████████| 79/79 [00:03<00:00, 22.53it/s]\n",
      "Lin.Test Epoch: [22] Loss: 2.6178 ACC: 57.85% : 100%|██████████| 16/16 [00:01<00:00, 11.70it/s]\n",
      "Lin.Train Epoch: [23] Loss: 2.7833 ACC: 55.26: 100%|██████████| 79/79 [00:03<00:00, 22.33it/s]\n",
      "Lin.Test Epoch: [23] Loss: 2.7204 ACC: 56.38% : 100%|██████████| 16/16 [00:01<00:00, 12.30it/s]\n",
      "Lin.Train Epoch: [24] Loss: 2.8335 ACC: 55.12: 100%|██████████| 79/79 [00:03<00:00, 22.63it/s]\n",
      "Lin.Test Epoch: [24] Loss: 2.9296 ACC: 54.55% : 100%|██████████| 16/16 [00:01<00:00, 12.80it/s]\n",
      "Lin.Train Epoch: [25] Loss: 2.8388 ACC: 55.05: 100%|██████████| 79/79 [00:03<00:00, 22.12it/s]\n",
      "Lin.Test Epoch: [25] Loss: 2.7548 ACC: 55.80% : 100%|██████████| 16/16 [00:01<00:00, 12.60it/s]\n",
      "Lin.Train Epoch: [26] Loss: 2.7741 ACC: 55.54: 100%|██████████| 79/79 [00:03<00:00, 22.28it/s]\n",
      "Lin.Test Epoch: [26] Loss: 2.3770 ACC: 58.60% : 100%|██████████| 16/16 [00:01<00:00, 11.44it/s]\n",
      "Lin.Train Epoch: [27] Loss: 2.6437 ACC: 55.84: 100%|██████████| 79/79 [00:03<00:00, 22.43it/s]\n",
      "Lin.Test Epoch: [27] Loss: 2.6751 ACC: 55.27% : 100%|██████████| 16/16 [00:01<00:00, 11.90it/s]\n",
      "Lin.Train Epoch: [28] Loss: 2.6397 ACC: 55.76: 100%|██████████| 79/79 [00:03<00:00, 22.33it/s]\n",
      "Lin.Test Epoch: [28] Loss: 2.4267 ACC: 57.67% : 100%|██████████| 16/16 [00:01<00:00, 11.74it/s]\n",
      "Lin.Train Epoch: [29] Loss: 2.5989 ACC: 55.58: 100%|██████████| 79/79 [00:03<00:00, 22.39it/s]\n",
      "Lin.Test Epoch: [29] Loss: 2.1895 ACC: 59.17% : 100%|██████████| 16/16 [00:01<00:00, 12.17it/s]\n",
      "Lin.Train Epoch: [30] Loss: 2.5288 ACC: 56.18: 100%|██████████| 79/79 [00:03<00:00, 22.46it/s]\n",
      "Lin.Test Epoch: [30] Loss: 2.7000 ACC: 56.70% : 100%|██████████| 16/16 [00:01<00:00, 12.10it/s]\n",
      "Lin.Train Epoch: [31] Loss: 2.4123 ACC: 56.53: 100%|██████████| 79/79 [00:03<00:00, 22.27it/s]\n",
      "Lin.Test Epoch: [31] Loss: 2.2211 ACC: 61.27% : 100%|██████████| 16/16 [00:01<00:00, 12.14it/s]\n",
      "Lin.Train Epoch: [32] Loss: 2.3693 ACC: 56.92: 100%|██████████| 79/79 [00:03<00:00, 22.15it/s]\n",
      "Lin.Test Epoch: [32] Loss: 2.3959 ACC: 55.40% : 100%|██████████| 16/16 [00:01<00:00, 11.82it/s]\n",
      "Lin.Train Epoch: [33] Loss: 2.4728 ACC: 56.21: 100%|██████████| 79/79 [00:03<00:00, 21.91it/s]\n",
      "Lin.Test Epoch: [33] Loss: 2.8117 ACC: 58.17% : 100%|██████████| 16/16 [00:01<00:00, 12.37it/s]\n",
      "Lin.Train Epoch: [34] Loss: 2.4569 ACC: 56.57: 100%|██████████| 79/79 [00:03<00:00, 22.25it/s]\n",
      "Lin.Test Epoch: [34] Loss: 2.1576 ACC: 59.08% : 100%|██████████| 16/16 [00:01<00:00, 11.87it/s]\n",
      "Lin.Train Epoch: [35] Loss: 2.4282 ACC: 56.62: 100%|██████████| 79/79 [00:03<00:00, 21.71it/s]\n",
      "Lin.Test Epoch: [35] Loss: 2.3727 ACC: 58.63% : 100%|██████████| 16/16 [00:01<00:00, 11.89it/s]\n",
      "Lin.Train Epoch: [36] Loss: 2.3503 ACC: 57.25: 100%|██████████| 79/79 [00:03<00:00, 22.07it/s]\n",
      "Lin.Test Epoch: [36] Loss: 2.3029 ACC: 58.55% : 100%|██████████| 16/16 [00:01<00:00, 11.79it/s]\n",
      "Lin.Train Epoch: [37] Loss: 2.3317 ACC: 57.38: 100%|██████████| 79/79 [00:03<00:00, 21.93it/s]\n",
      "Lin.Test Epoch: [37] Loss: 2.4073 ACC: 55.57% : 100%|██████████| 16/16 [00:01<00:00, 11.92it/s]\n",
      "Lin.Train Epoch: [38] Loss: 2.2944 ACC: 57.02: 100%|██████████| 79/79 [00:03<00:00, 22.24it/s]\n",
      "Lin.Test Epoch: [38] Loss: 2.4282 ACC: 55.57% : 100%|██████████| 16/16 [00:01<00:00, 11.99it/s]\n",
      "Lin.Train Epoch: [39] Loss: 2.3505 ACC: 56.86: 100%|██████████| 79/79 [00:03<00:00, 22.32it/s]\n",
      "Lin.Test Epoch: [39] Loss: 2.0548 ACC: 59.65% : 100%|██████████| 16/16 [00:01<00:00, 11.74it/s]\n",
      "Lin.Train Epoch: [40] Loss: 2.2690 ACC: 56.98: 100%|██████████| 79/79 [00:03<00:00, 22.52it/s]\n",
      "Lin.Test Epoch: [40] Loss: 2.1116 ACC: 59.42% : 100%|██████████| 16/16 [00:01<00:00, 11.59it/s]\n",
      "Lin.Train Epoch: [41] Loss: 2.1672 ACC: 57.84: 100%|██████████| 79/79 [00:03<00:00, 22.28it/s]\n",
      "Lin.Test Epoch: [41] Loss: 1.9654 ACC: 60.05% : 100%|██████████| 16/16 [00:01<00:00, 12.47it/s]\n",
      "Lin.Train Epoch: [42] Loss: 2.1682 ACC: 57.05: 100%|██████████| 79/79 [00:03<00:00, 22.07it/s]\n",
      "Lin.Test Epoch: [42] Loss: 1.9382 ACC: 60.65% : 100%|██████████| 16/16 [00:01<00:00, 12.76it/s]\n",
      "Lin.Train Epoch: [43] Loss: 2.0485 ACC: 58.24: 100%|██████████| 79/79 [00:03<00:00, 21.94it/s]\n",
      "Lin.Test Epoch: [43] Loss: 2.1934 ACC: 57.50% : 100%|██████████| 16/16 [00:01<00:00, 11.85it/s]\n",
      "Lin.Train Epoch: [44] Loss: 2.0619 ACC: 57.94: 100%|██████████| 79/79 [00:03<00:00, 21.91it/s]\n",
      "Lin.Test Epoch: [44] Loss: 2.0311 ACC: 59.88% : 100%|██████████| 16/16 [00:01<00:00, 11.16it/s]\n",
      "Lin.Train Epoch: [45] Loss: 2.1047 ACC: 57.02: 100%|██████████| 79/79 [00:03<00:00, 22.12it/s]\n",
      "Lin.Test Epoch: [45] Loss: 2.0498 ACC: 57.98% : 100%|██████████| 16/16 [00:01<00:00, 11.31it/s]\n",
      "Lin.Train Epoch: [46] Loss: 2.0758 ACC: 58.05: 100%|██████████| 79/79 [00:03<00:00, 22.18it/s]\n",
      "Lin.Test Epoch: [46] Loss: 1.9115 ACC: 60.20% : 100%|██████████| 16/16 [00:01<00:00, 11.70it/s]\n",
      "Lin.Train Epoch: [47] Loss: 1.9578 ACC: 58.51: 100%|██████████| 79/79 [00:03<00:00, 21.80it/s]\n",
      "Lin.Test Epoch: [47] Loss: 1.9552 ACC: 58.45% : 100%|██████████| 16/16 [00:01<00:00, 11.82it/s]\n",
      "Lin.Train Epoch: [48] Loss: 1.9152 ACC: 58.44: 100%|██████████| 79/79 [00:03<00:00, 22.22it/s]\n",
      "Lin.Test Epoch: [48] Loss: 1.9548 ACC: 59.03% : 100%|██████████| 16/16 [00:01<00:00, 11.68it/s]\n",
      "Lin.Train Epoch: [49] Loss: 1.9070 ACC: 58.33: 100%|██████████| 79/79 [00:03<00:00, 22.06it/s]\n",
      "Lin.Test Epoch: [49] Loss: 1.7642 ACC: 59.82% : 100%|██████████| 16/16 [00:01<00:00, 11.08it/s]\n",
      "Lin.Train Epoch: [50] Loss: 1.9526 ACC: 58.03: 100%|██████████| 79/79 [00:03<00:00, 22.31it/s]\n",
      "Lin.Test Epoch: [50] Loss: 1.7770 ACC: 60.85% : 100%|██████████| 16/16 [00:01<00:00, 12.22it/s]\n",
      "Lin.Train Epoch: [51] Loss: 1.8832 ACC: 58.14: 100%|██████████| 79/79 [00:03<00:00, 22.22it/s]\n",
      "Lin.Test Epoch: [51] Loss: 1.8021 ACC: 60.72% : 100%|██████████| 16/16 [00:01<00:00, 12.02it/s]\n",
      "Lin.Train Epoch: [52] Loss: 1.8503 ACC: 59.13: 100%|██████████| 79/79 [00:03<00:00, 22.33it/s]\n",
      "Lin.Test Epoch: [52] Loss: 1.7562 ACC: 59.85% : 100%|██████████| 16/16 [00:01<00:00, 11.91it/s]\n",
      "Lin.Train Epoch: [53] Loss: 1.7408 ACC: 60.02: 100%|██████████| 79/79 [00:03<00:00, 22.10it/s]\n",
      "Lin.Test Epoch: [53] Loss: 1.5623 ACC: 62.10% : 100%|██████████| 16/16 [00:01<00:00, 12.06it/s]\n",
      "Lin.Train Epoch: [54] Loss: 1.7158 ACC: 59.94: 100%|██████████| 79/79 [00:03<00:00, 21.95it/s]\n",
      "Lin.Test Epoch: [54] Loss: 1.7260 ACC: 58.53% : 100%|██████████| 16/16 [00:01<00:00, 12.03it/s]\n",
      "Lin.Train Epoch: [55] Loss: 1.7945 ACC: 59.05: 100%|██████████| 79/79 [00:03<00:00, 21.82it/s]\n",
      "Lin.Test Epoch: [55] Loss: 1.7050 ACC: 60.77% : 100%|██████████| 16/16 [00:01<00:00, 11.70it/s]\n",
      "Lin.Train Epoch: [56] Loss: 1.7996 ACC: 59.08: 100%|██████████| 79/79 [00:03<00:00, 22.69it/s]\n",
      "Lin.Test Epoch: [56] Loss: 1.4692 ACC: 64.20% : 100%|██████████| 16/16 [00:01<00:00, 11.64it/s]\n",
      "Lin.Train Epoch: [57] Loss: 1.6392 ACC: 60.84: 100%|██████████| 79/79 [00:03<00:00, 22.36it/s]\n",
      "Lin.Test Epoch: [57] Loss: 1.5491 ACC: 62.10% : 100%|██████████| 16/16 [00:01<00:00, 11.59it/s]\n",
      "Lin.Train Epoch: [58] Loss: 1.6078 ACC: 60.68: 100%|██████████| 79/79 [00:03<00:00, 22.28it/s]\n",
      "Lin.Test Epoch: [58] Loss: 1.7046 ACC: 60.42% : 100%|██████████| 16/16 [00:01<00:00, 11.73it/s]\n",
      "Lin.Train Epoch: [59] Loss: 1.6518 ACC: 60.14: 100%|██████████| 79/79 [00:03<00:00, 22.36it/s]\n",
      "Lin.Test Epoch: [59] Loss: 1.4606 ACC: 62.98% : 100%|██████████| 16/16 [00:01<00:00, 11.82it/s]\n",
      "Lin.Train Epoch: [60] Loss: 1.6091 ACC: 60.17: 100%|██████████| 79/79 [00:03<00:00, 22.86it/s]\n",
      "Lin.Test Epoch: [60] Loss: 1.4806 ACC: 62.52% : 100%|██████████| 16/16 [00:01<00:00, 11.41it/s]\n",
      "Lin.Train Epoch: [61] Loss: 1.5960 ACC: 60.45: 100%|██████████| 79/79 [00:03<00:00, 22.57it/s]\n",
      "Lin.Test Epoch: [61] Loss: 1.4495 ACC: 63.40% : 100%|██████████| 16/16 [00:01<00:00, 11.55it/s]\n",
      "Lin.Train Epoch: [62] Loss: 1.5491 ACC: 60.90: 100%|██████████| 79/79 [00:03<00:00, 22.15it/s]\n",
      "Lin.Test Epoch: [62] Loss: 1.4430 ACC: 63.18% : 100%|██████████| 16/16 [00:01<00:00, 12.57it/s]\n",
      "Lin.Train Epoch: [63] Loss: 1.5297 ACC: 60.91: 100%|██████████| 79/79 [00:03<00:00, 22.48it/s]\n",
      "Lin.Test Epoch: [63] Loss: 1.4133 ACC: 62.75% : 100%|██████████| 16/16 [00:01<00:00, 12.43it/s]\n",
      "Lin.Train Epoch: [64] Loss: 1.4947 ACC: 61.30: 100%|██████████| 79/79 [00:03<00:00, 22.46it/s]\n",
      "Lin.Test Epoch: [64] Loss: 1.4135 ACC: 62.95% : 100%|██████████| 16/16 [00:01<00:00, 11.78it/s]\n",
      "Lin.Train Epoch: [65] Loss: 1.4997 ACC: 61.29: 100%|██████████| 79/79 [00:03<00:00, 22.30it/s]\n",
      "Lin.Test Epoch: [65] Loss: 1.3305 ACC: 64.30% : 100%|██████████| 16/16 [00:01<00:00, 12.11it/s]\n",
      "Lin.Train Epoch: [66] Loss: 1.4934 ACC: 61.28: 100%|██████████| 79/79 [00:03<00:00, 22.17it/s]\n",
      "Lin.Test Epoch: [66] Loss: 1.3398 ACC: 64.62% : 100%|██████████| 16/16 [00:01<00:00, 11.82it/s]\n",
      "Lin.Train Epoch: [67] Loss: 1.4190 ACC: 62.48: 100%|██████████| 79/79 [00:03<00:00, 22.07it/s]\n",
      "Lin.Test Epoch: [67] Loss: 1.3347 ACC: 64.45% : 100%|██████████| 16/16 [00:01<00:00, 11.83it/s]\n",
      "Lin.Train Epoch: [68] Loss: 1.4207 ACC: 61.87: 100%|██████████| 79/79 [00:03<00:00, 22.17it/s]\n",
      "Lin.Test Epoch: [68] Loss: 1.4139 ACC: 63.00% : 100%|██████████| 16/16 [00:01<00:00, 11.84it/s]\n",
      "Lin.Train Epoch: [69] Loss: 1.4425 ACC: 62.37: 100%|██████████| 79/79 [00:03<00:00, 21.77it/s]\n",
      "Lin.Test Epoch: [69] Loss: 1.3378 ACC: 64.58% : 100%|██████████| 16/16 [00:01<00:00, 12.01it/s]\n",
      "Lin.Train Epoch: [70] Loss: 1.3889 ACC: 62.61: 100%|██████████| 79/79 [00:03<00:00, 22.05it/s]\n",
      "Lin.Test Epoch: [70] Loss: 1.3354 ACC: 64.28% : 100%|██████████| 16/16 [00:01<00:00, 12.33it/s]\n",
      "Lin.Train Epoch: [71] Loss: 1.3737 ACC: 62.91: 100%|██████████| 79/79 [00:03<00:00, 22.54it/s]\n",
      "Lin.Test Epoch: [71] Loss: 1.2885 ACC: 64.65% : 100%|██████████| 16/16 [00:01<00:00, 11.87it/s]\n",
      "Lin.Train Epoch: [72] Loss: 1.3314 ACC: 63.19: 100%|██████████| 79/79 [00:03<00:00, 22.04it/s]\n",
      "Lin.Test Epoch: [72] Loss: 1.2705 ACC: 64.80% : 100%|██████████| 16/16 [00:01<00:00, 11.11it/s]\n",
      "Lin.Train Epoch: [73] Loss: 1.3244 ACC: 63.73: 100%|██████████| 79/79 [00:03<00:00, 22.30it/s]\n",
      "Lin.Test Epoch: [73] Loss: 1.2576 ACC: 64.50% : 100%|██████████| 16/16 [00:01<00:00, 11.72it/s]\n",
      "Lin.Train Epoch: [74] Loss: 1.3285 ACC: 63.68: 100%|██████████| 79/79 [00:03<00:00, 22.39it/s]\n",
      "Lin.Test Epoch: [74] Loss: 1.2615 ACC: 64.30% : 100%|██████████| 16/16 [00:01<00:00, 12.53it/s]\n",
      "Lin.Train Epoch: [75] Loss: 1.2964 ACC: 64.28: 100%|██████████| 79/79 [00:03<00:00, 22.13it/s]\n",
      "Lin.Test Epoch: [75] Loss: 1.2681 ACC: 64.30% : 100%|██████████| 16/16 [00:01<00:00, 11.54it/s]\n",
      "Lin.Train Epoch: [76] Loss: 1.3036 ACC: 63.82: 100%|██████████| 79/79 [00:03<00:00, 22.14it/s]\n",
      "Lin.Test Epoch: [76] Loss: 1.2615 ACC: 64.62% : 100%|██████████| 16/16 [00:01<00:00, 11.87it/s]\n",
      "Lin.Train Epoch: [77] Loss: 1.2675 ACC: 64.63: 100%|██████████| 79/79 [00:03<00:00, 22.28it/s]\n",
      "Lin.Test Epoch: [77] Loss: 1.1869 ACC: 66.05% : 100%|██████████| 16/16 [00:01<00:00, 12.47it/s]\n",
      "Lin.Train Epoch: [78] Loss: 1.2620 ACC: 64.27: 100%|██████████| 79/79 [00:03<00:00, 22.18it/s]\n",
      "Lin.Test Epoch: [78] Loss: 1.2096 ACC: 65.48% : 100%|██████████| 16/16 [00:01<00:00, 11.47it/s]\n",
      "Lin.Train Epoch: [79] Loss: 1.2610 ACC: 64.61: 100%|██████████| 79/79 [00:03<00:00, 22.08it/s]\n",
      "Lin.Test Epoch: [79] Loss: 1.1851 ACC: 66.27% : 100%|██████████| 16/16 [00:01<00:00, 11.65it/s]\n",
      "Lin.Train Epoch: [80] Loss: 1.2486 ACC: 64.80: 100%|██████████| 79/79 [00:03<00:00, 22.36it/s]\n",
      "Lin.Test Epoch: [80] Loss: 1.1839 ACC: 66.33% : 100%|██████████| 16/16 [00:01<00:00, 12.27it/s]\n",
      "Lin.Train Epoch: [81] Loss: 1.2221 ACC: 65.61: 100%|██████████| 79/79 [00:03<00:00, 22.57it/s]\n",
      "Lin.Test Epoch: [81] Loss: 1.1765 ACC: 66.20% : 100%|██████████| 16/16 [00:01<00:00, 11.68it/s]\n",
      "Lin.Train Epoch: [82] Loss: 1.2260 ACC: 65.20: 100%|██████████| 79/79 [00:03<00:00, 22.02it/s]\n",
      "Lin.Test Epoch: [82] Loss: 1.1648 ACC: 66.65% : 100%|██████████| 16/16 [00:01<00:00, 11.95it/s]\n",
      "Lin.Train Epoch: [83] Loss: 1.2027 ACC: 65.52: 100%|██████████| 79/79 [00:03<00:00, 22.26it/s]\n",
      "Lin.Test Epoch: [83] Loss: 1.1724 ACC: 66.15% : 100%|██████████| 16/16 [00:01<00:00, 11.89it/s]\n",
      "Lin.Train Epoch: [84] Loss: 1.1913 ACC: 66.15: 100%|██████████| 79/79 [00:03<00:00, 22.16it/s]\n",
      "Lin.Test Epoch: [84] Loss: 1.1659 ACC: 66.55% : 100%|██████████| 16/16 [00:01<00:00, 11.76it/s]\n",
      "Lin.Train Epoch: [85] Loss: 1.1956 ACC: 65.89: 100%|██████████| 79/79 [00:03<00:00, 22.19it/s]\n",
      "Lin.Test Epoch: [85] Loss: 1.1552 ACC: 67.03% : 100%|██████████| 16/16 [00:01<00:00, 11.98it/s]\n",
      "Lin.Train Epoch: [86] Loss: 1.1769 ACC: 66.45: 100%|██████████| 79/79 [00:03<00:00, 22.24it/s]\n",
      "Lin.Test Epoch: [86] Loss: 1.1481 ACC: 67.00% : 100%|██████████| 16/16 [00:01<00:00, 11.39it/s]\n",
      "Lin.Train Epoch: [87] Loss: 1.1908 ACC: 65.44: 100%|██████████| 79/79 [00:03<00:00, 21.98it/s]\n",
      "Lin.Test Epoch: [87] Loss: 1.1488 ACC: 66.72% : 100%|██████████| 16/16 [00:01<00:00, 11.68it/s]\n",
      "Lin.Train Epoch: [88] Loss: 1.1934 ACC: 66.12: 100%|██████████| 79/79 [00:03<00:00, 22.44it/s]\n",
      "Lin.Test Epoch: [88] Loss: 1.1544 ACC: 66.12% : 100%|██████████| 16/16 [00:01<00:00, 11.58it/s]\n",
      "Lin.Train Epoch: [89] Loss: 1.1635 ACC: 66.55: 100%|██████████| 79/79 [00:03<00:00, 22.15it/s]\n",
      "Lin.Test Epoch: [89] Loss: 1.1570 ACC: 66.72% : 100%|██████████| 16/16 [00:01<00:00, 11.73it/s]\n",
      "Lin.Train Epoch: [90] Loss: 1.1630 ACC: 66.77: 100%|██████████| 79/79 [00:03<00:00, 22.23it/s]\n",
      "Lin.Test Epoch: [90] Loss: 1.1457 ACC: 66.90% : 100%|██████████| 16/16 [00:01<00:00, 11.73it/s]\n",
      "Lin.Train Epoch: [91] Loss: 1.1628 ACC: 66.86: 100%|██████████| 79/79 [00:03<00:00, 22.40it/s]\n",
      "Lin.Test Epoch: [91] Loss: 1.1416 ACC: 67.15% : 100%|██████████| 16/16 [00:01<00:00, 11.82it/s]\n",
      "Lin.Train Epoch: [92] Loss: 1.1687 ACC: 66.97: 100%|██████████| 79/79 [00:03<00:00, 22.26it/s]\n",
      "Lin.Test Epoch: [92] Loss: 1.1364 ACC: 67.22% : 100%|██████████| 16/16 [00:01<00:00, 12.08it/s]\n",
      "Lin.Train Epoch: [93] Loss: 1.1579 ACC: 66.47: 100%|██████████| 79/79 [00:03<00:00, 21.96it/s]\n",
      "Lin.Test Epoch: [93] Loss: 1.1349 ACC: 67.35% : 100%|██████████| 16/16 [00:01<00:00, 11.56it/s]\n",
      "Lin.Train Epoch: [94] Loss: 1.1493 ACC: 66.96: 100%|██████████| 79/79 [00:03<00:00, 22.02it/s]\n",
      "Lin.Test Epoch: [94] Loss: 1.1300 ACC: 67.22% : 100%|██████████| 16/16 [00:01<00:00, 11.33it/s]\n",
      "Lin.Train Epoch: [95] Loss: 1.1343 ACC: 67.22: 100%|██████████| 79/79 [00:03<00:00, 22.62it/s]\n",
      "Lin.Test Epoch: [95] Loss: 1.1306 ACC: 67.25% : 100%|██████████| 16/16 [00:01<00:00, 12.53it/s]\n",
      "Lin.Train Epoch: [96] Loss: 1.1465 ACC: 67.36: 100%|██████████| 79/79 [00:03<00:00, 22.04it/s]\n",
      "Lin.Test Epoch: [96] Loss: 1.1341 ACC: 67.15% : 100%|██████████| 16/16 [00:01<00:00, 11.87it/s]\n",
      "Lin.Train Epoch: [97] Loss: 1.1344 ACC: 67.16: 100%|██████████| 79/79 [00:03<00:00, 21.86it/s]\n",
      "Lin.Test Epoch: [97] Loss: 1.1263 ACC: 67.05% : 100%|██████████| 16/16 [00:01<00:00, 11.65it/s]\n",
      "Lin.Train Epoch: [98] Loss: 1.1245 ACC: 67.76: 100%|██████████| 79/79 [00:03<00:00, 22.01it/s]\n",
      "Lin.Test Epoch: [98] Loss: 1.1290 ACC: 67.38% : 100%|██████████| 16/16 [00:01<00:00, 11.83it/s]\n",
      "Lin.Train Epoch: [99] Loss: 1.1345 ACC: 67.47: 100%|██████████| 79/79 [00:03<00:00, 22.14it/s]\n",
      "Lin.Test Epoch: [99] Loss: 1.1252 ACC: 67.22% : 100%|██████████| 16/16 [00:01<00:00, 11.56it/s]\n",
      "Lin.Train Epoch: [100] Loss: 1.1323 ACC: 67.18: 100%|██████████| 79/79 [00:03<00:00, 22.29it/s]\n",
      "Lin.Test Epoch: [100] Loss: 1.1261 ACC: 66.97% : 100%|██████████| 16/16 [00:01<00:00, 11.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1261112451553346,\n",
       " 66.975,\n",
       " LinearClassifier(\n",
       "   (classifier): Linear(in_features=512, out_features=40, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_id = 1\n",
    "lin_epoch = 100\n",
    "num_class = np.sum(args.class_split[:task_id+1])\n",
    "classifier = LinearClassifier(num_classes = num_class).to(device)\n",
    "lin_optimizer = torch.optim.SGD(classifier.parameters(), 0.2, momentum=0.9, weight_decay=0) # Infomax: no weight decay, epoch 100, cosine scheduler\n",
    "lin_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(lin_optimizer, lin_epoch, eta_min=0.002) #scheduler + values ref: infomax paper\n",
    "linear_evaluation(model2, train_data_loaders_linear[:task_id+1], test_data_loaders[:task_id+1], lin_optimizer,classifier, lin_scheduler, lin_epoch, device, task_id)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_top_k(outputs, targets, top_k=(1,5)):\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argsort(outputs, dim=-1, descending=True)\n",
    "        result= []\n",
    "        for k in top_k:\n",
    "            correct_k = torch.sum((prediction[:, 0:k] == targets.unsqueeze(dim=-1)).any(dim=-1).float()).item() \n",
    "            result.append(correct_k)\n",
    "        return result\n",
    "    \n",
    "\n",
    "def store_samples(loader, num=150):\n",
    "    x_data = loader.dataset.train_data\n",
    "    select = np.random.randint(0,x_data.shape[0],num)\n",
    "    return torch.Tensor(x_data[select])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_id = 0\n",
    "old_samples = store_samples(train_data_loaders_linear[task_id],num=150)\n",
    "old_labels = torch.ones(old_samples.shape[0],dtype=torch.long) * task_id \n",
    "\n",
    "data_normalize_mean = (0.5071, 0.4865, 0.4409)\n",
    "data_normalize_std = (0.2673, 0.2564, 0.2762)\n",
    "random_crop_size = 32\n",
    "transform_linear = transforms.Compose( [\n",
    "          transforms.RandomResizedCrop(random_crop_size,  interpolation=transforms.InterpolationMode.BICUBIC), # scale=(0.2, 1.0) is possible\n",
    "          transforms.RandomHorizontalFlip(),\n",
    "          transforms.Normalize(data_normalize_mean, data_normalize_std),\n",
    "      ] )\n",
    "\n",
    "old_data_loader = DataLoader(TensorDataset(old_samples,old_labels,transform=transform_linear), batch_size=16, shuffle=True, \n",
    "                         num_workers = 5, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_train(net, data_loader, old_data_loader, new_batch_size, task_id, optimizer, scheduler, epochs, device):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.eval() # for not update batchnorm \n",
    "        total_num, train_bar = 0, tqdm(data_loader)\n",
    "        linear_loss = 0.0\n",
    "\n",
    "        current_data_loader = DataLoader(data_loader.dataset, batch_size=new_batch_size, shuffle=True, num_workers = 5, pin_memory=True)\n",
    "        dataloader_iterator = iter(old_data_loader)\n",
    "        for x1, _ in current_data_loader:\n",
    "            try:\n",
    "                x2, y2 = next(dataloader_iterator)\n",
    "            except StopIteration:\n",
    "                dataloader_iterator = iter(old_data_loader)\n",
    "                x2, y2 = next(dataloader_iterator)\n",
    "\n",
    "            y1 = torch.ones(x1.shape[0],dtype=torch.long) * task_id\n",
    "            x_all = torch.cat((x1, x2), dim=0)\n",
    "            y_all = torch.cat((y1, y2), dim=0)\n",
    "            x_all = x_all.to(device)\n",
    "            y_all = y_all.to(device)\n",
    "\n",
    "            # Forward prop of the model with single augmented batch\n",
    "            features = net(x_all)\n",
    "            logits = net.contrastive_projector(features) \n",
    "            \n",
    "            #c_weights = torch.nn.functional.normalize(net.contrastive_projector.weight,dim=1)\n",
    "            #logits = features @ c_weights.T\n",
    "            \n",
    "            # Cross Entropy Loss \n",
    "            linear_loss = F.cross_entropy(logits, y_all)\n",
    "\n",
    "            # Backpropagation part\n",
    "            optimizer.zero_grad()\n",
    "            linear_loss.backward()\n",
    "            # net.contrastive_projector.weight.grad[0:task_id] = torch.zeros(net.contrastive_projector.weight.grad[0:task_id].shape).to(device)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulating number of examples, losses and correct predictions\n",
    "            batchsize_bc = features.shape[0]\n",
    "            total_num += batchsize_bc\n",
    "            linear_loss += linear_loss.item() * batchsize_bc\n",
    "\n",
    "            train_bar.set_description('Lin.Train Epoch: [{}] Loss: {:.4f}'.format(epoch, linear_loss / total_num))\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        # wandb.log({\" Linear Layer Train Loss \": linear_loss / total_num, \" Epoch \": epoch})\n",
    "    return linear_loss/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    proj_hidden = args.proj_hidden\n",
    "    proj_out = args.proj_out\n",
    "    encoder = Encoder(hidden_dim=proj_hidden, output_dim=proj_out, normalization = args.normalization, weight_standard = args.weight_standard, appr_name = args.appr)\n",
    "    model2 = Siamese(encoder)\n",
    "    model2.to(device) #automatically detects from model\n",
    "#load model here\n",
    "file_name = \"./checkpoints/checkpoint_cifar100-algocassle_barlow-e[500, 500, 500, 500, 500]-b256-lr0.25-CS[20, 20, 20, 20, 20]_task_1_same_lr_True_norm_batch_ws_False.pth.tar\"\n",
    "dict = torch.load(file_name)\n",
    "model2.temporal_projector = nn.Sequential(\n",
    "            nn.Linear(args.proj_out, args.proj_hidden, bias=False),\n",
    "            nn.BatchNorm1d(args.proj_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.proj_hidden, args.proj_out),\n",
    "        ).to(device)\n",
    "model2.load_state_dict(dict['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin.Train Epoch: [1] Loss: 0.0003: 100%|██████████| 40/40 [00:05<00:00,  7.21it/s]\n",
      "Lin.Train Epoch: [2] Loss: 0.0000: 100%|██████████| 40/40 [00:05<00:00,  7.15it/s]\n",
      "Lin.Train Epoch: [3] Loss: 0.0000: 100%|██████████| 40/40 [00:05<00:00,  7.23it/s]\n",
      "Lin.Train Epoch: [4] Loss: 0.0006:  20%|██        | 8/40 [00:02<00:08,  3.79it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_701/749710106.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_data_loaders_generic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlin_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrastive_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loaders_linear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mold_encoders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlin_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_701/3083799026.py\u001b[0m in \u001b[0;36mcontrastive_train\u001b[0;34m(net, data_loader, old_encoders, old_labels, task_id, optimizer, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mlinear_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrastive_projector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrastive_projector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "contrastive_projector =  nn.Linear(512, 2, bias=False).to(device)\n",
    "model2.contrastive_projector = contrastive_projector \n",
    "task_id = 1\n",
    "lin_epoch= 10\n",
    "lin_optimizer = torch.optim.SGD(model2.parameters(), 1e-3, momentum=0.9, weight_decay=0) \n",
    "test_loss = contrastive_train(model2, train_data_loaders_linear[task_id],old_data_loader, old_labels, task_id, lin_optimizer, None, epochs=lin_epoch, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin.Train Epoch: [1] Loss: 1234.0271 ACC: 8.21: 100%|██████████| 79/79 [00:03<00:00, 21.60it/s]\n",
      "Lin.Test Epoch: [1] Loss: 600.4466 ACC: 14.65% : 100%|██████████| 16/16 [00:01<00:00, 11.73it/s]\n",
      "Lin.Train Epoch: [2] Loss: 178.7394 ACC: 37.83: 100%|██████████| 79/79 [00:03<00:00, 22.08it/s]\n",
      "Lin.Test Epoch: [2] Loss: 45.3162 ACC: 49.65% : 100%|██████████| 16/16 [00:01<00:00, 11.01it/s]\n",
      "Lin.Train Epoch: [3] Loss: 39.8305 ACC: 49.73: 100%|██████████| 79/79 [00:03<00:00, 21.98it/s]\n",
      "Lin.Test Epoch: [3] Loss: 34.8870 ACC: 52.62% : 100%|██████████| 16/16 [00:01<00:00, 11.21it/s]\n",
      "Lin.Train Epoch: [4] Loss: 34.9601 ACC: 49.29: 100%|██████████| 79/79 [00:03<00:00, 22.09it/s]\n",
      "Lin.Test Epoch: [4] Loss: 27.7315 ACC: 51.62% : 100%|██████████| 16/16 [00:01<00:00, 11.49it/s]\n",
      "Lin.Train Epoch: [5] Loss: 36.4738 ACC: 48.58: 100%|██████████| 79/79 [00:03<00:00, 21.42it/s]\n",
      "Lin.Test Epoch: [5] Loss: 30.4145 ACC: 51.68% : 100%|██████████| 16/16 [00:01<00:00, 10.64it/s]\n",
      "Lin.Train Epoch: [6] Loss: 37.1492 ACC: 48.59: 100%|██████████| 79/79 [00:03<00:00, 22.45it/s]\n",
      "Lin.Test Epoch: [6] Loss: 58.3502 ACC: 45.07% : 100%|██████████| 16/16 [00:01<00:00, 11.35it/s]\n",
      "Lin.Train Epoch: [7] Loss: 41.9291 ACC: 47.47: 100%|██████████| 79/79 [00:03<00:00, 22.29it/s]\n",
      "Lin.Test Epoch: [7] Loss: 42.5529 ACC: 46.02% : 100%|██████████| 16/16 [00:01<00:00, 10.95it/s]\n",
      "Lin.Train Epoch: [8] Loss: 29.0449 ACC: 49.70: 100%|██████████| 79/79 [00:03<00:00, 21.73it/s]\n",
      "Lin.Test Epoch: [8] Loss: 28.3674 ACC: 48.55% : 100%|██████████| 16/16 [00:01<00:00, 11.83it/s]\n",
      "Lin.Train Epoch: [9] Loss: 31.9570 ACC: 48.62: 100%|██████████| 79/79 [00:03<00:00, 21.91it/s]\n",
      "Lin.Test Epoch: [9] Loss: 43.5784 ACC: 47.35% : 100%|██████████| 16/16 [00:01<00:00, 11.10it/s]\n",
      "Lin.Train Epoch: [10] Loss: 34.1228 ACC: 47.33: 100%|██████████| 79/79 [00:03<00:00, 20.95it/s]\n",
      "Lin.Test Epoch: [10] Loss: 35.4312 ACC: 47.93% : 100%|██████████| 16/16 [00:01<00:00, 11.52it/s]\n",
      "Lin.Train Epoch: [11] Loss: 36.0335 ACC: 48.41: 100%|██████████| 79/79 [00:03<00:00, 21.74it/s]\n",
      "Lin.Test Epoch: [11] Loss: 46.3722 ACC: 40.35% : 100%|██████████| 16/16 [00:01<00:00, 11.18it/s]\n",
      "Lin.Train Epoch: [12] Loss: 43.2218 ACC: 46.16: 100%|██████████| 79/79 [00:03<00:00, 22.20it/s]\n",
      "Lin.Test Epoch: [12] Loss: 33.5107 ACC: 49.15% : 100%|██████████| 16/16 [00:01<00:00, 11.32it/s]\n",
      "Lin.Train Epoch: [13] Loss: 39.9601 ACC: 48.49: 100%|██████████| 79/79 [00:03<00:00, 21.65it/s]\n",
      "Lin.Test Epoch: [13] Loss: 80.1362 ACC: 35.00% : 100%|██████████| 16/16 [00:01<00:00, 11.37it/s]\n",
      "Lin.Train Epoch: [14] Loss: 43.8463 ACC: 48.03: 100%|██████████| 79/79 [00:03<00:00, 21.81it/s]\n",
      "Lin.Test Epoch: [14] Loss: 31.1005 ACC: 46.15% : 100%|██████████| 16/16 [00:01<00:00, 11.42it/s]\n",
      "Lin.Train Epoch: [15] Loss: 27.5280 ACC: 51.33: 100%|██████████| 79/79 [00:03<00:00, 21.80it/s]\n",
      "Lin.Test Epoch: [15] Loss: 30.2815 ACC: 47.20% : 100%|██████████| 16/16 [00:01<00:00, 10.87it/s]\n",
      "Lin.Train Epoch: [16] Loss: 31.3346 ACC: 50.82: 100%|██████████| 79/79 [00:03<00:00, 21.31it/s]\n",
      "Lin.Test Epoch: [16] Loss: 42.3471 ACC: 41.98% : 100%|██████████| 16/16 [00:01<00:00, 10.35it/s]\n",
      "Lin.Train Epoch: [17] Loss: 34.0913 ACC: 49.74: 100%|██████████| 79/79 [00:03<00:00, 21.66it/s]\n",
      "Lin.Test Epoch: [17] Loss: 31.1175 ACC: 46.83% : 100%|██████████| 16/16 [00:01<00:00, 11.35it/s]\n",
      "Lin.Train Epoch: [18] Loss: 42.0450 ACC: 48.50: 100%|██████████| 79/79 [00:03<00:00, 21.59it/s]\n",
      "Lin.Test Epoch: [18] Loss: 44.0934 ACC: 43.30% : 100%|██████████| 16/16 [00:01<00:00, 10.98it/s]\n",
      "Lin.Train Epoch: [19] Loss: 36.6088 ACC: 49.08: 100%|██████████| 79/79 [00:03<00:00, 21.81it/s]\n",
      "Lin.Test Epoch: [19] Loss: 26.4981 ACC: 50.85% : 100%|██████████| 16/16 [00:01<00:00, 11.07it/s]\n",
      "Lin.Train Epoch: [20] Loss: 26.2286 ACC: 51.40: 100%|██████████| 79/79 [00:03<00:00, 21.44it/s]\n",
      "Lin.Test Epoch: [20] Loss: 39.5467 ACC: 52.95% : 100%|██████████| 16/16 [00:01<00:00, 10.54it/s]\n",
      "Lin.Train Epoch: [21] Loss: 32.5729 ACC: 50.30: 100%|██████████| 79/79 [00:03<00:00, 21.76it/s]\n",
      "Lin.Test Epoch: [21] Loss: 25.5432 ACC: 51.40% : 100%|██████████| 16/16 [00:01<00:00, 11.60it/s]\n",
      "Lin.Train Epoch: [22] Loss: 27.7783 ACC: 50.90: 100%|██████████| 79/79 [00:03<00:00, 22.48it/s]\n",
      "Lin.Test Epoch: [22] Loss: 28.3802 ACC: 46.90% : 100%|██████████| 16/16 [00:01<00:00, 11.90it/s]\n",
      "Lin.Train Epoch: [23] Loss: 24.5246 ACC: 51.02: 100%|██████████| 79/79 [00:03<00:00, 21.75it/s]\n",
      "Lin.Test Epoch: [23] Loss: 26.1685 ACC: 52.20% : 100%|██████████| 16/16 [00:01<00:00, 11.04it/s]\n",
      "Lin.Train Epoch: [24] Loss: 28.5409 ACC: 50.72: 100%|██████████| 79/79 [00:03<00:00, 21.30it/s]\n",
      "Lin.Test Epoch: [24] Loss: 28.0363 ACC: 49.90% : 100%|██████████| 16/16 [00:01<00:00, 10.71it/s]\n",
      "Lin.Train Epoch: [25] Loss: 24.8209 ACC: 51.56: 100%|██████████| 79/79 [00:03<00:00, 21.93it/s]\n",
      "Lin.Test Epoch: [25] Loss: 27.2787 ACC: 52.18% : 100%|██████████| 16/16 [00:01<00:00, 11.50it/s]\n",
      "Lin.Train Epoch: [26] Loss: 20.3606 ACC: 53.30: 100%|██████████| 79/79 [00:03<00:00, 21.11it/s]\n",
      "Lin.Test Epoch: [26] Loss: 33.3133 ACC: 50.60% : 100%|██████████| 16/16 [00:01<00:00, 11.35it/s]\n",
      "Lin.Train Epoch: [27] Loss: 31.2157 ACC: 49.52: 100%|██████████| 79/79 [00:03<00:00, 21.48it/s]\n",
      "Lin.Test Epoch: [27] Loss: 48.9390 ACC: 39.75% : 100%|██████████| 16/16 [00:01<00:00, 11.22it/s]\n",
      "Lin.Train Epoch: [28] Loss: 36.0056 ACC: 51.02: 100%|██████████| 79/79 [00:03<00:00, 21.80it/s]\n",
      "Lin.Test Epoch: [28] Loss: 25.8203 ACC: 49.73% : 100%|██████████| 16/16 [00:01<00:00, 11.75it/s]\n",
      "Lin.Train Epoch: [29] Loss: 21.7229 ACC: 54.28: 100%|██████████| 79/79 [00:03<00:00, 21.90it/s]\n",
      "Lin.Test Epoch: [29] Loss: 17.9494 ACC: 54.12% : 100%|██████████| 16/16 [00:01<00:00, 11.08it/s]\n",
      "Lin.Train Epoch: [30] Loss: 22.2459 ACC: 51.73: 100%|██████████| 79/79 [00:03<00:00, 22.23it/s]\n",
      "Lin.Test Epoch: [30] Loss: 19.7941 ACC: 54.47% : 100%|██████████| 16/16 [00:01<00:00, 11.38it/s]\n",
      "Lin.Train Epoch: [31] Loss: 20.0172 ACC: 53.30: 100%|██████████| 79/79 [00:03<00:00, 21.40it/s]\n",
      "Lin.Test Epoch: [31] Loss: 19.3450 ACC: 55.88% : 100%|██████████| 16/16 [00:01<00:00, 11.33it/s]\n",
      "Lin.Train Epoch: [32] Loss: 19.8078 ACC: 53.08: 100%|██████████| 79/79 [00:03<00:00, 22.05it/s]\n",
      "Lin.Test Epoch: [32] Loss: 19.1142 ACC: 54.33% : 100%|██████████| 16/16 [00:01<00:00, 11.24it/s]\n",
      "Lin.Train Epoch: [33] Loss: 19.9769 ACC: 53.30: 100%|██████████| 79/79 [00:03<00:00, 21.29it/s]\n",
      "Lin.Test Epoch: [33] Loss: 18.4715 ACC: 55.43% : 100%|██████████| 16/16 [00:01<00:00, 10.64it/s]\n",
      "Lin.Train Epoch: [34] Loss: 18.2770 ACC: 53.33: 100%|██████████| 79/79 [00:03<00:00, 21.96it/s]\n",
      "Lin.Test Epoch: [34] Loss: 19.9213 ACC: 55.20% : 100%|██████████| 16/16 [00:01<00:00, 11.41it/s]\n",
      "Lin.Train Epoch: [35] Loss: 18.3308 ACC: 53.53: 100%|██████████| 79/79 [00:03<00:00, 21.57it/s]\n",
      "Lin.Test Epoch: [35] Loss: 19.2696 ACC: 52.30% : 100%|██████████| 16/16 [00:01<00:00, 11.29it/s]\n",
      "Lin.Train Epoch: [36] Loss: 21.4260 ACC: 54.17: 100%|██████████| 79/79 [00:03<00:00, 22.07it/s]\n",
      "Lin.Test Epoch: [36] Loss: 27.9267 ACC: 52.30% : 100%|██████████| 16/16 [00:01<00:00, 10.58it/s]\n",
      "Lin.Train Epoch: [37] Loss: 20.3261 ACC: 53.97: 100%|██████████| 79/79 [00:03<00:00, 21.17it/s]\n",
      "Lin.Test Epoch: [37] Loss: 22.1716 ACC: 48.88% : 100%|██████████| 16/16 [00:01<00:00, 11.82it/s]\n",
      "Lin.Train Epoch: [38] Loss: 18.8403 ACC: 53.56: 100%|██████████| 79/79 [00:03<00:00, 21.71it/s]\n",
      "Lin.Test Epoch: [38] Loss: 19.7137 ACC: 55.83% : 100%|██████████| 16/16 [00:01<00:00, 11.24it/s]\n",
      "Lin.Train Epoch: [39] Loss: 19.0202 ACC: 53.41: 100%|██████████| 79/79 [00:03<00:00, 21.81it/s]\n",
      "Lin.Test Epoch: [39] Loss: 22.2533 ACC: 50.48% : 100%|██████████| 16/16 [00:01<00:00, 11.40it/s]\n",
      "Lin.Train Epoch: [40] Loss: 16.6100 ACC: 54.46: 100%|██████████| 79/79 [00:03<00:00, 21.51it/s]\n",
      "Lin.Test Epoch: [40] Loss: 25.0830 ACC: 50.02% : 100%|██████████| 16/16 [00:01<00:00, 11.21it/s]\n",
      "Lin.Train Epoch: [41] Loss: 22.2154 ACC: 52.53: 100%|██████████| 79/79 [00:03<00:00, 21.58it/s]\n",
      "Lin.Test Epoch: [41] Loss: 16.1572 ACC: 55.45% : 100%|██████████| 16/16 [00:01<00:00, 11.65it/s]\n",
      "Lin.Train Epoch: [42] Loss: 15.4416 ACC: 55.15: 100%|██████████| 79/79 [00:03<00:00, 21.72it/s]\n",
      "Lin.Test Epoch: [42] Loss: 17.6768 ACC: 55.60% : 100%|██████████| 16/16 [00:01<00:00, 11.35it/s]\n",
      "Lin.Train Epoch: [43] Loss: 15.1211 ACC: 55.61: 100%|██████████| 79/79 [00:03<00:00, 21.84it/s]\n",
      "Lin.Test Epoch: [43] Loss: 14.2584 ACC: 57.07% : 100%|██████████| 16/16 [00:01<00:00, 11.78it/s]\n",
      "Lin.Train Epoch: [44] Loss: 14.9017 ACC: 54.85: 100%|██████████| 79/79 [00:03<00:00, 21.78it/s]\n",
      "Lin.Test Epoch: [44] Loss: 17.1274 ACC: 54.12% : 100%|██████████| 16/16 [00:01<00:00, 11.14it/s]\n",
      "Lin.Train Epoch: [45] Loss: 14.3378 ACC: 55.30: 100%|██████████| 79/79 [00:03<00:00, 21.93it/s]\n",
      "Lin.Test Epoch: [45] Loss: 14.5224 ACC: 56.30% : 100%|██████████| 16/16 [00:01<00:00, 12.06it/s]\n",
      "Lin.Train Epoch: [46] Loss: 17.6644 ACC: 54.26: 100%|██████████| 79/79 [00:03<00:00, 22.29it/s]\n",
      "Lin.Test Epoch: [46] Loss: 22.4229 ACC: 48.98% : 100%|██████████| 16/16 [00:01<00:00, 11.07it/s]\n",
      "Lin.Train Epoch: [47] Loss: 13.4683 ACC: 55.88: 100%|██████████| 79/79 [00:03<00:00, 21.90it/s]\n",
      "Lin.Test Epoch: [47] Loss: 13.7501 ACC: 55.53% : 100%|██████████| 16/16 [00:01<00:00, 11.05it/s]\n",
      "Lin.Train Epoch: [48] Loss: 12.8825 ACC: 56.07: 100%|██████████| 79/79 [00:03<00:00, 21.83it/s]\n",
      "Lin.Test Epoch: [48] Loss: 11.3728 ACC: 59.92% : 100%|██████████| 16/16 [00:01<00:00, 11.20it/s]\n",
      "Lin.Train Epoch: [49] Loss: 11.3554 ACC: 57.30: 100%|██████████| 79/79 [00:03<00:00, 20.90it/s]\n",
      "Lin.Test Epoch: [49] Loss: 14.6547 ACC: 54.07% : 100%|██████████| 16/16 [00:01<00:00, 10.96it/s]\n",
      "Lin.Train Epoch: [50] Loss: 11.9299 ACC: 56.21: 100%|██████████| 79/79 [00:03<00:00, 21.80it/s]\n",
      "Lin.Test Epoch: [50] Loss: 14.8075 ACC: 52.83% : 100%|██████████| 16/16 [00:01<00:00, 11.29it/s]\n",
      "Lin.Train Epoch: [51] Loss: 11.4276 ACC: 57.00: 100%|██████████| 79/79 [00:03<00:00, 21.60it/s]\n",
      "Lin.Test Epoch: [51] Loss: 14.3818 ACC: 52.60% : 100%|██████████| 16/16 [00:01<00:00, 10.65it/s]\n",
      "Lin.Train Epoch: [52] Loss: 12.4414 ACC: 55.20: 100%|██████████| 79/79 [00:03<00:00, 21.84it/s]\n",
      "Lin.Test Epoch: [52] Loss: 12.6275 ACC: 55.35% : 100%|██████████| 16/16 [00:01<00:00, 11.12it/s]\n",
      "Lin.Train Epoch: [53] Loss: 10.7207 ACC: 57.65: 100%|██████████| 79/79 [00:03<00:00, 21.55it/s]\n",
      "Lin.Test Epoch: [53] Loss: 12.6804 ACC: 54.02% : 100%|██████████| 16/16 [00:01<00:00, 10.32it/s]\n",
      "Lin.Train Epoch: [54] Loss: 10.0259 ACC: 57.93: 100%|██████████| 79/79 [00:03<00:00, 21.63it/s]\n",
      "Lin.Test Epoch: [54] Loss: 11.8034 ACC: 55.85% : 100%|██████████| 16/16 [00:01<00:00, 11.47it/s]\n",
      "Lin.Train Epoch: [55] Loss: 11.2106 ACC: 56.53: 100%|██████████| 79/79 [00:03<00:00, 21.22it/s]\n",
      "Lin.Test Epoch: [55] Loss: 13.1144 ACC: 55.12% : 100%|██████████| 16/16 [00:01<00:00, 10.85it/s]\n",
      "Lin.Train Epoch: [56] Loss: 10.3211 ACC: 57.63: 100%|██████████| 79/79 [00:03<00:00, 22.10it/s]\n",
      "Lin.Test Epoch: [56] Loss: 11.0480 ACC: 57.23% : 100%|██████████| 16/16 [00:01<00:00, 10.81it/s]\n",
      "Lin.Train Epoch: [57] Loss: 10.4018 ACC: 57.26: 100%|██████████| 79/79 [00:03<00:00, 21.75it/s]\n",
      "Lin.Test Epoch: [57] Loss: 10.0754 ACC: 57.60% : 100%|██████████| 16/16 [00:01<00:00, 10.34it/s]\n",
      "Lin.Train Epoch: [58] Loss: 9.1843 ACC: 58.14: 100%|██████████| 79/79 [00:03<00:00, 21.97it/s] \n",
      "Lin.Test Epoch: [58] Loss: 9.0190 ACC: 60.08% : 100%|██████████| 16/16 [00:01<00:00, 11.48it/s]\n",
      "Lin.Train Epoch: [59] Loss: 8.6903 ACC: 59.16: 100%|██████████| 79/79 [00:03<00:00, 21.68it/s] \n",
      "Lin.Test Epoch: [59] Loss: 10.6903 ACC: 55.35% : 100%|██████████| 16/16 [00:01<00:00, 11.24it/s]\n",
      "Lin.Train Epoch: [60] Loss: 9.4030 ACC: 57.14: 100%|██████████| 79/79 [00:03<00:00, 21.71it/s] \n",
      "Lin.Test Epoch: [60] Loss: 9.6633 ACC: 58.05% : 100%|██████████| 16/16 [00:01<00:00, 12.35it/s]\n",
      "Lin.Train Epoch: [61] Loss: 9.5716 ACC: 57.09: 100%|██████████| 79/79 [00:03<00:00, 21.77it/s] \n",
      "Lin.Test Epoch: [61] Loss: 8.1612 ACC: 60.80% : 100%|██████████| 16/16 [00:01<00:00, 11.35it/s]\n",
      "Lin.Train Epoch: [62] Loss: 8.4551 ACC: 58.59: 100%|██████████| 79/79 [00:03<00:00, 21.99it/s]\n",
      "Lin.Test Epoch: [62] Loss: 7.1895 ACC: 61.68% : 100%|██████████| 16/16 [00:01<00:00, 11.32it/s]\n",
      "Lin.Train Epoch: [63] Loss: 7.6270 ACC: 59.91: 100%|██████████| 79/79 [00:03<00:00, 21.73it/s]\n",
      "Lin.Test Epoch: [63] Loss: 8.2871 ACC: 58.27% : 100%|██████████| 16/16 [00:01<00:00, 11.41it/s]\n",
      "Lin.Train Epoch: [64] Loss: 7.8563 ACC: 59.39: 100%|██████████| 79/79 [00:03<00:00, 21.81it/s]\n",
      "Lin.Test Epoch: [64] Loss: 7.4968 ACC: 60.00% : 100%|██████████| 16/16 [00:01<00:00, 11.24it/s]\n",
      "Lin.Train Epoch: [65] Loss: 7.1743 ACC: 60.34: 100%|██████████| 79/79 [00:03<00:00, 21.63it/s]\n",
      "Lin.Test Epoch: [65] Loss: 7.0804 ACC: 61.27% : 100%|██████████| 16/16 [00:01<00:00, 11.14it/s]\n",
      "Lin.Train Epoch: [66] Loss: 7.2774 ACC: 59.96: 100%|██████████| 79/79 [00:03<00:00, 21.37it/s]\n",
      "Lin.Test Epoch: [66] Loss: 9.0286 ACC: 56.65% : 100%|██████████| 16/16 [00:01<00:00, 11.29it/s]\n",
      "Lin.Train Epoch: [67] Loss: 7.1545 ACC: 60.34: 100%|██████████| 79/79 [00:03<00:00, 21.81it/s]\n",
      "Lin.Test Epoch: [67] Loss: 8.1122 ACC: 57.55% : 100%|██████████| 16/16 [00:01<00:00, 11.17it/s]\n",
      "Lin.Train Epoch: [68] Loss: 7.4457 ACC: 59.28: 100%|██████████| 79/79 [00:03<00:00, 21.67it/s]\n",
      "Lin.Test Epoch: [68] Loss: 10.0614 ACC: 55.12% : 100%|██████████| 16/16 [00:01<00:00, 10.40it/s]\n",
      "Lin.Train Epoch: [69] Loss: 6.9943 ACC: 60.22: 100%|██████████| 79/79 [00:03<00:00, 21.45it/s]\n",
      "Lin.Test Epoch: [69] Loss: 7.0635 ACC: 59.92% : 100%|██████████| 16/16 [00:01<00:00, 10.76it/s]\n",
      "Lin.Train Epoch: [70] Loss: 6.3755 ACC: 61.39: 100%|██████████| 79/79 [00:03<00:00, 22.14it/s]\n",
      "Lin.Test Epoch: [70] Loss: 7.9415 ACC: 58.83% : 100%|██████████| 16/16 [00:01<00:00, 11.50it/s]\n",
      "Lin.Train Epoch: [71] Loss: 6.1628 ACC: 61.07: 100%|██████████| 79/79 [00:03<00:00, 21.51it/s]\n",
      "Lin.Test Epoch: [71] Loss: 7.4268 ACC: 59.30% : 100%|██████████| 16/16 [00:01<00:00, 11.11it/s]\n",
      "Lin.Train Epoch: [72] Loss: 6.3885 ACC: 60.82: 100%|██████████| 79/79 [00:03<00:00, 21.88it/s]\n",
      "Lin.Test Epoch: [72] Loss: 6.4362 ACC: 60.15% : 100%|██████████| 16/16 [00:01<00:00, 11.33it/s]\n",
      "Lin.Train Epoch: [73] Loss: 5.8025 ACC: 62.09: 100%|██████████| 79/79 [00:03<00:00, 22.37it/s]\n",
      "Lin.Test Epoch: [73] Loss: 6.1499 ACC: 61.02% : 100%|██████████| 16/16 [00:01<00:00, 11.11it/s]\n",
      "Lin.Train Epoch: [74] Loss: 5.7170 ACC: 61.88: 100%|██████████| 79/79 [00:03<00:00, 21.95it/s]\n",
      "Lin.Test Epoch: [74] Loss: 6.1508 ACC: 61.48% : 100%|██████████| 16/16 [00:01<00:00, 12.06it/s]\n",
      "Lin.Train Epoch: [75] Loss: 5.7543 ACC: 61.58: 100%|██████████| 79/79 [00:03<00:00, 21.81it/s]\n",
      "Lin.Test Epoch: [75] Loss: 5.6654 ACC: 63.12% : 100%|██████████| 16/16 [00:01<00:00, 10.52it/s]\n",
      "Lin.Train Epoch: [76] Loss: 5.6206 ACC: 61.95: 100%|██████████| 79/79 [00:03<00:00, 22.09it/s]\n",
      "Lin.Test Epoch: [76] Loss: 5.6394 ACC: 62.78% : 100%|██████████| 16/16 [00:01<00:00, 11.15it/s]\n",
      "Lin.Train Epoch: [77] Loss: 5.3252 ACC: 62.26: 100%|██████████| 79/79 [00:03<00:00, 21.15it/s]\n",
      "Lin.Test Epoch: [77] Loss: 5.5466 ACC: 62.00% : 100%|██████████| 16/16 [00:01<00:00, 11.36it/s]\n",
      "Lin.Train Epoch: [78] Loss: 5.2599 ACC: 62.48: 100%|██████████| 79/79 [00:03<00:00, 21.84it/s]\n",
      "Lin.Test Epoch: [78] Loss: 5.6306 ACC: 62.90% : 100%|██████████| 16/16 [00:01<00:00, 11.57it/s]\n",
      "Lin.Train Epoch: [79] Loss: 5.1856 ACC: 62.48: 100%|██████████| 79/79 [00:03<00:00, 21.75it/s]\n",
      "Lin.Test Epoch: [79] Loss: 5.4371 ACC: 62.62% : 100%|██████████| 16/16 [00:01<00:00, 11.08it/s]\n",
      "Lin.Train Epoch: [80] Loss: 4.9494 ACC: 62.96: 100%|██████████| 79/79 [00:03<00:00, 21.47it/s]\n",
      "Lin.Test Epoch: [80] Loss: 5.1244 ACC: 63.28% : 100%|██████████| 16/16 [00:01<00:00, 10.89it/s]\n",
      "Lin.Train Epoch: [81] Loss: 4.8975 ACC: 62.92: 100%|██████████| 79/79 [00:03<00:00, 21.74it/s]\n",
      "Lin.Test Epoch: [81] Loss: 5.1414 ACC: 62.92% : 100%|██████████| 16/16 [00:01<00:00, 11.39it/s]\n",
      "Lin.Train Epoch: [82] Loss: 4.9197 ACC: 63.18: 100%|██████████| 79/79 [00:03<00:00, 21.94it/s]\n",
      "Lin.Test Epoch: [82] Loss: 4.7911 ACC: 64.15% : 100%|██████████| 16/16 [00:01<00:00, 11.06it/s]\n",
      "Lin.Train Epoch: [83] Loss: 4.7184 ACC: 63.68: 100%|██████████| 79/79 [00:03<00:00, 21.60it/s]\n",
      "Lin.Test Epoch: [83] Loss: 4.9916 ACC: 63.50% : 100%|██████████| 16/16 [00:01<00:00, 12.32it/s]\n",
      "Lin.Train Epoch: [84] Loss: 4.6029 ACC: 63.95: 100%|██████████| 79/79 [00:03<00:00, 22.26it/s]\n",
      "Lin.Test Epoch: [84] Loss: 4.6374 ACC: 64.62% : 100%|██████████| 16/16 [00:01<00:00, 11.11it/s]\n",
      "Lin.Train Epoch: [85] Loss: 4.5128 ACC: 63.97: 100%|██████████| 79/79 [00:03<00:00, 21.78it/s]\n",
      "Lin.Test Epoch: [85] Loss: 4.6402 ACC: 64.22% : 100%|██████████| 16/16 [00:01<00:00, 10.32it/s]\n",
      "Lin.Train Epoch: [86] Loss: 4.4956 ACC: 64.04: 100%|██████████| 79/79 [00:03<00:00, 21.61it/s]\n",
      "Lin.Test Epoch: [86] Loss: 4.5291 ACC: 64.35% : 100%|██████████| 16/16 [00:01<00:00, 11.14it/s]\n",
      "Lin.Train Epoch: [87] Loss: 4.3969 ACC: 64.23: 100%|██████████| 79/79 [00:03<00:00, 22.55it/s]\n",
      "Lin.Test Epoch: [87] Loss: 4.4944 ACC: 64.42% : 100%|██████████| 16/16 [00:01<00:00, 11.13it/s]\n",
      "Lin.Train Epoch: [88] Loss: 4.4338 ACC: 63.88: 100%|██████████| 79/79 [00:03<00:00, 21.75it/s]\n",
      "Lin.Test Epoch: [88] Loss: 4.5252 ACC: 65.03% : 100%|██████████| 16/16 [00:01<00:00, 11.22it/s]\n",
      "Lin.Train Epoch: [89] Loss: 4.3727 ACC: 64.46: 100%|██████████| 79/79 [00:03<00:00, 21.56it/s]\n",
      "Lin.Test Epoch: [89] Loss: 4.4748 ACC: 65.12% : 100%|██████████| 16/16 [00:01<00:00, 11.49it/s]\n",
      "Lin.Train Epoch: [90] Loss: 4.2424 ACC: 64.56: 100%|██████████| 79/79 [00:03<00:00, 21.96it/s]\n",
      "Lin.Test Epoch: [90] Loss: 4.3969 ACC: 64.85% : 100%|██████████| 16/16 [00:01<00:00, 11.59it/s]\n",
      "Lin.Train Epoch: [91] Loss: 4.2298 ACC: 64.62: 100%|██████████| 79/79 [00:03<00:00, 21.77it/s]\n",
      "Lin.Test Epoch: [91] Loss: 4.5887 ACC: 64.03% : 100%|██████████| 16/16 [00:01<00:00, 10.98it/s]\n",
      "Lin.Train Epoch: [92] Loss: 4.2241 ACC: 64.76: 100%|██████████| 79/79 [00:03<00:00, 21.70it/s]\n",
      "Lin.Test Epoch: [92] Loss: 4.2945 ACC: 65.75% : 100%|██████████| 16/16 [00:01<00:00, 11.07it/s]\n",
      "Lin.Train Epoch: [93] Loss: 4.1869 ACC: 64.42: 100%|██████████| 79/79 [00:03<00:00, 21.82it/s]\n",
      "Lin.Test Epoch: [93] Loss: 4.2987 ACC: 64.83% : 100%|██████████| 16/16 [00:01<00:00, 11.11it/s]\n",
      "Lin.Train Epoch: [94] Loss: 4.1705 ACC: 64.59: 100%|██████████| 79/79 [00:03<00:00, 21.91it/s]\n",
      "Lin.Test Epoch: [94] Loss: 4.3077 ACC: 65.28% : 100%|██████████| 16/16 [00:01<00:00, 11.25it/s]\n",
      "Lin.Train Epoch: [95] Loss: 4.1034 ACC: 65.20: 100%|██████████| 79/79 [00:03<00:00, 21.95it/s]\n",
      "Lin.Test Epoch: [95] Loss: 4.2604 ACC: 65.65% : 100%|██████████| 16/16 [00:01<00:00, 11.03it/s]\n",
      "Lin.Train Epoch: [96] Loss: 4.0573 ACC: 64.98: 100%|██████████| 79/79 [00:03<00:00, 21.62it/s]\n",
      "Lin.Test Epoch: [96] Loss: 4.2010 ACC: 65.83% : 100%|██████████| 16/16 [00:01<00:00, 11.08it/s]\n",
      "Lin.Train Epoch: [97] Loss: 4.0479 ACC: 65.22: 100%|██████████| 79/79 [00:03<00:00, 21.40it/s]\n",
      "Lin.Test Epoch: [97] Loss: 4.1720 ACC: 66.00% : 100%|██████████| 16/16 [00:01<00:00, 10.25it/s]\n",
      "Lin.Train Epoch: [98] Loss: 4.0117 ACC: 64.95: 100%|██████████| 79/79 [00:03<00:00, 21.84it/s]\n",
      "Lin.Test Epoch: [98] Loss: 4.1939 ACC: 65.83% : 100%|██████████| 16/16 [00:01<00:00, 11.01it/s]\n",
      "Lin.Train Epoch: [99] Loss: 4.0919 ACC: 64.96: 100%|██████████| 79/79 [00:03<00:00, 21.62it/s]\n",
      "Lin.Test Epoch: [99] Loss: 4.1738 ACC: 65.88% : 100%|██████████| 16/16 [00:01<00:00, 11.35it/s]\n",
      "Lin.Train Epoch: [100] Loss: 4.0588 ACC: 65.21: 100%|██████████| 79/79 [00:03<00:00, 21.64it/s]\n",
      "Lin.Test Epoch: [100] Loss: 4.1778 ACC: 65.67% : 100%|██████████| 16/16 [00:01<00:00, 11.31it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.177773349761963,\n",
       " 65.675,\n",
       " LinearClassifier(\n",
       "   (classifier): Linear(in_features=512, out_features=40, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_id = 1\n",
    "lin_epoch = 100\n",
    "num_class = np.sum(args.class_split[:task_id+1])\n",
    "classifier = LinearClassifier(num_classes = num_class).to(device)\n",
    "lin_optimizer = torch.optim.SGD(classifier.parameters(), 0.2, momentum=0.9, weight_decay=0) # Infomax: no weight decay, epoch 100, cosine scheduler\n",
    "lin_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(lin_optimizer, lin_epoch, eta_min=0.002) #scheduler + values ref: infomax paper\n",
    "linear_evaluation(model2, train_data_loaders_linear[:task_id+1], test_data_loaders[:task_id+1], lin_optimizer,classifier, lin_scheduler, lin_epoch, device, task_id)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_top_k(outputs, targets, top_k=(1,5)):\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argsort(outputs, dim=-1, descending=True)\n",
    "        result= []\n",
    "        for k in top_k:\n",
    "            correct_k = torch.sum((prediction[:, 0:k] == targets.unsqueeze(dim=-1)).any(dim=-1).float()).item() \n",
    "            result.append(correct_k)\n",
    "        return result\n",
    "    \n",
    "def contrastive_train2(net, data_loader, task_id, optimizer, classifier, scheduler, epochs, device):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.eval() # for not update batchnorm \n",
    "        total_num, train_bar = 0, tqdm(data_loader)\n",
    "        linear_loss = 0.0\n",
    "        for data_tuple in train_bar:\n",
    "            # Forward prop of the model with single augmented batch\n",
    "            pos_1, targets = data_tuple\n",
    "            pos_1 = pos_1[0]\n",
    "            pos_1 = torch.cat((pos_1, -pos_1), dim=0)\n",
    "            # print(pos_1.shape)\n",
    "            pos_1 = pos_1.to(device)\n",
    "            features = net(pos_1)\n",
    "\n",
    "            # Batchsize\n",
    "            batchsize_bc = features.shape[0]\n",
    "            targets = torch.zeros(targets.shape[0],dtype=torch.long).to(device)\n",
    "            targets = torch.cat((targets, torch.ones(targets.shape[0],dtype=torch.long).to(device)), dim=0)\n",
    "            # print(targets.shape)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            logits = classifier(features.detach()) \n",
    "\n",
    "            # Cross Entropy Loss \n",
    "            linear_loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # Backpropagation part\n",
    "            optimizer.zero_grad()\n",
    "            linear_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulating number of examples, losses and correct predictions\n",
    "            total_num += batchsize_bc\n",
    "            linear_loss += linear_loss.item() * batchsize_bc\n",
    "\n",
    "            train_bar.set_description('Lin.Train Epoch: [{}] Loss: {:.4f}'.format(epoch, linear_loss / total_num))\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        # wandb.log({\" Linear Layer Train Loss \": linear_loss / total_num, \" Epoch \": epoch})\n",
    "    return linear_loss/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedml_academic",
   "language": "python",
   "name": "fedml_academic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 29 2022, 02:18:16) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
