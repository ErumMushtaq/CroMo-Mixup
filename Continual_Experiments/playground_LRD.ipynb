{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duygu/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"\")))\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "\n",
    "from dataloaders.dataloader_cifar10 import get_cifar10\n",
    "from dataloaders.dataloader_cifar100 import get_cifar100\n",
    "from utils.eval_metrics import linear_evaluation, get_t_SNE_plot\n",
    "from models.linear_classifer import LinearClassifier\n",
    "from models.ssl import  SimSiam, Siamese, Encoder, Predictor\n",
    "\n",
    "from trainers.train_simsiam import train_simsiam\n",
    "from trainers.train_infomax import train_infomax\n",
    "from trainers.train_barlow import train_barlow\n",
    "\n",
    "from trainers.train_PFR import train_PFR_simsiam\n",
    "from trainers.train_PFR_contrastive import train_PFR_contrastive_simsiam\n",
    "from trainers.train_contrastive import train_contrastive_simsiam\n",
    "from trainers.train_ering import train_ering_simsiam\n",
    "\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from utils.lr_schedulers import LinearWarmupCosineAnnealingLR, SimSiamScheduler\n",
    "from utils.eval_metrics import Knn_Validation_cont\n",
    "from copy import deepcopy\n",
    "from loss import invariance_loss,CovarianceLoss,ErrorCovarianceLoss\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils.lr_schedulers import LinearWarmupCosineAnnealingLR, SimSiamScheduler\n",
    "from utils.eval_metrics import Knn_Validation_cont\n",
    "from copy import deepcopy\n",
    "from loss import invariance_loss,CovarianceLoss,ErrorCovarianceLoss,BarlowTwinsLoss\n",
    "from utils.lars import LARS\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[0.1, 2.0]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = torchvision.transforms.functional.gaussian_blur(x,kernel_size=[3,3],sigma=sigma)#kernel size and sigma are open problems but right now seems ok!\n",
    "        return x\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    normalization = 'batch'\n",
    "    weight_standard = True\n",
    "    same_lr = False\n",
    "    pretrain_batch_size = 512\n",
    "    pretrain_warmup_epochs = 10\n",
    "    pretrain_warmup_lr = 3e-3\n",
    "    pretrain_base_lr = 0.03\n",
    "    pretrain_momentum = 0.9\n",
    "    pretrain_weight_decay = 5e-4\n",
    "    min_lr = 0.00\n",
    "    lambdap = 1.0\n",
    "    appr = 'barlow_PFR'\n",
    "    knn_report_freq = 10\n",
    "    cuda_device = 4\n",
    "    num_workers = 8\n",
    "    contrastive_ratio = 0.001\n",
    "    dataset = 'cifar100'\n",
    "    class_split = [25,25,25,25]\n",
    "    epochs = [500,500,500,500]\n",
    "    cov_loss_weight = 1.0\n",
    "    sim_loss_weight = 250.0\n",
    "    info_loss = 'invariance'\n",
    "    lambda_norm = 1.0\n",
    "    subspace_rate = 0.99\n",
    "    lambda_param = 5e-3\n",
    "    bsize = 32\n",
    "    msize = 150\n",
    "    proj_hidden = 2048\n",
    "    proj_out = 2048 #infomax\n",
    "    pred_hidden = 512\n",
    "    pred_out = 2048\n",
    "    scale_loss = 0.1\n",
    "    contrastive_ratio = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "        get_dataloaders = get_cifar10\n",
    "        num_classes=10\n",
    "elif args.dataset == \"cifar100\":\n",
    "    get_dataloaders = get_cifar100\n",
    "    num_classes=100\n",
    "assert sum(args.class_split) == num_classes\n",
    "assert len(args.class_split) == len(args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n"
     ]
    }
   ],
   "source": [
    "num_worker = args.num_workers\n",
    "#device\n",
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb init\n",
    "wandb.init(project=\"CSSL\",  entity=\"yavuz-team\",\n",
    "            mode=\"disabled\",\n",
    "            config=args,\n",
    "            name= str(args.dataset) + '-algo' + str(args.appr) + \"-e\" + str(args.epochs) + \"-b\" \n",
    "            + str(args.pretrain_batch_size) + \"-lr\" + str(args.pretrain_base_lr)+\"-CS\"+str(args.class_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    transform = T.Compose([\n",
    "            T.RandomResizedCrop(size=32, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)]), p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([GaussianBlur()], p=0.5), \n",
    "            T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])])\n",
    "\n",
    "    transform_prime = T.Compose([\n",
    "            T.RandomResizedCrop(size=32, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)]), p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([GaussianBlur()], p=0.5), \n",
    "            T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataloaders..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Dataloaders\n",
    "print(\"Creating Dataloaders..\")\n",
    "#Class Based\n",
    "train_data_loaders, train_data_loaders_knn, test_data_loaders, _, train_data_loaders_linear, train_data_loaders_pure  = get_dataloaders(transform, transform_prime, \\\n",
    "                                    classes=args.class_split, valid_rate = 0.00, batch_size=args.pretrain_batch_size, seed = 0, num_worker= num_worker)\n",
    "_, train_data_loaders_knn_all, test_data_loaders_all, _, train_data_loaders_linear_all, train_data_loaders_pure_all = get_dataloaders(transform, transform_prime, \\\n",
    "                                        classes=[num_classes], valid_rate = 0.00, batch_size=args.pretrain_batch_size, seed = 0, num_worker= num_worker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    proj_hidden = args.proj_hidden\n",
    "    proj_out = args.proj_out\n",
    "    encoder = Encoder(hidden_dim=proj_hidden, output_dim=proj_out, normalization = args.normalization, weight_standard = args.weight_standard,appr_name =args.appr)\n",
    "    model = Siamese(encoder)\n",
    "    model.to(device) #automatically detects from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.temporal_projector = nn.Sequential(\n",
    "            nn.Linear(args.proj_out, args.proj_hidden, bias=False),\n",
    "            nn.BatchNorm1d(args.proj_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.proj_hidden, args.proj_out),\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    x = F.normalize(x, dim=-1, p=2)\n",
    "    y = F.normalize(y, dim=-1, p=2)\n",
    "    return  - (x * y).sum(dim=-1).mean()\n",
    "\n",
    "def get_linear_vector(model, loader, rate=0.99,device = None, task=None):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for x,y in loader:\n",
    "        x = x.to(device)\n",
    "        out = model(x).cpu().detach().numpy()\n",
    "        outs.append(out)\n",
    "        \n",
    "    outs = np.concatenate(outs)\n",
    "    outs = outs.transpose()\n",
    "    outs = torch.tensor(outs)\n",
    "\n",
    "    \n",
    "\n",
    "    remaining = outs\n",
    "    U, S, V = torch.svd(remaining)\n",
    "    for i in range(len(S)):\n",
    "        total = torch.norm(outs)**2 \n",
    "        hand =  torch.norm(S[0:i+1])**2\n",
    "        \n",
    "        if hand / total > rate:\n",
    "            break\n",
    "\n",
    "    print(U[:,0:i+1].shape)\n",
    "\n",
    "    \n",
    "    Q = U[:,0:i+1]\n",
    "    Q_weighted = Q * S[0:i+1].reshape(1,-1)\n",
    "\n",
    "    vector = torch.mean(Q_weighted,dim=1)\n",
    "    vector = torch.nn.functional.normalize(vector,dim=0)\n",
    "   \n",
    "\n",
    "    return vector\n",
    "\n",
    "def extract_subspace(model, loader, rate=0.99,device = None, Q_prev = None, task=None):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for x,y in loader:\n",
    "        x = x.to(device)\n",
    "        out = model(x).cpu().detach().numpy()\n",
    "        outs.append(out)\n",
    "        \n",
    "    outs = np.concatenate(outs)\n",
    "    outs = outs.transpose()\n",
    "    outs = torch.tensor(outs)\n",
    "\n",
    "    if Q_prev == None:\n",
    "        projected = torch.zeros(1)\n",
    "    else:\n",
    "        Q_prev = Q_prev.to('cpu')\n",
    "        projected = Q_prev  @ Q_prev.T @ outs \n",
    "\n",
    "    remaining = outs - projected\n",
    "    U, S, V = torch.svd(remaining)\n",
    "    for i in range(len(S)):\n",
    "        total = torch.norm(outs)**2 \n",
    "        hand =  torch.norm(projected)**2 + torch.norm(S[0:i+1])**2\n",
    "        \n",
    "        if hand / total > rate:\n",
    "            break\n",
    "\n",
    "    print(U[:,0:i+1].shape)\n",
    "\n",
    "    if Q_prev == None:\n",
    "        Q_prev = U[:,0:i+1]\n",
    "    else:\n",
    "        Q_prev = torch.cat((Q_prev, U[:,0:i+1]),dim=1)\n",
    "        Q_prev, _ = torch.linalg.qr(Q_prev, mode=\"reduced\")\n",
    "    wandb.log({\"Task\": task, \"LRD Space Used \": Q_prev.shape[1]/Q_prev.shape[0] })  \n",
    "    print(Q_prev.shape)\n",
    "    return Q_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory(memory_x, memory_y, dataloader, size, task):\n",
    "    indices = np.random.choice(len(dataloader.dataset), size=size, replace=False)\n",
    "    x, _ =  dataloader.dataset[indices]\n",
    "    memory_x = torch.cat((memory_x, x), dim=0)\n",
    "    y = torch.ones(x.shape[0],dtype=torch.long) * task\n",
    "    memory_y = torch.cat((memory_y, y), dim=0)\n",
    "    return memory_x, memory_y.to(dtype=torch.long)\n",
    "\n",
    "def train_LRD_cross_barlow(model, train_data_loaders, knn_train_data_loaders, train_data_loaders_pure, test_data_loaders, device, args):#just for 2 tasks\n",
    "    \n",
    "    memory_x = torch.Tensor()\n",
    "    memory_y = torch.Tensor()\n",
    "\n",
    "    epoch_counter = 0\n",
    "    criterion = nn.CosineSimilarity(dim=1)\n",
    "    cross_loss = BarlowTwinsLoss(lambda_param= args.lambda_param, scale_loss =args.scale_loss)\n",
    "    Q = None\n",
    "    contrastive_classifier = None\n",
    "\n",
    "    cross_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    for task_id, loader in enumerate(train_data_loaders):\n",
    "        # Optimizer and Scheduler\n",
    "        model.task_id = task_id\n",
    "        init_lr = args.pretrain_base_lr*args.pretrain_batch_size/256.\n",
    "        if task_id != 0 and args.same_lr != True:\n",
    "            init_lr = init_lr / 10\n",
    "\n",
    "        project_dim = args.proj_out\n",
    "        covarince_loss = CovarianceLoss(project_dim,device=device)\n",
    "\n",
    "            \n",
    "        # optimizer = torch.optim.SGD(model.parameters(), lr=init_lr, momentum=args.pretrain_momentum, weight_decay= args.pretrain_weight_decay)\n",
    "        # scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=args.pretrain_warmup_epochs , max_epochs=args.epochs[task_id],warmup_start_lr=args.pretrain_warmup_lr,eta_min=args.min_lr) #eta_min=2e-4 is removed scheduler + values ref: infomax paper\n",
    "       \n",
    "        optimizer = LARS(model.parameters(),lr=init_lr, momentum=args.pretrain_momentum, weight_decay= args.pretrain_weight_decay, eta=0.02, clip_lr=True, exclude_bias_n_norm=True)  \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs[task_id]) #eta_min=2e-4 is removed scheduler + values ref: infomax paper\n",
    "\n",
    "        loss_ = []\n",
    "        for epoch in range(args.epochs[task_id]):\n",
    "            start = time.time()\n",
    "            model.train()\n",
    "            epoch_loss_task = []\n",
    "            epoch_loss_kd = []\n",
    "            epoch_loss_contrastive = []\n",
    "            for x1, x2, y in loader:\n",
    "                x1,x2 = x1.to(device), x2.to(device)\n",
    "                f1 = model.encoder.backbone(x1).squeeze() # NxC\n",
    "                f2 = model.encoder.backbone(x2).squeeze() # NxC\n",
    "\n",
    "\n",
    "                if task_id > 0:\n",
    "                    #samples from old tasks with labels (task ids)\n",
    "                    indices = np.random.choice(len(memory_x), size=min(args.bsize, len(memory_x)), replace=False)\n",
    "                    x_old = memory_x[indices].to(device)\n",
    "                    y_old = memory_y[indices].to(device)\n",
    "                    #samples from the new task\n",
    "                    indices = np.random.choice(len(train_data_loaders_pure[task_id].dataset), size=args.bsize, replace=False)\n",
    "                    x_new, _ =  train_data_loaders_pure[task_id].dataset[indices]\n",
    "                    x_new = x_new.to(device)\n",
    "                    y_new = torch.ones(x_new.shape[0],dtype=torch.long).to(device) * task_id          \n",
    "                    #concatenate\n",
    "                    x = torch.cat((x_old,x_new),dim=0)\n",
    "                    y = torch.cat((y_old,y_new),dim=0)\n",
    "                    \n",
    "\n",
    "                    #pass from the model\n",
    "                    encoded_vectors = model.encoder.backbone(x).squeeze() # NxC\n",
    "\n",
    "                    #do classification loss\n",
    "                    outputs = contrastive_classifier(encoded_vectors) \n",
    "                    contrastive_loss = cross_loss(outputs, y)\n",
    "\n",
    "                    \n",
    "                    f1_projected = f1 @ Q @ Q.T  \n",
    "                    f2_projected = f2 @ Q @ Q.T\n",
    "\n",
    "                else:\n",
    "                    contrastive_loss = torch.tensor(0)\n",
    "\n",
    "\n",
    "\n",
    "                z1 = model.encoder.projector(f1) # NxC\n",
    "                z2 = model.encoder.projector(f2) # NxC\n",
    "\n",
    "                z1 = F.normalize(z1, p=2)\n",
    "                z2 = F.normalize(z2, p=2)\n",
    "\n",
    "                loss_task = cross_loss(z1, z2) \n",
    "\n",
    "                if task_id != 0: #do Distillation\n",
    "                    f1Old = oldModel(x1).squeeze().detach()\n",
    "                    f2Old = oldModel(x2).squeeze().detach()\n",
    "\n",
    "                    lossKD = (-(criterion(f1_projected, f1Old).mean() * 0.5\n",
    "                                            + criterion(f2_projected, f2Old).mean() * 0.5) )\n",
    "                else:\n",
    "                    lossKD = torch.tensor(0)\n",
    "                \n",
    "\n",
    "\n",
    "                epoch_loss_task.append(loss_task.item())\n",
    "                epoch_loss_kd.append(lossKD.item())\n",
    "                epoch_loss_contrastive.append(contrastive_loss.item())\n",
    "                \n",
    "                if task_id > 0:\n",
    "                    #tune the classifier (might be optional in the future)\n",
    "                    contrastive_optimizer.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_task +  args.lambdap * lossKD + args.contrastive_ratio * contrastive_loss\n",
    "                loss.backward()\n",
    "\n",
    "                if task_id > 0:\n",
    "                    #tune the classifier (might be optional in the future)\n",
    "                    contrastive_optimizer.step()\n",
    "            \n",
    "                optimizer.step() \n",
    "                    \n",
    "            epoch_counter += 1\n",
    "            scheduler.step()\n",
    "            loss_.append(np.mean(epoch_loss_task))\n",
    "            end = time.time()\n",
    "            print('epoch end')\n",
    "            if (epoch+1) % args.knn_report_freq == 0:\n",
    "                knn_acc, task_acc_arr = Knn_Validation_cont(model, knn_train_data_loaders[:task_id+1], test_data_loaders[:task_id+1], device=device, K=200, sigma=0.5) \n",
    "                wandb.log({\" Global Knn Accuracy \": knn_acc, \" Epoch \": epoch_counter})\n",
    "                for i, acc in enumerate(task_acc_arr):\n",
    "                    wandb.log({\" Knn Accuracy Task-\"+str(i): acc, \" Epoch \": epoch_counter})\n",
    "                    print(f\" Knn Accuracy Task- {str(i)} : {acc},  Epoch : {epoch_counter}\")\n",
    "                print(f'Task {task_id:2d} | Epoch {epoch:3d} | Time:  {end-start:.1f}s  | Loss: {np.mean(epoch_loss_task):.4f} | KDLoss: {np.mean(epoch_loss_kd):.4f} | Contrastive_Loss: {np.mean(epoch_loss_contrastive):.4f}   | Knn:  {knn_acc*100:.2f}')\n",
    "                print(task_acc_arr)\n",
    "            else:\n",
    "                print(f'Task {task_id:2d} | Epoch {epoch:3d} | Time:  {end-start:.1f}s  | Loss: {np.mean(epoch_loss_task):.4f} | KDLoss: {np.mean(epoch_loss_kd):.4f} | Contrastive_Loss: {np.mean(epoch_loss_contrastive):.4f} ')\n",
    "        \n",
    "            wandb.log({\" Average Training Loss \": np.mean(epoch_loss_task), \" Epoch \": epoch_counter, \" Average KD Loss \": np.mean(epoch_loss_kd) , \" Average Contrastive Loss \": np.mean(epoch_loss_contrastive) })  \n",
    "            wandb.log({\" lr \": optimizer.param_groups[0]['lr'], \" Epoch \": epoch_counter})\n",
    "            \n",
    "\n",
    "        oldModel = deepcopy(model.encoder.backbone)  # save t-1 model\n",
    "        oldModel.to(device)\n",
    "        oldModel.eval()\n",
    "        for param in oldModel.parameters(): #Freeze old model\n",
    "            param.requires_grad = False\n",
    "\n",
    "        Q = None # each time make Q empty\n",
    "        \n",
    "        Q = extract_subspace(model, knn_train_data_loaders[task_id], rate= args.subspace_rate,device = device, Q_prev = Q, task=task_id)\n",
    "        Q = Q.to(device)\n",
    "\n",
    "        vec = get_linear_vector(model, knn_train_data_loaders[task_id], rate= args.subspace_rate, device = device, task=task_id)\n",
    "\n",
    "        new_contrastive_classifier = nn.Linear(f1.shape[1], task_id+2, bias=False).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if contrastive_classifier != None:\n",
    "                new_contrastive_classifier.weight[:task_id,:] = contrastive_classifier.weight[:task_id,:].detach().cpu()\n",
    "\n",
    "            new_contrastive_classifier.weight[task_id,:] = vec.detach().cpu()\n",
    "            new_contrastive_classifier.weight[task_id+1,:] = torch.nn.functional.normalize(new_contrastive_classifier.weight[task_id+1,:],dim=0)\n",
    "            \n",
    "\n",
    "        contrastive_classifier = new_contrastive_classifier\n",
    "        contrastive_optimizer = torch.optim.SGD(contrastive_classifier.parameters(), lr=0.001)\n",
    "\n",
    "        memory_x, memory_y = update_memory(memory_x,memory_y, train_data_loaders_pure[task_id], args.msize, task_id)\n",
    "\n",
    "        \n",
    "\n",
    "    return model, loss_, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    normalization = 'batch'\n",
    "    weight_standard = True\n",
    "    same_lr = False\n",
    "    pretrain_batch_size = 512\n",
    "    pretrain_warmup_epochs = 10\n",
    "    pretrain_warmup_lr = 3e-3\n",
    "    pretrain_base_lr = 0.03\n",
    "    pretrain_momentum = 0.9\n",
    "    pretrain_weight_decay = 5e-4\n",
    "    min_lr = 0.00\n",
    "    lambdap = 1.0\n",
    "    appr = 'barlow_PFR'\n",
    "    knn_report_freq = 5\n",
    "    cuda_device = 4\n",
    "    num_workers = 8\n",
    "    contrastive_ratio = 0.001\n",
    "    dataset = 'cifar100'\n",
    "    class_split = [25,25,25,25]\n",
    "    epochs = [5,5,500,500]\n",
    "    cov_loss_weight = 1.0\n",
    "    sim_loss_weight = 250.0\n",
    "    info_loss = 'invariance'\n",
    "    lambda_norm = 1.0\n",
    "    subspace_rate = 0.99\n",
    "    lambda_param = 5e-3\n",
    "    bsize = 32\n",
    "    msize = 150\n",
    "    proj_hidden = 2048\n",
    "    proj_out = 2048 #infomax\n",
    "    pred_hidden = 512\n",
    "    pred_out = 2048\n",
    "    scale_loss = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch end\n",
      "Task  0 | Epoch   0 | Time:  13.2s  | Loss: -343.9242 | KDLoss: 0.0000 | Contrastive_Loss: 0.0000 \n",
      "epoch end\n",
      "Task  0 | Epoch   1 | Time:  13.2s  | Loss: -344.2932 | KDLoss: 0.0000 | Contrastive_Loss: 0.0000 \n",
      "epoch end\n",
      "Task  0 | Epoch   2 | Time:  13.1s  | Loss: -343.8786 | KDLoss: 0.0000 | Contrastive_Loss: 0.0000 \n",
      "epoch end\n",
      "Task  0 | Epoch   3 | Time:  13.3s  | Loss: -344.4109 | KDLoss: 0.0000 | Contrastive_Loss: 0.0000 \n",
      "epoch end\n",
      " Knn Accuracy Task- 0 : 0.1444,  Epoch : 5\n",
      "Task  0 | Epoch   4 | Time:  13.4s  | Loss: -344.4367 | KDLoss: 0.0000 | Contrastive_Loss: 0.0000   | Knn:  14.44\n",
      "[0.1444]\n",
      "torch.Size([512, 3])\n",
      "torch.Size([512, 3])\n",
      "torch.Size([512, 3])\n",
      "epoch end\n",
      "Task  1 | Epoch   0 | Time:  17.2s  | Loss: -344.3045 | KDLoss: -0.9721 | Contrastive_Loss: 2.4481 \n",
      "epoch end\n",
      "Task  1 | Epoch   1 | Time:  17.4s  | Loss: -343.8689 | KDLoss: -0.9385 | Contrastive_Loss: 1.7123 \n",
      "epoch end\n",
      "Task  1 | Epoch   2 | Time:  17.2s  | Loss: -344.0959 | KDLoss: -0.9354 | Contrastive_Loss: 1.5365 \n",
      "epoch end\n",
      "Task  1 | Epoch   3 | Time:  17.3s  | Loss: -344.1065 | KDLoss: -0.9396 | Contrastive_Loss: 1.5961 \n",
      "epoch end\n",
      " Knn Accuracy Task- 0 : 0.1588,  Epoch : 10\n",
      " Knn Accuracy Task- 1 : 0.14,  Epoch : 10\n",
      "Task  1 | Epoch   4 | Time:  17.1s  | Loss: -344.2649 | KDLoss: -0.9407 | Contrastive_Loss: 2.1197   | Knn:  9.90\n",
      "[0.1588, 0.14]\n",
      "torch.Size([512, 1])\n",
      "torch.Size([512, 1])\n",
      "torch.Size([512, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1, 512].  Tensor sizes: [2, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2222/2832911527.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_LRD_cross_barlow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loaders_knn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_loaders_pure\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_data_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2222/2526720215.py\u001b[0m in \u001b[0;36mtrain_LRD_cross_barlow\u001b[0;34m(model, train_data_loaders, knn_train_data_loaders, train_data_loaders_pure, test_data_loaders, device, args)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontrastive_classifier\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mnew_contrastive_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontrastive_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mnew_contrastive_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (2) at non-singleton dimension 0.  Target sizes: [1, 512].  Tensor sizes: [2, 512]"
     ]
    }
   ],
   "source": [
    "model, loss, optimizer = train_LRD_cross_barlow(model, train_data_loaders, train_data_loaders_knn,train_data_loaders_pure , test_data_loaders, device, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    proj_hidden = args.proj_hidden\n",
    "    proj_out = args.proj_out\n",
    "    encoder = Encoder(hidden_dim=proj_hidden, output_dim=proj_out, normalization = args.normalization, weight_standard = args.weight_standard,appr_name=args.appr)\n",
    "    old_model = Siamese(encoder)\n",
    "    old_model.to(device) #automatically detects from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_model.load_state_dict(dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:0! (when checking argument for argument weight in method wrapper__cudnn_convolution)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44370/1036516990.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_subspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loaders_knn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubspace_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mold_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44370/446098673.py\u001b[0m in \u001b[0;36mextract_subspace\u001b[0;34m(model, loader, rate, device, Q_prev)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#out = model.encoder(x).squeeze().cpu().detach().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/Continual_Experiments/models/ssl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/Continual_Experiments/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         return F.conv2d(x, weight, self.bias, self.stride,\n\u001b[0;32m---> 35\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:0! (when checking argument for argument weight in method wrapper__cudnn_convolution)"
     ]
    }
   ],
   "source": [
    "Q = extract_subspace(old_model, train_data_loaders_knn[0], rate= args.subspace_rate,device = device,Q_prev = None)\n",
    "old_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9881, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11185/3644558521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/Continual_Experiments/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/Continual_Experiments/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         return F.conv2d(x, weight, self.bias, self.stride,\n\u001b[0;32m---> 35\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "Q = Q.to(device)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "for x1, x2, y in train_data_loaders[1]:\n",
    "    x1,x2 = x1.to(device), x2.to(device)\n",
    "    f1 = model.encoder.backbone(x1).squeeze() # NxC\n",
    "    f2 = model.encoder.backbone(x2).squeeze() # NxC\n",
    "\n",
    "    if Q != None:#let's do projection\n",
    "        f1_projected = f1 @ Q @ Q.T  \n",
    "        f2_projected = f2 @ Q @ Q.T\n",
    "\n",
    "        f1 = f1 - f1_projected\n",
    "        f2 = f2 - f2_projected\n",
    "\n",
    "        norm_loss_1 = torch.norm(f1_projected,dim =1) / (torch.norm(f1,dim =1) + 0.0000001) \n",
    "        norm_loss_1 = torch.mean(norm_loss_1)\n",
    "\n",
    "        norm_loss_2 = torch.norm(f2_projected,dim =1) / (torch.norm(f2,dim =1) + 0.0000001) \n",
    "        norm_loss_2 = torch.mean(norm_loss_2)\n",
    "\n",
    "        loss_norm = (norm_loss_1 + norm_loss_2) / 2\n",
    "        print(loss_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_acc, task_acc_arr = Knn_Validation_cont(model, train_data_loaders_knn[:1], test_data_loaders[:task_id+1], device=device, K=200, sigma=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Classifier Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin.Train Epoch: [1] Loss: 1.9388 : 100%|| 98/98 [00:09<00:00, 10.29it/s]\n",
      "Lin.Test Epoch: [1] Loss: 1.7908 ACC@1: 63.40% ACC@5: 80.53% : 100%|| 20/20 [00:02<00:00,  6.98it/s]\n",
      "Lin.Train Epoch: [2] Loss: 1.6584 : 100%|| 98/98 [00:09<00:00, 10.25it/s]\n",
      "Lin.Test Epoch: [2] Loss: 1.6033 ACC@1: 69.04% ACC@5: 85.03% : 100%|| 20/20 [00:02<00:00,  7.09it/s]\n",
      "Lin.Train Epoch: [3] Loss: 1.4899 : 100%|| 98/98 [00:09<00:00, 10.60it/s]\n",
      "Lin.Test Epoch: [3] Loss: 1.4646 ACC@1: 71.50% ACC@5: 87.95% : 100%|| 20/20 [00:02<00:00,  6.97it/s]\n",
      "Lin.Train Epoch: [4] Loss: 1.3616 : 100%|| 98/98 [00:09<00:00, 10.46it/s]\n",
      "Lin.Test Epoch: [4] Loss: 1.3554 ACC@1: 72.52% ACC@5: 90.94% : 100%|| 20/20 [00:02<00:00,  7.05it/s]\n",
      "Lin.Train Epoch: [5] Loss: 1.2603 : 100%|| 98/98 [00:09<00:00, 10.20it/s]\n",
      "Lin.Test Epoch: [5] Loss: 1.2686 ACC@1: 73.70% ACC@5: 92.30% : 100%|| 20/20 [00:02<00:00,  6.93it/s]\n",
      "Lin.Train Epoch: [6] Loss: 1.1782 : 100%|| 98/98 [00:09<00:00, 10.29it/s]\n",
      "Lin.Test Epoch: [6] Loss: 1.1970 ACC@1: 73.93% ACC@5: 93.41% : 100%|| 20/20 [00:02<00:00,  7.09it/s]\n",
      "Lin.Train Epoch: [7] Loss: 1.1106 : 100%|| 98/98 [00:09<00:00, 10.54it/s]\n",
      "Lin.Test Epoch: [7] Loss: 1.1373 ACC@1: 74.77% ACC@5: 94.59% : 100%|| 20/20 [00:02<00:00,  6.90it/s]\n",
      "Lin.Train Epoch: [8] Loss: 1.0542 : 100%|| 98/98 [00:09<00:00, 10.21it/s]\n",
      "Lin.Test Epoch: [8] Loss: 1.0876 ACC@1: 75.28% ACC@5: 95.05% : 100%|| 20/20 [00:02<00:00,  6.88it/s]\n",
      "Lin.Train Epoch: [9] Loss: 1.0064 : 100%|| 98/98 [00:09<00:00, 10.12it/s]\n",
      "Lin.Test Epoch: [9] Loss: 1.0446 ACC@1: 75.87% ACC@5: 96.11% : 100%|| 20/20 [00:02<00:00,  6.86it/s]\n",
      "Lin.Train Epoch: [10] Loss: 0.9655 : 100%|| 98/98 [00:09<00:00, 10.54it/s]\n",
      "Lin.Test Epoch: [10] Loss: 1.0076 ACC@1: 76.36% ACC@5: 96.50% : 100%|| 20/20 [00:02<00:00,  6.97it/s]\n",
      "Lin.Train Epoch: [11] Loss: 0.9301 : 100%|| 98/98 [00:09<00:00, 10.12it/s]\n",
      "Lin.Test Epoch: [11] Loss: 0.9754 ACC@1: 76.62% ACC@5: 96.91% : 100%|| 20/20 [00:02<00:00,  6.97it/s]\n",
      "Lin.Train Epoch: [12] Loss: 0.8990 : 100%|| 98/98 [00:09<00:00, 10.22it/s]\n",
      "Lin.Test Epoch: [12] Loss: 0.9474 ACC@1: 77.07% ACC@5: 97.07% : 100%|| 20/20 [00:02<00:00,  6.70it/s]\n",
      "Lin.Train Epoch: [13] Loss: 0.8718 : 100%|| 98/98 [00:09<00:00, 10.42it/s]\n",
      "Lin.Test Epoch: [13] Loss: 0.9233 ACC@1: 77.22% ACC@5: 97.18% : 100%|| 20/20 [00:02<00:00,  6.98it/s]\n",
      "Lin.Train Epoch: [14] Loss: 0.8476 : 100%|| 98/98 [00:09<00:00, 10.50it/s]\n",
      "Lin.Test Epoch: [14] Loss: 0.9013 ACC@1: 77.56% ACC@5: 97.28% : 100%|| 20/20 [00:02<00:00,  6.92it/s]\n",
      "Lin.Train Epoch: [15] Loss: 0.8260 : 100%|| 98/98 [00:09<00:00, 10.21it/s]\n",
      "Lin.Test Epoch: [15] Loss: 0.8815 ACC@1: 77.87% ACC@5: 97.41% : 100%|| 20/20 [00:03<00:00,  6.47it/s]\n",
      "Lin.Train Epoch: [16] Loss: 0.8064 : 100%|| 98/98 [00:09<00:00, 10.28it/s]\n",
      "Lin.Test Epoch: [16] Loss: 0.8636 ACC@1: 78.11% ACC@5: 97.61% : 100%|| 20/20 [00:02<00:00,  7.07it/s]\n",
      "Lin.Train Epoch: [17] Loss: 0.7889 : 100%|| 98/98 [00:09<00:00, 10.38it/s]\n",
      "Lin.Test Epoch: [17] Loss: 0.8477 ACC@1: 78.16% ACC@5: 97.64% : 100%|| 20/20 [00:02<00:00,  6.89it/s]\n",
      "Lin.Train Epoch: [18] Loss: 0.7729 : 100%|| 98/98 [00:09<00:00, 10.08it/s]\n",
      "Lin.Test Epoch: [18] Loss: 0.8326 ACC@1: 78.54% ACC@5: 97.81% : 100%|| 20/20 [00:02<00:00,  6.87it/s]\n",
      "Lin.Train Epoch: [19] Loss: 0.7582 : 100%|| 98/98 [00:09<00:00, 10.18it/s]\n",
      "Lin.Test Epoch: [19] Loss: 0.8190 ACC@1: 78.80% ACC@5: 97.93% : 100%|| 20/20 [00:03<00:00,  6.63it/s]\n",
      "Lin.Train Epoch: [20] Loss: 0.7448 : 100%|| 98/98 [00:09<00:00, 10.52it/s]\n",
      "Lin.Test Epoch: [20] Loss: 0.8069 ACC@1: 78.95% ACC@5: 98.00% : 100%|| 20/20 [00:03<00:00,  6.58it/s]\n",
      "Lin.Train Epoch: [21] Loss: 0.7325 : 100%|| 98/98 [00:09<00:00, 10.26it/s]\n",
      "Lin.Test Epoch: [21] Loss: 0.7954 ACC@1: 79.06% ACC@5: 98.06% : 100%|| 20/20 [00:02<00:00,  6.69it/s]\n",
      "Lin.Train Epoch: [22] Loss: 0.7211 : 100%|| 98/98 [00:09<00:00, 10.22it/s]\n",
      "Lin.Test Epoch: [22] Loss: 0.7851 ACC@1: 79.19% ACC@5: 98.18% : 100%|| 20/20 [00:03<00:00,  6.49it/s]\n",
      "Lin.Train Epoch: [23] Loss: 0.7105 : 100%|| 98/98 [00:09<00:00, 10.45it/s]\n",
      "Lin.Test Epoch: [23] Loss: 0.7754 ACC@1: 79.43% ACC@5: 98.21% : 100%|| 20/20 [00:03<00:00,  6.64it/s]\n",
      "Lin.Train Epoch: [24] Loss: 0.7008 : 100%|| 98/98 [00:09<00:00, 10.46it/s]\n",
      "Lin.Test Epoch: [24] Loss: 0.7665 ACC@1: 79.46% ACC@5: 98.26% : 100%|| 20/20 [00:03<00:00,  6.44it/s]\n",
      "Lin.Train Epoch: [25] Loss: 0.6916 : 100%|| 98/98 [00:09<00:00,  9.94it/s]\n",
      "Lin.Test Epoch: [25] Loss: 0.7582 ACC@1: 79.68% ACC@5: 98.27% : 100%|| 20/20 [00:03<00:00,  6.11it/s]\n",
      "Lin.Train Epoch: [26] Loss: 0.6831 : 100%|| 98/98 [00:09<00:00, 10.25it/s]\n",
      "Lin.Test Epoch: [26] Loss: 0.7504 ACC@1: 79.81% ACC@5: 98.32% : 100%|| 20/20 [00:02<00:00,  6.79it/s]\n",
      "Lin.Train Epoch: [27] Loss: 0.6751 : 100%|| 98/98 [00:09<00:00, 10.06it/s]\n",
      "Lin.Test Epoch: [27] Loss: 0.7427 ACC@1: 80.15% ACC@5: 98.38% : 100%|| 20/20 [00:02<00:00,  6.95it/s]\n",
      "Lin.Train Epoch: [28] Loss: 0.6676 : 100%|| 98/98 [00:09<00:00, 10.17it/s]\n",
      "Lin.Test Epoch: [28] Loss: 0.7360 ACC@1: 80.17% ACC@5: 98.39% : 100%|| 20/20 [00:03<00:00,  6.39it/s]\n",
      "Lin.Train Epoch: [29] Loss: 0.6608 : 100%|| 98/98 [00:09<00:00,  9.85it/s]\n",
      "Lin.Test Epoch: [29] Loss: 0.7299 ACC@1: 80.21% ACC@5: 98.39% : 100%|| 20/20 [00:03<00:00,  6.40it/s]\n",
      "Lin.Train Epoch: [30] Loss: 0.6540 : 100%|| 98/98 [00:09<00:00, 10.28it/s]\n",
      "Lin.Test Epoch: [30] Loss: 0.7243 ACC@1: 80.22% ACC@5: 98.45% : 100%|| 20/20 [00:02<00:00,  6.82it/s]\n",
      "Lin.Train Epoch: [31] Loss: 0.6479 : 100%|| 98/98 [00:09<00:00, 10.08it/s]\n",
      "Lin.Test Epoch: [31] Loss: 0.7181 ACC@1: 80.31% ACC@5: 98.41% : 100%|| 20/20 [00:02<00:00,  6.77it/s]\n",
      "Lin.Train Epoch: [32] Loss: 0.6421 : 100%|| 98/98 [00:10<00:00,  9.75it/s]\n",
      "Lin.Test Epoch: [32] Loss: 0.7127 ACC@1: 80.45% ACC@5: 98.46% : 100%|| 20/20 [00:03<00:00,  6.00it/s]\n",
      "Lin.Train Epoch: [33] Loss: 0.6366 : 100%|| 98/98 [00:09<00:00,  9.90it/s]\n",
      "Lin.Test Epoch: [33] Loss: 0.7077 ACC@1: 80.51% ACC@5: 98.46% : 100%|| 20/20 [00:03<00:00,  6.41it/s]\n",
      "Lin.Train Epoch: [34] Loss: 0.6314 : 100%|| 98/98 [00:09<00:00,  9.85it/s]\n",
      "Lin.Test Epoch: [34] Loss: 0.7027 ACC@1: 80.56% ACC@5: 98.51% : 100%|| 20/20 [00:03<00:00,  6.57it/s]\n",
      "Lin.Train Epoch: [35] Loss: 0.6263 : 100%|| 98/98 [00:09<00:00, 10.15it/s]\n",
      "Lin.Test Epoch: [35] Loss: 0.6988 ACC@1: 80.67% ACC@5: 98.48% : 100%|| 20/20 [00:03<00:00,  6.61it/s]\n",
      "Lin.Train Epoch: [36] Loss: 0.6218 : 100%|| 98/98 [00:09<00:00, 10.27it/s]\n",
      "Lin.Test Epoch: [36] Loss: 0.6941 ACC@1: 80.67% ACC@5: 98.55% : 100%|| 20/20 [00:03<00:00,  6.36it/s]\n",
      "Lin.Train Epoch: [37] Loss: 0.6174 : 100%|| 98/98 [00:09<00:00,  9.95it/s]\n",
      "Lin.Test Epoch: [37] Loss: 0.6902 ACC@1: 80.72% ACC@5: 98.49% : 100%|| 20/20 [00:03<00:00,  6.23it/s]\n",
      "Lin.Train Epoch: [38] Loss: 0.6131 : 100%|| 98/98 [00:10<00:00,  9.67it/s]\n",
      "Lin.Test Epoch: [38] Loss: 0.6862 ACC@1: 80.71% ACC@5: 98.58% : 100%|| 20/20 [00:03<00:00,  5.89it/s]\n",
      "Lin.Train Epoch: [39] Loss: 0.6091 : 100%|| 98/98 [00:09<00:00, 10.19it/s]\n",
      "Lin.Test Epoch: [39] Loss: 0.6827 ACC@1: 80.85% ACC@5: 98.59% : 100%|| 20/20 [00:03<00:00,  6.00it/s]\n",
      "Lin.Train Epoch: [40] Loss: 0.6054 : 100%|| 98/98 [00:09<00:00,  9.84it/s]\n",
      "Lin.Test Epoch: [40] Loss: 0.6792 ACC@1: 80.97% ACC@5: 98.58% : 100%|| 20/20 [00:03<00:00,  5.93it/s]\n",
      "Lin.Train Epoch: [41] Loss: 0.6018 : 100%|| 98/98 [00:10<00:00,  9.79it/s]\n",
      "Lin.Test Epoch: [41] Loss: 0.6761 ACC@1: 80.95% ACC@5: 98.61% : 100%|| 20/20 [00:03<00:00,  5.76it/s]\n",
      "Lin.Train Epoch: [42] Loss: 0.6130 :   3%|         | 3/98 [00:02<01:07,  1.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44370/2438117765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m test_loss, test_acc1, test_acc5, classifier = linear_evaluation(model, train_data_loaders_knn_all[0],\n\u001b[1;32m     12\u001b[0m                                                                     \u001b[0mtest_data_loaders_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlin_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                                                     lin_scheduler, epochs=lin_epoch, device=device) \n\u001b[0m",
      "\u001b[0;32m~/CI_SSL/utils/eval_metrics.py\u001b[0m in \u001b[0;36mlinear_evaluation\u001b[0;34m(net, data_loader, test_data_loader, train_optimizer, classifier, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mlinear_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_acc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_acc5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;31m# Testing for linear evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/utils/eval_metrics.py\u001b[0m in \u001b[0;36mlinear_train\u001b[0;34m(net, data_loader, train_optimizer, classifier, scheduler, epoch, device)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatchsize_bc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test Linear classification acc\n",
    "print(\"Starting Classifier Training..\")\n",
    "lin_epoch = 100\n",
    "if args.dataset == 'cifar10':\n",
    "    classifier = LinearClassifier(num_classes = 10).to(device)\n",
    "elif args.dataset == 'cifar100':\n",
    "    classifier = LinearClassifier(num_classes = 100).to(device)\n",
    "\n",
    "lin_optimizer = torch.optim.SGD(classifier.parameters(), 0.1, momentum=0.9) # Infomax: no weight decay, epoch 100, cosine scheduler\n",
    "lin_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(lin_optimizer, lin_epoch, eta_min=2e-4) #scheduler + values ref: infomax paper\n",
    "test_loss, test_acc1, test_acc5, classifier = linear_evaluation(model, train_data_loaders_knn_all[0],\n",
    "                                                                    test_data_loaders_all[0],lin_optimizer, classifier, \n",
    "                                                                    lin_scheduler, epochs=lin_epoch, device=device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedml_academic",
   "language": "python",
   "name": "fedml_academic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
