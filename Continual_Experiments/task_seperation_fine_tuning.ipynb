{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duygu/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"\")))\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "\n",
    "from dataloaders.dataloader_cifar10 import get_cifar10\n",
    "from dataloaders.dataloader_cifar100 import get_cifar100\n",
    "from utils.eval_metrics import linear_evaluation, get_t_SNE_plot\n",
    "from models.linear_classifer import LinearClassifier\n",
    "from models.ssl import  SimSiam, Siamese, Encoder, Predictor\n",
    "\n",
    "from trainers.train_simsiam import train_simsiam\n",
    "from trainers.train_infomax import train_infomax\n",
    "from trainers.train_barlow import train_barlow\n",
    "\n",
    "from trainers.train_PFR import train_PFR_simsiam\n",
    "from trainers.train_PFR_contrastive import train_PFR_contrastive_simsiam\n",
    "from trainers.train_contrastive import train_contrastive_simsiam\n",
    "from trainers.train_ering import train_ering_simsiam\n",
    "\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from utils.lr_schedulers import LinearWarmupCosineAnnealingLR, SimSiamScheduler\n",
    "from utils.eval_metrics import Knn_Validation_cont\n",
    "from copy import deepcopy\n",
    "from loss import invariance_loss,CovarianceLoss,ErrorCovarianceLoss\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from models.linear_classifer import LinearClassifier\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloaders.dataset import TensorDataset\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[0.1, 2.0]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = torchvision.transforms.functional.gaussian_blur(x,kernel_size=[3,3],sigma=sigma)#kernel size and sigma are open problems but right now seems ok!\n",
    "        return x\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    normalization = 'batch'\n",
    "    weight_standard = False\n",
    "    same_lr = False\n",
    "    pretrain_batch_size = 512\n",
    "    pretrain_warmup_epochs = 10\n",
    "    pretrain_warmup_lr = 3e-3\n",
    "    pretrain_base_lr = 0.03\n",
    "    pretrain_momentum = 0.9\n",
    "    pretrain_weight_decay = 5e-4\n",
    "    min_lr = 0.00\n",
    "    lambdap = 1.0\n",
    "    appr = 'barlow_PFR'\n",
    "    knn_report_freq = 10\n",
    "    cuda_device = 5\n",
    "    num_workers = 8\n",
    "    contrastive_ratio = 0.001\n",
    "    dataset = 'cifar100'\n",
    "    class_split = [20,20,20,20,20]\n",
    "    epochs = [500,500,500,500,500]\n",
    "    cov_loss_weight = 1.0\n",
    "    sim_loss_weight = 250.0\n",
    "    info_loss = 'invariance'\n",
    "    lambda_norm = 1.0\n",
    "    subspace_rate = 0.99\n",
    "    lambda_param = 5e-3\n",
    "    bsize = 32\n",
    "    msize = 150\n",
    "    proj_hidden = 2048\n",
    "    proj_out = 2048 #infomax 64\n",
    "    pred_hidden = 512\n",
    "    pred_out = 2048\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "    get_dataloaders = get_cifar10\n",
    "    num_classes=10\n",
    "elif args.dataset == \"cifar100\":\n",
    "    get_dataloaders = get_cifar100\n",
    "    num_classes=100\n",
    "assert sum(args.class_split) == num_classes\n",
    "assert len(args.class_split) == len(args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "num_worker = args.num_workers\n",
    "#device\n",
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb init\n",
    "wandb.init(project=\"CSSL\",  entity=\"yavuz-team\",\n",
    "            mode=\"disabled\",\n",
    "            config=args,\n",
    "            name= str(args.dataset) + '-algo' + str(args.appr) + \"-e\" + str(args.epochs) + \"-b\" \n",
    "            + str(args.pretrain_batch_size) + \"-lr\" + str(args.pretrain_base_lr)+\"-CS\"+str(args.class_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    transform = T.Compose([\n",
    "            T.RandomResizedCrop(size=32, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)]), p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([GaussianBlur()], p=0.5), \n",
    "            T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])])\n",
    "\n",
    "    transform_prime = T.Compose([\n",
    "            T.RandomResizedCrop(size=32, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)]), p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([GaussianBlur()], p=0.5), \n",
    "            T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataloaders..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Dataloaders\n",
    "print(\"Creating Dataloaders..\")\n",
    "#Class Based\n",
    "train_data_loaders, train_data_loaders_knn, test_data_loaders, _, train_data_loaders_linear, train_data_loaders_pure, train_data_loaders_generic  = get_dataloaders(transform, transform_prime, \\\n",
    "                                    classes=args.class_split, valid_rate = 0.00, batch_size=args.pretrain_batch_size, seed = 0, num_worker= num_worker)\n",
    "_, train_data_loaders_knn_all, test_data_loaders_all, _, train_data_loaders_linear_all, train_data_loaders_pure_all, _ = get_dataloaders(transform, transform_prime, \\\n",
    "                                        classes=[num_classes], valid_rate = 0.00, batch_size=args.pretrain_batch_size, seed = 0, num_worker= num_worker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    proj_hidden = args.proj_hidden\n",
    "    proj_out = args.proj_out\n",
    "    encoder = Encoder(hidden_dim=proj_hidden, output_dim=proj_out, normalization = args.normalization, weight_standard = args.weight_standard, appr_name = args.appr)\n",
    "    model = Siamese(encoder)\n",
    "    model.to(device) #automatically detects from model\n",
    "#load model here\n",
    "file_name = \"checkpoints/checkpoint_cifar100-algocassle_contrastive_v3_barlow-e[500, 500, 500, 500, 500]-b256-lr0.1-CS[20, 20, 20, 20, 20]acc_59.199999999999996.pth.tar\"\n",
    "dict = torch.load(file_name)\n",
    "model.temporal_projector = nn.Sequential(\n",
    "            nn.Linear(args.proj_out, args.proj_hidden, bias=False),\n",
    "            nn.BatchNorm1d(args.proj_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.proj_hidden, args.proj_out),\n",
    "        ).to(device)\n",
    "model.contrastive_projector =  nn.Linear(512, len(train_data_loaders_generic), bias=False).to(device)\n",
    "model.load_state_dict(dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_eigenvector(model, loader):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        out = model(x).cpu().detach().numpy()\n",
    "        outs.append(out)\n",
    "\n",
    "    outs = np.concatenate(outs)\n",
    "    outs = outs.transpose()\n",
    "    outs = torch.tensor(outs)\n",
    "\n",
    "    U, S, V = torch.svd(outs)\n",
    "    return U[0:1,:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_top_k(outputs, targets, top_k=(1,5)):\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argsort(outputs, dim=-1, descending=True)\n",
    "        result= []\n",
    "        for k in top_k:\n",
    "            correct_k = torch.sum((prediction[:, 0:k] == targets.unsqueeze(dim=-1)).any(dim=-1).float()).item() \n",
    "            result.append(correct_k)\n",
    "        return result\n",
    "    \n",
    "def contrastive_train_first_task(net, data_loader, task_id, optimizer, classifier, scheduler, epochs, device):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.eval() # for not update batchnorm \n",
    "        total_num, train_bar = 0, tqdm(data_loader)\n",
    "        linear_loss = 0.0\n",
    "        for data_tuple in train_bar:\n",
    "            # Forward prop of the model with single augmented batch\n",
    "            pos_1, targets = data_tuple\n",
    "            pos_1 = pos_1.to(device)\n",
    "            features = net(pos_1)\n",
    "\n",
    "            # Batchsize\n",
    "            batchsize_bc = features.shape[0]\n",
    "            targets = torch.ones(targets.shape[0],dtype=torch.long).to(device) * task_id \n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            c_weights = torch.nn.functional.normalize(classifier.weight,dim=1)\n",
    "            \n",
    "            logits = features.detach() @ c_weights.T\n",
    "            #classifier(features.detach()) \n",
    "\n",
    "            # Cross Entropy Loss \n",
    "            linear_loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # Backpropagation part\n",
    "            optimizer.zero_grad()\n",
    "            linear_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulating number of examples, losses and correct predictions\n",
    "            total_num += batchsize_bc\n",
    "            linear_loss += linear_loss.item() * batchsize_bc\n",
    "\n",
    "            train_bar.set_description('Lin.Train Epoch: [{}] Loss: {:.4f}'.format(epoch, linear_loss / total_num))\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        # wandb.log({\" Linear Layer Train Loss \": linear_loss / total_num, \" Epoch \": epoch})\n",
    "    return linear_loss/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_train(net, data_loader, task_id, optimizer, scheduler, epochs, device):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.eval() # for not update batchnorm \n",
    "        total_num, train_bar = 0, tqdm(data_loader)\n",
    "        linear_loss = 0.0\n",
    "        for data_tuple in train_bar:\n",
    "            # Forward prop of the model with single augmented batch\n",
    "            pos_1, targets = data_tuple\n",
    "            pos_1 = pos_1.to(device)\n",
    "            features = net(pos_1)\n",
    "            \n",
    "            #logits = net.contrastive_projector(features) \n",
    "            \n",
    "            c_weights = torch.nn.functional.normalize(net.contrastive_projector.weight,dim=1)\n",
    "                \n",
    "            logits = features @ c_weights.T\n",
    "\n",
    "            # Batchsize\n",
    "            batchsize_bc = features.shape[0]\n",
    "            targets = torch.ones(targets.shape[0],dtype=torch.long).to(device) * task_id \n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Cross Entropy Loss \n",
    "            linear_loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # Backpropagation part\n",
    "            optimizer.zero_grad()\n",
    "            linear_loss.backward()\n",
    "            net.contrastive_projector.weight.grad[0:task_id] = torch.zeros(net.contrastive_projector.weight.grad[0:task_id].shape).to(device)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulating number of examples, losses and correct predictions\n",
    "            total_num += batchsize_bc\n",
    "            linear_loss += linear_loss.item() * batchsize_bc\n",
    "\n",
    "            train_bar.set_description('Lin.Train Epoch: [{}] Loss: {:.4f}'.format(epoch, linear_loss / total_num))\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        # wandb.log({\" Linear Layer Train Loss \": linear_loss / total_num, \" Epoch \": epoch})\n",
    "    return linear_loss/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin.Train Epoch: [1] Loss: 0.0000: 100%|██████████| 20/20 [00:03<00:00,  5.31it/s]\n",
      "Lin.Train Epoch: [2] Loss: 0.0000: 100%|██████████| 20/20 [00:03<00:00,  5.71it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]Exception ignored in: <function _releaseLock at 0x7f98779bba70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/duygu/anaconda3/envs/fedml_academic/lib/python3.7/logging/__init__.py\", line 221, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt\n",
      "Lin.Train Epoch: [3] Loss: 0.0000:  95%|█████████▌| 19/20 [00:03<00:00,  8.71it/s]Exception ignored in: <function Socket.__del__ at 0x7f9876df1ef0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/duygu/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/zmq/sugar/socket.py\", line 110, in __del__\n",
      "    def __del__(self):\n",
      "KeyboardInterrupt\n",
      "Lin.Train Epoch: [3] Loss: 0.0000: 100%|██████████| 20/20 [00:03<00:00,  5.32it/s]\n",
      "Lin.Train Epoch: [4] Loss: 0.0000: 100%|██████████| 20/20 [00:03<00:00,  5.48it/s]\n",
      "Lin.Train Epoch: [5] Loss: 0.0000: 100%|██████████| 20/20 [00:03<00:00,  5.49it/s]\n",
      "Lin.Train Epoch: [6] Loss: 0.0000:  85%|████████▌ | 17/20 [00:03<00:00,  5.43it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_86180/2379935409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# lin_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(lin_optimizer, lin_epoch, eta_min=0.002)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m test_loss = contrastive_train_first_task(model, train_data_loaders_generic[task_id], task_id, lin_optimizer, \n\u001b[0;32m----> 8\u001b[0;31m                                                             contrastive_projector, None, epochs=lin_epoch, device=device) \n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_86180/988372737.py\u001b[0m in \u001b[0;36mcontrastive_train_first_task\u001b[0;34m(net, data_loader, task_id, optimizer, classifier, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtotal_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mlinear_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_tuple\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Forward prop of the model with single augmented batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpos_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "contrastive_projector =  nn.Linear(512, 2).to(device)\n",
    "task_id = 0\n",
    "lin_epoch= 10\n",
    "lin_optimizer = torch.optim.SGD(contrastive_projector.parameters(), 1e-3, momentum=0.9, weight_decay=0) \n",
    "# lin_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(lin_optimizer, lin_epoch, eta_min=0.002) \n",
    "test_loss = contrastive_train_first_task(model, train_data_loaders_linear[task_id], task_id, lin_optimizer, \n",
    "                                                            contrastive_projector, None, epochs=lin_epoch, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    proj_hidden = args.proj_hidden\n",
    "    proj_out = args.proj_out\n",
    "    encoder = Encoder(hidden_dim=proj_hidden, output_dim=proj_out, normalization = args.normalization, weight_standard = args.weight_standard, appr_name = args.appr)\n",
    "    model2 = Siamese(encoder)\n",
    "    model2.to(device) #automatically detects from model\n",
    "#load model here\n",
    "file_name = \"./checkpoints/checkpoint_cifar100-algocassle_barlow-e[500, 500, 500, 500, 500]-b256-lr0.25-CS[20, 20, 20, 20, 20]_task_1_same_lr_True_norm_batch_ws_False.pth.tar\"\n",
    "dict = torch.load(file_name)\n",
    "model2.temporal_projector = nn.Sequential(\n",
    "            nn.Linear(args.proj_out, args.proj_hidden, bias=False),\n",
    "            nn.BatchNorm1d(args.proj_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.proj_hidden, args.proj_out),\n",
    "        ).to(device)\n",
    "model2.load_state_dict(dict['state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_top_k(outputs, targets, top_k=(1,5)):\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argsort(outputs, dim=-1, descending=True)\n",
    "        result= []\n",
    "        for k in top_k:\n",
    "            correct_k = torch.sum((prediction[:, 0:k] == targets.unsqueeze(dim=-1)).any(dim=-1).float()).item() \n",
    "            result.append(correct_k)\n",
    "        return result\n",
    "\n",
    "def linear_test(net, data_loader, classifier, epoch, device, task_num):\n",
    "    # evaluate model:\n",
    "    net.eval() # for not update batchnorm\n",
    "    linear_loss = 0.0\n",
    "    num = 0\n",
    "    total_loss, total_correct_1, total_num, test_bar = 0.0, 0.0, 0, tqdm(data_loader)\n",
    "    with torch.no_grad():\n",
    "        for data_tuple in test_bar:\n",
    "            data, target = [t.to(device) for t in data_tuple]\n",
    "            output = net(data)\n",
    "            if classifier is not None:  #else net is already a classifier\n",
    "                output = classifier(output) \n",
    "            linear_loss = F.cross_entropy(output, target)\n",
    "            \n",
    "            # Batchsize for loss and accuracy\n",
    "            num = data.size(0)\n",
    "            total_num += num \n",
    "            total_loss += linear_loss.item() * num \n",
    "            # Accumulating number of correct predictions \n",
    "            correct_top_1 = correct_top_k(output, target, top_k=[1])    \n",
    "            total_correct_1 += correct_top_1[0]\n",
    "            test_bar.set_description('Lin.Test Epoch: [{}] Loss: {:.4f} ACC: {:.2f}% '\n",
    "                                     .format(epoch,  total_loss / total_num,\n",
    "                                             total_correct_1 / total_num * 100\n",
    "                                             ))\n",
    "        acc_1 = total_correct_1/total_num*100\n",
    "        wandb.log({f\" {task_num} Linear Layer Test Loss \": linear_loss / total_num, \"Linear Epoch \": epoch})\n",
    "        wandb.log({f\" {task_num} Linear Layer Test - Acc\": acc_1, \"Linear Epoch \": epoch})\n",
    "    return total_loss / total_num, acc_1  \n",
    "\n",
    "def linear_train(net, data_loader, train_optimizer, classifier, scheduler, epoch, device, task_num):\n",
    "\n",
    "    net.eval() # for not update batchnorm \n",
    "    total_num, train_bar = 0, tqdm(data_loader)\n",
    "    linear_loss = 0.0\n",
    "    total_correct_1 = 0.0\n",
    "    for data_tuple in train_bar:\n",
    "        # Forward prop of the model with single augmented batch\n",
    "        pos_1, target = data_tuple\n",
    "        pos_1 = pos_1.to(device)\n",
    "        feature_1 = net(pos_1)\n",
    "        # Batchsize\n",
    "        batchsize_bc = feature_1.shape[0]\n",
    "        features = feature_1\n",
    "        targets = target.to(device)\n",
    "        logits = classifier(features.detach()) \n",
    "        # Cross Entropy Loss \n",
    "        linear_loss_1 = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # Number of correct predictions\n",
    "        linear_correct_1 = correct_top_k(logits, targets, top_k=[1])\n",
    "    \n",
    "        # Backpropagation part\n",
    "        train_optimizer.zero_grad()\n",
    "        linear_loss_1.backward()\n",
    "        train_optimizer.step()\n",
    "\n",
    "        # Accumulating number of examples, losses and correct predictions\n",
    "        total_num += batchsize_bc\n",
    "        linear_loss += linear_loss_1.item() * batchsize_bc\n",
    "        total_correct_1 += linear_correct_1[0] \n",
    "\n",
    "        acc_1 = total_correct_1/total_num*100\n",
    "        # # This bar is used for live tracking on command line (batch_size -> batchsize_bc: to show current batchsize )\n",
    "        train_bar.set_description('Lin.Train Epoch: [{}] Loss: {:.4f} ACC: {:.2f}'.format(\\\n",
    "                epoch, linear_loss / total_num, acc_1))\n",
    "    scheduler.step()\n",
    "    acc_1 = total_correct_1/total_num*100   \n",
    "    wandb.log({f\" {task_num} Linear Layer Train Loss \": linear_loss / total_num, \"Linear Epoch \": epoch})\n",
    "    wandb.log({f\" {task_num} Linear Layer Train - Acc\": acc_1, \"Linear Epoch \": epoch})\n",
    "        \n",
    "    return linear_loss/total_num, acc_1\n",
    "\n",
    "\n",
    "def linear_evaluation(net, data_loaders,test_data_loaders,train_optimizer,classifier, scheduler, epochs, device, task_num):\n",
    "    train_X = torch.Tensor([])\n",
    "    train_Y = torch.tensor([],dtype=int)\n",
    "    for loader in data_loaders:\n",
    "        train_X = torch.cat((train_X, loader.dataset.train_data), dim=0)\n",
    "        train_Y = torch.cat((train_Y, loader.dataset.label_data), dim=0)\n",
    "    data_loader = DataLoader(TensorDataset(train_X, train_Y,transform=data_loaders[0].dataset.transform), batch_size=256, shuffle=True, num_workers = 5, pin_memory=True)\n",
    "\n",
    "    test_X = torch.Tensor([])\n",
    "    test_Y = torch.tensor([],dtype=int)\n",
    "    for loader in test_data_loaders:\n",
    "        test_X = torch.cat((test_X, loader.dataset.train_data), dim=0)\n",
    "        test_Y = torch.cat((test_Y, loader.dataset.label_data), dim=0)\n",
    "    test_data_loader = DataLoader(TensorDataset(test_X, test_Y,transform=test_data_loaders[0].dataset.transform), batch_size=256, shuffle=True, num_workers = 5, pin_memory=True)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        linear_train(net,data_loader,train_optimizer,classifier,scheduler, epoch, device, task_num)\n",
    "        with torch.no_grad():\n",
    "            # Testing for linear evaluation\n",
    "            test_loss, test_acc1 = linear_test(net, test_data_loader, classifier, epoch, device, task_num)\n",
    "\n",
    "    return test_loss, test_acc1, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin.Train Epoch: [1] Loss: 3.3257 ACC: 43.93: 100%|██████████| 79/79 [00:03<00:00, 21.00it/s]\n",
      "Lin.Test Epoch: [1] Loss: 3.4347 ACC: 47.48% : 100%|██████████| 16/16 [00:01<00:00, 12.17it/s]\n",
      "Lin.Train Epoch: [2] Loss: 3.3782 ACC: 48.93: 100%|██████████| 79/79 [00:03<00:00, 20.80it/s]\n",
      "Lin.Test Epoch: [2] Loss: 2.6580 ACC: 53.70% : 100%|██████████| 16/16 [00:01<00:00, 11.61it/s]\n",
      "Lin.Train Epoch: [3] Loss: 3.0190 ACC: 51.44: 100%|██████████| 79/79 [00:03<00:00, 20.93it/s]\n",
      "Lin.Test Epoch: [3] Loss: 2.6857 ACC: 55.33% : 100%|██████████| 16/16 [00:01<00:00, 11.48it/s]\n",
      "Lin.Train Epoch: [4] Loss: 2.9855 ACC: 52.05: 100%|██████████| 79/79 [00:03<00:00, 19.92it/s]\n",
      "Lin.Test Epoch: [4] Loss: 2.8573 ACC: 54.02% : 100%|██████████| 16/16 [00:01<00:00, 11.68it/s]\n",
      "Lin.Train Epoch: [5] Loss: 3.0599 ACC: 52.29: 100%|██████████| 79/79 [00:04<00:00, 19.74it/s]\n",
      "Lin.Test Epoch: [5] Loss: 3.1281 ACC: 52.88% : 100%|██████████| 16/16 [00:01<00:00, 10.30it/s]\n",
      "Lin.Train Epoch: [6] Loss: 3.1692 ACC: 52.26: 100%|██████████| 79/79 [00:04<00:00, 19.73it/s]\n",
      "Lin.Test Epoch: [6] Loss: 3.3627 ACC: 51.70% : 100%|██████████| 16/16 [00:01<00:00, 10.02it/s]\n",
      "Lin.Train Epoch: [7] Loss: 3.0549 ACC: 52.51: 100%|██████████| 79/79 [00:04<00:00, 19.59it/s]\n",
      "Lin.Test Epoch: [7] Loss: 2.9083 ACC: 54.55% : 100%|██████████| 16/16 [00:01<00:00, 11.22it/s]\n",
      "Lin.Train Epoch: [8] Loss: 3.0792 ACC: 53.31: 100%|██████████| 79/79 [00:03<00:00, 19.89it/s]\n",
      "Lin.Test Epoch: [8] Loss: 2.8671 ACC: 55.55% : 100%|██████████| 16/16 [00:01<00:00, 11.68it/s]\n",
      "Lin.Train Epoch: [9] Loss: 2.9977 ACC: 53.81: 100%|██████████| 79/79 [00:03<00:00, 20.69it/s]\n",
      "Lin.Test Epoch: [9] Loss: 2.7414 ACC: 57.03% : 100%|██████████| 16/16 [00:01<00:00, 11.92it/s]\n",
      "Lin.Train Epoch: [10] Loss: 2.9589 ACC: 53.90: 100%|██████████| 79/79 [00:03<00:00, 21.39it/s]\n",
      "Lin.Test Epoch: [10] Loss: 2.9964 ACC: 53.73% : 100%|██████████| 16/16 [00:01<00:00, 11.35it/s]\n",
      "Lin.Train Epoch: [11] Loss: 3.0025 ACC: 53.91: 100%|██████████| 79/79 [00:03<00:00, 20.52it/s]\n",
      "Lin.Test Epoch: [11] Loss: 3.0925 ACC: 54.00% : 100%|██████████| 16/16 [00:01<00:00, 11.73it/s]\n",
      "Lin.Train Epoch: [12] Loss: 3.0381 ACC: 53.62: 100%|██████████| 79/79 [00:03<00:00, 20.82it/s]\n",
      "Lin.Test Epoch: [12] Loss: 3.4392 ACC: 53.30% : 100%|██████████| 16/16 [00:01<00:00, 11.03it/s]\n",
      "Lin.Train Epoch: [13] Loss: 2.9929 ACC: 54.37: 100%|██████████| 79/79 [00:03<00:00, 20.27it/s]\n",
      "Lin.Test Epoch: [13] Loss: 2.4941 ACC: 58.73% : 100%|██████████| 16/16 [00:01<00:00, 11.29it/s]\n",
      "Lin.Train Epoch: [14] Loss: 3.0450 ACC: 54.04: 100%|██████████| 79/79 [00:03<00:00, 20.14it/s]\n",
      "Lin.Test Epoch: [14] Loss: 2.6528 ACC: 57.40% : 100%|██████████| 16/16 [00:01<00:00, 11.56it/s]\n",
      "Lin.Train Epoch: [15] Loss: 2.9402 ACC: 54.86: 100%|██████████| 79/79 [00:03<00:00, 20.47it/s]\n",
      "Lin.Test Epoch: [15] Loss: 3.1404 ACC: 54.35% : 100%|██████████| 16/16 [00:01<00:00, 11.21it/s]\n",
      "Lin.Train Epoch: [16] Loss: 2.8164 ACC: 55.54: 100%|██████████| 79/79 [00:04<00:00, 19.61it/s]\n",
      "Lin.Test Epoch: [16] Loss: 2.6966 ACC: 55.35% : 100%|██████████| 16/16 [00:01<00:00, 12.15it/s]\n",
      "Lin.Train Epoch: [17] Loss: 2.8909 ACC: 54.11: 100%|██████████| 79/79 [00:03<00:00, 20.26it/s]\n",
      "Lin.Test Epoch: [17] Loss: 3.0459 ACC: 53.00% : 100%|██████████| 16/16 [00:01<00:00, 11.74it/s]\n",
      "Lin.Train Epoch: [18] Loss: 2.9902 ACC: 54.29: 100%|██████████| 79/79 [00:03<00:00, 21.31it/s]\n",
      "Lin.Test Epoch: [18] Loss: 3.0377 ACC: 55.00% : 100%|██████████| 16/16 [00:01<00:00, 12.21it/s]\n",
      "Lin.Train Epoch: [19] Loss: 2.8636 ACC: 54.62: 100%|██████████| 79/79 [00:03<00:00, 20.15it/s]\n",
      "Lin.Test Epoch: [19] Loss: 2.6944 ACC: 57.45% : 100%|██████████| 16/16 [00:01<00:00, 10.53it/s]\n",
      "Lin.Train Epoch: [20] Loss: 2.8476 ACC: 54.74: 100%|██████████| 79/79 [00:03<00:00, 20.66it/s]\n",
      "Lin.Test Epoch: [20] Loss: 2.9749 ACC: 54.85% : 100%|██████████| 16/16 [00:01<00:00, 11.51it/s]\n",
      "Lin.Train Epoch: [21] Loss: 2.9736 ACC: 54.21: 100%|██████████| 79/79 [00:03<00:00, 20.25it/s]\n",
      "Lin.Test Epoch: [21] Loss: 3.0416 ACC: 56.25% : 100%|██████████| 16/16 [00:01<00:00, 11.09it/s]\n",
      "Lin.Train Epoch: [22] Loss: 2.8464 ACC: 55.18: 100%|██████████| 79/79 [00:03<00:00, 20.17it/s]\n",
      "Lin.Test Epoch: [22] Loss: 3.2126 ACC: 53.33% : 100%|██████████| 16/16 [00:01<00:00, 10.85it/s]\n",
      "Lin.Train Epoch: [23] Loss: 3.1480 ACC: 54.32: 100%|██████████| 79/79 [00:03<00:00, 19.92it/s]\n",
      "Lin.Test Epoch: [23] Loss: 2.4987 ACC: 59.45% : 100%|██████████| 16/16 [00:01<00:00, 11.47it/s]\n",
      "Lin.Train Epoch: [24] Loss: 2.7844 ACC: 55.16: 100%|██████████| 79/79 [00:03<00:00, 20.87it/s]\n",
      "Lin.Test Epoch: [24] Loss: 2.5537 ACC: 57.57% : 100%|██████████| 16/16 [00:01<00:00, 10.71it/s]\n",
      "Lin.Train Epoch: [25] Loss: 2.7347 ACC: 55.35: 100%|██████████| 79/79 [00:04<00:00, 19.45it/s]\n",
      "Lin.Test Epoch: [25] Loss: 2.3757 ACC: 59.05% : 100%|██████████| 16/16 [00:01<00:00, 11.95it/s]\n",
      "Lin.Train Epoch: [26] Loss: 2.6138 ACC: 56.04: 100%|██████████| 79/79 [00:03<00:00, 20.80it/s]\n",
      "Lin.Test Epoch: [26] Loss: 2.3127 ACC: 59.08% : 100%|██████████| 16/16 [00:01<00:00, 11.44it/s]\n",
      "Lin.Train Epoch: [27] Loss: 2.6892 ACC: 55.71: 100%|██████████| 79/79 [00:03<00:00, 21.46it/s]\n",
      "Lin.Test Epoch: [27] Loss: 2.4689 ACC: 57.95% : 100%|██████████| 16/16 [00:01<00:00, 11.03it/s]\n",
      "Lin.Train Epoch: [28] Loss: 2.6322 ACC: 55.66: 100%|██████████| 79/79 [00:03<00:00, 20.37it/s]\n",
      "Lin.Test Epoch: [28] Loss: 2.3038 ACC: 58.58% : 100%|██████████| 16/16 [00:01<00:00, 11.19it/s]\n",
      "Lin.Train Epoch: [29] Loss: 2.5233 ACC: 55.84: 100%|██████████| 79/79 [00:03<00:00, 20.48it/s]\n",
      "Lin.Test Epoch: [29] Loss: 2.4927 ACC: 57.63% : 100%|██████████| 16/16 [00:01<00:00, 11.83it/s]\n",
      "Lin.Train Epoch: [30] Loss: 2.5662 ACC: 56.06: 100%|██████████| 79/79 [00:04<00:00, 19.68it/s]\n",
      "Lin.Test Epoch: [30] Loss: 2.4657 ACC: 56.77% : 100%|██████████| 16/16 [00:01<00:00, 10.24it/s]\n",
      "Lin.Train Epoch: [31] Loss: 2.5681 ACC: 56.24: 100%|██████████| 79/79 [00:03<00:00, 20.10it/s]\n",
      "Lin.Test Epoch: [31] Loss: 2.2054 ACC: 58.65% : 100%|██████████| 16/16 [00:01<00:00, 11.00it/s]\n",
      "Lin.Train Epoch: [32] Loss: 2.6041 ACC: 56.02: 100%|██████████| 79/79 [00:03<00:00, 19.76it/s]\n",
      "Lin.Test Epoch: [32] Loss: 2.4270 ACC: 59.40% : 100%|██████████| 16/16 [00:01<00:00, 11.32it/s]\n",
      "Lin.Train Epoch: [33] Loss: 2.5341 ACC: 55.99: 100%|██████████| 79/79 [00:03<00:00, 19.97it/s]\n",
      "Lin.Test Epoch: [33] Loss: 2.4325 ACC: 57.03% : 100%|██████████| 16/16 [00:01<00:00, 10.62it/s]\n",
      "Lin.Train Epoch: [34] Loss: 2.5134 ACC: 56.02: 100%|██████████| 79/79 [00:03<00:00, 19.98it/s]\n",
      "Lin.Test Epoch: [34] Loss: 2.4090 ACC: 57.40% : 100%|██████████| 16/16 [00:01<00:00, 11.98it/s]\n",
      "Lin.Train Epoch: [35] Loss: 2.5951 ACC: 55.97: 100%|██████████| 79/79 [00:03<00:00, 20.96it/s]\n",
      "Lin.Test Epoch: [35] Loss: 2.1380 ACC: 60.38% : 100%|██████████| 16/16 [00:01<00:00, 12.05it/s]\n",
      "Lin.Train Epoch: [36] Loss: 2.3270 ACC: 56.98: 100%|██████████| 79/79 [00:03<00:00, 21.11it/s]\n",
      "Lin.Test Epoch: [36] Loss: 2.3582 ACC: 58.85% : 100%|██████████| 16/16 [00:01<00:00, 11.90it/s]\n",
      "Lin.Train Epoch: [37] Loss: 2.4324 ACC: 56.09: 100%|██████████| 79/79 [00:03<00:00, 20.89it/s]\n",
      "Lin.Test Epoch: [37] Loss: 2.2075 ACC: 58.27% : 100%|██████████| 16/16 [00:01<00:00, 11.93it/s]\n",
      "Lin.Train Epoch: [38] Loss: 2.3329 ACC: 57.16: 100%|██████████| 79/79 [00:03<00:00, 20.53it/s]\n",
      "Lin.Test Epoch: [38] Loss: 1.9800 ACC: 60.22% : 100%|██████████| 16/16 [00:01<00:00, 11.60it/s]\n",
      "Lin.Train Epoch: [39] Loss: 2.2908 ACC: 56.69: 100%|██████████| 79/79 [00:04<00:00, 19.60it/s]\n",
      "Lin.Test Epoch: [39] Loss: 2.1917 ACC: 58.73% : 100%|██████████| 16/16 [00:01<00:00, 10.88it/s]\n",
      "Lin.Train Epoch: [40] Loss: 2.1840 ACC: 57.40: 100%|██████████| 79/79 [00:04<00:00, 19.73it/s]\n",
      "Lin.Test Epoch: [40] Loss: 2.2772 ACC: 57.70% : 100%|██████████| 16/16 [00:01<00:00, 10.97it/s]\n",
      "Lin.Train Epoch: [41] Loss: 2.2633 ACC: 57.34: 100%|██████████| 79/79 [00:03<00:00, 20.06it/s]\n",
      "Lin.Test Epoch: [41] Loss: 2.3602 ACC: 57.77% : 100%|██████████| 16/16 [00:01<00:00, 11.19it/s]\n",
      "Lin.Train Epoch: [42] Loss: 2.2736 ACC: 56.92: 100%|██████████| 79/79 [00:03<00:00, 20.00it/s]\n",
      "Lin.Test Epoch: [42] Loss: 2.1103 ACC: 59.90% : 100%|██████████| 16/16 [00:01<00:00, 11.06it/s]\n",
      "Lin.Train Epoch: [43] Loss: 2.1417 ACC: 57.82: 100%|██████████| 79/79 [00:03<00:00, 20.77it/s]\n",
      "Lin.Test Epoch: [43] Loss: 1.9564 ACC: 59.58% : 100%|██████████| 16/16 [00:01<00:00, 12.06it/s]\n",
      "Lin.Train Epoch: [44] Loss: 2.1088 ACC: 57.74: 100%|██████████| 79/79 [00:03<00:00, 21.60it/s]\n",
      "Lin.Test Epoch: [44] Loss: 2.0328 ACC: 59.72% : 100%|██████████| 16/16 [00:01<00:00, 11.32it/s]\n",
      "Lin.Train Epoch: [45] Loss: 2.1095 ACC: 57.83: 100%|██████████| 79/79 [00:03<00:00, 20.60it/s]\n",
      "Lin.Test Epoch: [45] Loss: 2.0755 ACC: 59.20% : 100%|██████████| 16/16 [00:01<00:00, 11.77it/s]\n",
      "Lin.Train Epoch: [46] Loss: 2.0491 ACC: 58.27: 100%|██████████| 79/79 [00:03<00:00, 20.67it/s]\n",
      "Lin.Test Epoch: [46] Loss: 1.8883 ACC: 60.17% : 100%|██████████| 16/16 [00:01<00:00, 10.83it/s]\n",
      "Lin.Train Epoch: [47] Loss: 1.9928 ACC: 58.38: 100%|██████████| 79/79 [00:03<00:00, 19.85it/s]\n",
      "Lin.Test Epoch: [47] Loss: 1.9802 ACC: 59.35% : 100%|██████████| 16/16 [00:01<00:00, 10.59it/s]\n",
      "Lin.Train Epoch: [48] Loss: 1.8788 ACC: 59.16: 100%|██████████| 79/79 [00:04<00:00, 19.73it/s]\n",
      "Lin.Test Epoch: [48] Loss: 1.8424 ACC: 59.70% : 100%|██████████| 16/16 [00:01<00:00, 11.15it/s]\n",
      "Lin.Train Epoch: [49] Loss: 1.8877 ACC: 59.19: 100%|██████████| 79/79 [00:04<00:00, 19.66it/s]\n",
      "Lin.Test Epoch: [49] Loss: 1.8140 ACC: 60.12% : 100%|██████████| 16/16 [00:01<00:00, 10.18it/s]\n",
      "Lin.Train Epoch: [50] Loss: 1.8612 ACC: 59.21: 100%|██████████| 79/79 [00:03<00:00, 20.03it/s]\n",
      "Lin.Test Epoch: [50] Loss: 1.7423 ACC: 60.77% : 100%|██████████| 16/16 [00:01<00:00, 11.61it/s]\n",
      "Lin.Train Epoch: [51] Loss: 1.8435 ACC: 58.81: 100%|██████████| 79/79 [00:03<00:00, 20.60it/s]\n",
      "Lin.Test Epoch: [51] Loss: 1.8298 ACC: 59.82% : 100%|██████████| 16/16 [00:01<00:00, 11.90it/s]\n",
      "Lin.Train Epoch: [52] Loss: 1.8356 ACC: 59.29: 100%|██████████| 79/79 [00:03<00:00, 20.80it/s]\n",
      "Lin.Test Epoch: [52] Loss: 1.8595 ACC: 58.90% : 100%|██████████| 16/16 [00:01<00:00, 12.29it/s]\n",
      "Lin.Train Epoch: [53] Loss: 1.8342 ACC: 58.68: 100%|██████████| 79/79 [00:03<00:00, 21.09it/s]\n",
      "Lin.Test Epoch: [53] Loss: 1.9232 ACC: 58.70% : 100%|██████████| 16/16 [00:01<00:00, 11.31it/s]\n",
      "Lin.Train Epoch: [54] Loss: 1.7759 ACC: 59.18: 100%|██████████| 79/79 [00:03<00:00, 20.57it/s]\n",
      "Lin.Test Epoch: [54] Loss: 1.6496 ACC: 61.02% : 100%|██████████| 16/16 [00:01<00:00, 11.93it/s]\n",
      "Lin.Train Epoch: [55] Loss: 1.7251 ACC: 59.48: 100%|██████████| 79/79 [00:03<00:00, 20.49it/s]\n",
      "Lin.Test Epoch: [55] Loss: 1.5832 ACC: 62.82% : 100%|██████████| 16/16 [00:01<00:00, 10.54it/s]\n",
      "Lin.Train Epoch: [56] Loss: 1.7263 ACC: 59.44: 100%|██████████| 79/79 [00:03<00:00, 19.99it/s]\n",
      "Lin.Test Epoch: [56] Loss: 1.6659 ACC: 60.62% : 100%|██████████| 16/16 [00:01<00:00, 11.32it/s]\n",
      "Lin.Train Epoch: [57] Loss: 1.7188 ACC: 59.41: 100%|██████████| 79/79 [00:03<00:00, 19.87it/s]\n",
      "Lin.Test Epoch: [57] Loss: 1.5305 ACC: 61.82% : 100%|██████████| 16/16 [00:01<00:00, 11.02it/s]\n",
      "Lin.Train Epoch: [58] Loss: 1.6385 ACC: 60.25: 100%|██████████| 79/79 [00:03<00:00, 20.31it/s]\n",
      "Lin.Test Epoch: [58] Loss: 1.4996 ACC: 62.40% : 100%|██████████| 16/16 [00:01<00:00, 10.82it/s]\n",
      "Lin.Train Epoch: [59] Loss: 1.6197 ACC: 60.68: 100%|██████████| 79/79 [00:03<00:00, 19.78it/s]\n",
      "Lin.Test Epoch: [59] Loss: 1.5912 ACC: 61.58% : 100%|██████████| 16/16 [00:01<00:00, 11.21it/s]\n",
      "Lin.Train Epoch: [60] Loss: 1.5885 ACC: 60.36: 100%|██████████| 79/79 [00:03<00:00, 20.73it/s]\n",
      "Lin.Test Epoch: [60] Loss: 1.5293 ACC: 62.82% : 100%|██████████| 16/16 [00:01<00:00, 11.49it/s]\n",
      "Lin.Train Epoch: [61] Loss: 1.5807 ACC: 60.93: 100%|██████████| 79/79 [00:03<00:00, 21.62it/s]\n",
      "Lin.Test Epoch: [61] Loss: 1.4919 ACC: 61.70% : 100%|██████████| 16/16 [00:01<00:00, 11.47it/s]\n",
      "Lin.Train Epoch: [62] Loss: 1.5256 ACC: 61.22: 100%|██████████| 79/79 [00:03<00:00, 20.53it/s]\n",
      "Lin.Test Epoch: [62] Loss: 1.4887 ACC: 62.55% : 100%|██████████| 16/16 [00:01<00:00, 10.85it/s]\n",
      "Lin.Train Epoch: [63] Loss: 1.5250 ACC: 61.39: 100%|██████████| 79/79 [00:03<00:00, 20.70it/s]\n",
      "Lin.Test Epoch: [63] Loss: 1.4920 ACC: 62.02% : 100%|██████████| 16/16 [00:01<00:00, 11.32it/s]\n",
      "Lin.Train Epoch: [64] Loss: 1.5192 ACC: 61.63: 100%|██████████| 79/79 [00:03<00:00, 20.90it/s]\n",
      "Lin.Test Epoch: [64] Loss: 1.3943 ACC: 63.45% : 100%|██████████| 16/16 [00:01<00:00, 11.11it/s]\n",
      "Lin.Train Epoch: [65] Loss: 1.4872 ACC: 61.30: 100%|██████████| 79/79 [00:03<00:00, 20.58it/s]\n",
      "Lin.Test Epoch: [65] Loss: 1.4432 ACC: 61.90% : 100%|██████████| 16/16 [00:01<00:00, 11.87it/s]\n",
      "Lin.Train Epoch: [66] Loss: 1.4726 ACC: 61.81: 100%|██████████| 79/79 [00:04<00:00, 19.62it/s]\n",
      "Lin.Test Epoch: [66] Loss: 1.2983 ACC: 64.70% : 100%|██████████| 16/16 [00:01<00:00, 10.84it/s]\n",
      "Lin.Train Epoch: [67] Loss: 1.4098 ACC: 62.42: 100%|██████████| 79/79 [00:03<00:00, 20.00it/s]\n",
      "Lin.Test Epoch: [67] Loss: 1.3245 ACC: 64.58% : 100%|██████████| 16/16 [00:01<00:00, 11.36it/s]\n",
      "Lin.Train Epoch: [68] Loss: 1.4377 ACC: 61.85: 100%|██████████| 79/79 [00:03<00:00, 19.84it/s]\n",
      "Lin.Test Epoch: [68] Loss: 1.3208 ACC: 64.88% : 100%|██████████| 16/16 [00:01<00:00, 11.78it/s]\n",
      "Lin.Train Epoch: [69] Loss: 1.4036 ACC: 62.48: 100%|██████████| 79/79 [00:03<00:00, 20.86it/s]\n",
      "Lin.Test Epoch: [69] Loss: 1.3967 ACC: 62.48% : 100%|██████████| 16/16 [00:01<00:00, 12.20it/s]\n",
      "Lin.Train Epoch: [70] Loss: 1.3906 ACC: 62.84: 100%|██████████| 79/79 [00:03<00:00, 21.47it/s]\n",
      "Lin.Test Epoch: [70] Loss: 1.3322 ACC: 64.72% : 100%|██████████| 16/16 [00:01<00:00, 11.22it/s]\n",
      "Lin.Train Epoch: [71] Loss: 1.3496 ACC: 63.24: 100%|██████████| 79/79 [00:03<00:00, 20.42it/s]\n",
      "Lin.Test Epoch: [71] Loss: 1.2795 ACC: 65.35% : 100%|██████████| 16/16 [00:01<00:00, 11.41it/s]\n",
      "Lin.Train Epoch: [72] Loss: 1.3523 ACC: 63.27: 100%|██████████| 79/79 [00:03<00:00, 20.72it/s]\n",
      "Lin.Test Epoch: [72] Loss: 1.2894 ACC: 64.53% : 100%|██████████| 16/16 [00:01<00:00, 11.89it/s]\n",
      "Lin.Train Epoch: [73] Loss: 1.3536 ACC: 63.08: 100%|██████████| 79/79 [00:03<00:00, 20.74it/s]\n",
      "Lin.Test Epoch: [73] Loss: 1.2575 ACC: 65.12% : 100%|██████████| 16/16 [00:01<00:00, 10.98it/s]\n",
      "Lin.Train Epoch: [74] Loss: 1.3269 ACC: 63.48: 100%|██████████| 79/79 [00:03<00:00, 19.88it/s]\n",
      "Lin.Test Epoch: [74] Loss: 1.2586 ACC: 65.28% : 100%|██████████| 16/16 [00:01<00:00, 11.22it/s]\n",
      "Lin.Train Epoch: [75] Loss: 1.2953 ACC: 63.78: 100%|██████████| 79/79 [00:03<00:00, 20.12it/s]\n",
      "Lin.Test Epoch: [75] Loss: 1.2542 ACC: 65.40% : 100%|██████████| 16/16 [00:01<00:00, 10.57it/s]\n",
      "Lin.Train Epoch: [76] Loss: 1.2991 ACC: 64.07: 100%|██████████| 79/79 [00:04<00:00, 19.71it/s]\n",
      "Lin.Test Epoch: [76] Loss: 1.2258 ACC: 65.88% : 100%|██████████| 16/16 [00:01<00:00, 11.08it/s]\n",
      "Lin.Train Epoch: [77] Loss: 1.2497 ACC: 65.22: 100%|██████████| 79/79 [00:03<00:00, 20.00it/s]\n",
      "Lin.Test Epoch: [77] Loss: 1.2140 ACC: 65.55% : 100%|██████████| 16/16 [00:01<00:00, 11.72it/s]\n",
      "Lin.Train Epoch: [78] Loss: 1.2633 ACC: 64.77: 100%|██████████| 79/79 [00:03<00:00, 21.30it/s]\n",
      "Lin.Test Epoch: [78] Loss: 1.2156 ACC: 65.08% : 100%|██████████| 16/16 [00:01<00:00, 12.35it/s]\n",
      "Lin.Train Epoch: [79] Loss: 1.2485 ACC: 64.68: 100%|██████████| 79/79 [00:03<00:00, 20.98it/s]\n",
      "Lin.Test Epoch: [79] Loss: 1.1971 ACC: 65.48% : 100%|██████████| 16/16 [00:01<00:00, 11.10it/s]\n",
      "Lin.Train Epoch: [80] Loss: 1.2302 ACC: 65.33: 100%|██████████| 79/79 [00:03<00:00, 20.41it/s]\n",
      "Lin.Test Epoch: [80] Loss: 1.1946 ACC: 66.15% : 100%|██████████| 16/16 [00:01<00:00, 11.25it/s]\n",
      "Lin.Train Epoch: [81] Loss: 1.2326 ACC: 65.38: 100%|██████████| 79/79 [00:03<00:00, 19.85it/s]\n",
      "Lin.Test Epoch: [81] Loss: 1.2019 ACC: 65.48% : 100%|██████████| 16/16 [00:01<00:00, 11.53it/s]\n",
      "Lin.Train Epoch: [82] Loss: 1.2318 ACC: 65.29: 100%|██████████| 79/79 [00:03<00:00, 20.90it/s]\n",
      "Lin.Test Epoch: [82] Loss: 1.1769 ACC: 66.55% : 100%|██████████| 16/16 [00:01<00:00, 10.58it/s]\n",
      "Lin.Train Epoch: [83] Loss: 1.2087 ACC: 65.64: 100%|██████████| 79/79 [00:03<00:00, 19.87it/s]\n",
      "Lin.Test Epoch: [83] Loss: 1.1814 ACC: 65.88% : 100%|██████████| 16/16 [00:01<00:00, 11.02it/s]\n",
      "Lin.Train Epoch: [84] Loss: 1.2065 ACC: 65.64: 100%|██████████| 79/79 [00:03<00:00, 19.98it/s]\n",
      "Lin.Test Epoch: [84] Loss: 1.1551 ACC: 66.50% : 100%|██████████| 16/16 [00:01<00:00, 10.49it/s]\n",
      "Lin.Train Epoch: [85] Loss: 1.1893 ACC: 65.94: 100%|██████████| 79/79 [00:03<00:00, 20.04it/s]\n",
      "Lin.Test Epoch: [85] Loss: 1.1552 ACC: 65.97% : 100%|██████████| 16/16 [00:01<00:00, 11.49it/s]\n",
      "Lin.Train Epoch: [86] Loss: 1.1888 ACC: 65.94: 100%|██████████| 79/79 [00:03<00:00, 20.74it/s]\n",
      "Lin.Test Epoch: [86] Loss: 1.1496 ACC: 65.97% : 100%|██████████| 16/16 [00:01<00:00, 11.38it/s]\n",
      "Lin.Train Epoch: [87] Loss: 1.1733 ACC: 66.59: 100%|██████████| 79/79 [00:03<00:00, 21.83it/s]\n",
      "Lin.Test Epoch: [87] Loss: 1.1664 ACC: 66.62% : 100%|██████████| 16/16 [00:01<00:00, 11.04it/s]\n",
      "Lin.Train Epoch: [88] Loss: 1.1817 ACC: 66.31: 100%|██████████| 79/79 [00:03<00:00, 20.52it/s]\n",
      "Lin.Test Epoch: [88] Loss: 1.1427 ACC: 66.72% : 100%|██████████| 16/16 [00:01<00:00, 11.37it/s]\n",
      "Lin.Train Epoch: [89] Loss: 1.1638 ACC: 66.40: 100%|██████████| 79/79 [00:03<00:00, 20.96it/s]\n",
      "Lin.Test Epoch: [89] Loss: 1.1397 ACC: 66.83% : 100%|██████████| 16/16 [00:01<00:00, 10.39it/s]\n",
      "Lin.Train Epoch: [90] Loss: 1.1598 ACC: 67.02: 100%|██████████| 79/79 [00:04<00:00, 19.46it/s]\n",
      "Lin.Test Epoch: [90] Loss: 1.1341 ACC: 66.40% : 100%|██████████| 16/16 [00:01<00:00, 10.24it/s]\n",
      "Lin.Train Epoch: [91] Loss: 1.1668 ACC: 66.25: 100%|██████████| 79/79 [00:04<00:00, 19.52it/s]\n",
      "Lin.Test Epoch: [91] Loss: 1.1416 ACC: 66.92% : 100%|██████████| 16/16 [00:01<00:00, 10.53it/s]\n",
      "Lin.Train Epoch: [92] Loss: 1.1678 ACC: 66.81: 100%|██████████| 79/79 [00:04<00:00, 19.65it/s]\n",
      "Lin.Test Epoch: [92] Loss: 1.1439 ACC: 66.88% : 100%|██████████| 16/16 [00:01<00:00, 10.58it/s]\n",
      "Lin.Train Epoch: [93] Loss: 1.1523 ACC: 67.17: 100%|██████████| 79/79 [00:03<00:00, 20.03it/s]\n",
      "Lin.Test Epoch: [93] Loss: 1.1311 ACC: 66.85% : 100%|██████████| 16/16 [00:01<00:00, 11.04it/s]\n",
      "Lin.Train Epoch: [94] Loss: 1.1470 ACC: 66.91: 100%|██████████| 79/79 [00:03<00:00, 19.88it/s]\n",
      "Lin.Test Epoch: [94] Loss: 1.1283 ACC: 67.05% : 100%|██████████| 16/16 [00:01<00:00, 11.81it/s]\n",
      "Lin.Train Epoch: [95] Loss: 1.1574 ACC: 66.51: 100%|██████████| 79/79 [00:03<00:00, 21.11it/s]\n",
      "Lin.Test Epoch: [95] Loss: 1.1311 ACC: 66.70% : 100%|██████████| 16/16 [00:01<00:00, 12.79it/s]\n",
      "Lin.Train Epoch: [96] Loss: 1.1491 ACC: 66.95: 100%|██████████| 79/79 [00:03<00:00, 20.95it/s]\n",
      "Lin.Test Epoch: [96] Loss: 1.1206 ACC: 67.03% : 100%|██████████| 16/16 [00:01<00:00, 11.14it/s]\n",
      "Lin.Train Epoch: [97] Loss: 1.1549 ACC: 66.73: 100%|██████████| 79/79 [00:03<00:00, 20.06it/s]\n",
      "Lin.Test Epoch: [97] Loss: 1.1217 ACC: 67.00% : 100%|██████████| 16/16 [00:01<00:00, 11.32it/s]\n",
      "Lin.Train Epoch: [98] Loss: 1.1380 ACC: 67.29: 100%|██████████| 79/79 [00:03<00:00, 20.44it/s]\n",
      "Lin.Test Epoch: [98] Loss: 1.1241 ACC: 67.00% : 100%|██████████| 16/16 [00:01<00:00, 10.64it/s]\n",
      "Lin.Train Epoch: [99] Loss: 1.1419 ACC: 67.27: 100%|██████████| 79/79 [00:03<00:00, 19.91it/s]\n",
      "Lin.Test Epoch: [99] Loss: 1.1210 ACC: 67.25% : 100%|██████████| 16/16 [00:01<00:00, 11.27it/s]\n",
      "Lin.Train Epoch: [100] Loss: 1.1508 ACC: 66.86: 100%|██████████| 79/79 [00:03<00:00, 20.01it/s]\n",
      "Lin.Test Epoch: [100] Loss: 1.1195 ACC: 67.25% : 100%|██████████| 16/16 [00:01<00:00, 10.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1195441608428955,\n",
       " 67.25,\n",
       " LinearClassifier(\n",
       "   (classifier): Linear(in_features=512, out_features=40, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_id = 1\n",
    "lin_epoch = 100\n",
    "num_class = np.sum(args.class_split[:task_id+1])\n",
    "classifier = LinearClassifier(num_classes = num_class).to(device)\n",
    "lin_optimizer = torch.optim.SGD(classifier.parameters(), 0.2, momentum=0.9, weight_decay=0) # Infomax: no weight decay, epoch 100, cosine scheduler\n",
    "lin_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(lin_optimizer, lin_epoch, eta_min=0.002) #scheduler + values ref: infomax paper\n",
    "linear_evaluation(model2, train_data_loaders_linear[:task_id+1], test_data_loaders[:task_id+1], lin_optimizer,classifier, lin_scheduler, lin_epoch, device, task_id)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin.Train Epoch: [1] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  3.00it/s]\n",
      "Lin.Train Epoch: [2] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  3.11it/s]\n",
      "Lin.Train Epoch: [3] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  3.09it/s]\n",
      "Lin.Train Epoch: [4] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  3.07it/s]\n",
      "Lin.Train Epoch: [5] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  3.09it/s]\n",
      "Lin.Train Epoch: [6] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  3.16it/s]\n",
      "Lin.Train Epoch: [7] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  3.19it/s]\n",
      "Lin.Train Epoch: [8] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  3.07it/s]\n",
      "Lin.Train Epoch: [9] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  2.94it/s]\n",
      "Lin.Train Epoch: [10] Loss: 0.0000: 100%|██████████| 20/20 [00:06<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "model2.contrastive_projector = contrastive_projector \n",
    "task_id = 1\n",
    "lin_epoch= 10\n",
    "lin_optimizer = torch.optim.SGD(model2.parameters(), 1e-3, momentum=0.9, weight_decay=0) \n",
    "test_loss = contrastive_train(model2, train_data_loaders_linear[task_id], task_id, lin_optimizer, None, epochs=lin_epoch, device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin.Train Epoch: [1] Loss: 3.0140 ACC: 44.57: 100%|██████████| 79/79 [00:04<00:00, 19.69it/s]\n",
      "Lin.Test Epoch: [1] Loss: 2.5046 ACC: 51.70% : 100%|██████████| 16/16 [00:01<00:00, 11.06it/s]\n",
      "Lin.Train Epoch: [2] Loss: 3.2415 ACC: 48.98: 100%|██████████| 79/79 [00:04<00:00, 19.28it/s]\n",
      "Lin.Test Epoch: [2] Loss: 2.2925 ACC: 56.50% : 100%|██████████| 16/16 [00:01<00:00, 11.58it/s]\n",
      "Lin.Train Epoch: [3] Loss: 2.8112 ACC: 51.92: 100%|██████████| 79/79 [00:03<00:00, 20.35it/s]\n",
      "Lin.Test Epoch: [3] Loss: 2.6728 ACC: 54.97% : 100%|██████████| 16/16 [00:01<00:00, 11.19it/s]\n",
      "Lin.Train Epoch: [4] Loss: 2.9044 ACC: 51.85: 100%|██████████| 79/79 [00:03<00:00, 20.46it/s]\n",
      "Lin.Test Epoch: [4] Loss: 3.2235 ACC: 51.52% : 100%|██████████| 16/16 [00:01<00:00, 11.25it/s]\n",
      "Lin.Train Epoch: [5] Loss: 3.1853 ACC: 52.03: 100%|██████████| 79/79 [00:03<00:00, 20.14it/s]\n",
      "Lin.Test Epoch: [5] Loss: 2.8158 ACC: 55.07% : 100%|██████████| 16/16 [00:01<00:00, 10.21it/s]\n",
      "Lin.Train Epoch: [6] Loss: 2.8871 ACC: 53.77: 100%|██████████| 79/79 [00:03<00:00, 20.78it/s]\n",
      "Lin.Test Epoch: [6] Loss: 2.6064 ACC: 56.10% : 100%|██████████| 16/16 [00:01<00:00, 10.84it/s]\n",
      "Lin.Train Epoch: [7] Loss: 2.9405 ACC: 53.34: 100%|██████████| 79/79 [00:03<00:00, 20.13it/s]\n",
      "Lin.Test Epoch: [7] Loss: 2.7293 ACC: 55.38% : 100%|██████████| 16/16 [00:01<00:00, 10.42it/s]\n",
      "Lin.Train Epoch: [8] Loss: 2.8924 ACC: 53.51: 100%|██████████| 79/79 [00:04<00:00, 18.95it/s]\n",
      "Lin.Test Epoch: [8] Loss: 3.1680 ACC: 51.70% : 100%|██████████| 16/16 [00:01<00:00, 10.76it/s]\n",
      "Lin.Train Epoch: [9] Loss: 3.0219 ACC: 53.39: 100%|██████████| 79/79 [00:03<00:00, 19.84it/s]\n",
      "Lin.Test Epoch: [9] Loss: 2.9071 ACC: 55.27% : 100%|██████████| 16/16 [00:01<00:00, 10.57it/s]\n",
      "Lin.Train Epoch: [10] Loss: 3.0289 ACC: 53.64: 100%|██████████| 79/79 [00:04<00:00, 19.02it/s]\n",
      "Lin.Test Epoch: [10] Loss: 2.9141 ACC: 54.40% : 100%|██████████| 16/16 [00:01<00:00, 10.02it/s]\n",
      "Lin.Train Epoch: [11] Loss: 3.0181 ACC: 53.56: 100%|██████████| 79/79 [00:03<00:00, 19.81it/s]\n",
      "Lin.Test Epoch: [11] Loss: 2.8369 ACC: 53.67% : 100%|██████████| 16/16 [00:01<00:00, 11.03it/s]\n",
      "Lin.Train Epoch: [12] Loss: 2.8265 ACC: 54.55: 100%|██████████| 79/79 [00:03<00:00, 20.21it/s]\n",
      "Lin.Test Epoch: [12] Loss: 2.6532 ACC: 56.73% : 100%|██████████| 16/16 [00:01<00:00, 10.89it/s]\n",
      "Lin.Train Epoch: [13] Loss: 2.8711 ACC: 53.96: 100%|██████████| 79/79 [00:03<00:00, 20.36it/s]\n",
      "Lin.Test Epoch: [13] Loss: 2.8572 ACC: 54.97% : 100%|██████████| 16/16 [00:01<00:00, 11.22it/s]\n",
      "Lin.Train Epoch: [14] Loss: 2.9903 ACC: 54.20: 100%|██████████| 79/79 [00:03<00:00, 20.51it/s]\n",
      "Lin.Test Epoch: [14] Loss: 2.9000 ACC: 54.93% : 100%|██████████| 16/16 [00:01<00:00, 11.26it/s]\n",
      "Lin.Train Epoch: [15] Loss: 2.8219 ACC: 54.42: 100%|██████████| 79/79 [00:03<00:00, 19.85it/s]\n",
      "Lin.Test Epoch: [15] Loss: 3.0464 ACC: 54.02% : 100%|██████████| 16/16 [00:01<00:00, 11.00it/s]\n",
      "Lin.Train Epoch: [16] Loss: 2.9245 ACC: 54.62: 100%|██████████| 79/79 [00:03<00:00, 20.10it/s]\n",
      "Lin.Test Epoch: [16] Loss: 2.6699 ACC: 57.20% : 100%|██████████| 16/16 [00:01<00:00, 11.03it/s]\n",
      "Lin.Train Epoch: [17] Loss: 2.7876 ACC: 55.35: 100%|██████████| 79/79 [00:04<00:00, 19.54it/s]\n",
      "Lin.Test Epoch: [17] Loss: 2.7050 ACC: 56.88% : 100%|██████████| 16/16 [00:01<00:00, 10.18it/s]\n",
      "Lin.Train Epoch: [18] Loss: 3.0899 ACC: 54.06: 100%|██████████| 79/79 [00:04<00:00, 19.27it/s]\n",
      "Lin.Test Epoch: [18] Loss: 2.7887 ACC: 56.07% : 100%|██████████| 16/16 [00:01<00:00, 10.68it/s]\n",
      "Lin.Train Epoch: [19] Loss: 2.9217 ACC: 54.76: 100%|██████████| 79/79 [00:04<00:00, 19.55it/s]\n",
      "Lin.Test Epoch: [19] Loss: 2.6780 ACC: 58.05% : 100%|██████████| 16/16 [00:01<00:00, 10.46it/s]\n",
      "Lin.Train Epoch: [20] Loss: 2.7319 ACC: 55.63: 100%|██████████| 79/79 [00:03<00:00, 19.77it/s]\n",
      "Lin.Test Epoch: [20] Loss: 2.4293 ACC: 58.73% : 100%|██████████| 16/16 [00:01<00:00, 11.26it/s]\n",
      "Lin.Train Epoch: [21] Loss: 2.8167 ACC: 55.36: 100%|██████████| 79/79 [00:03<00:00, 20.86it/s]\n",
      "Lin.Test Epoch: [21] Loss: 2.4232 ACC: 58.70% : 100%|██████████| 16/16 [00:01<00:00, 10.79it/s]\n",
      "Lin.Train Epoch: [22] Loss: 2.6500 ACC: 55.58: 100%|██████████| 79/79 [00:03<00:00, 20.25it/s]\n",
      "Lin.Test Epoch: [22] Loss: 2.5427 ACC: 57.07% : 100%|██████████| 16/16 [00:01<00:00, 11.26it/s]\n",
      "Lin.Train Epoch: [23] Loss: 2.7301 ACC: 55.28: 100%|██████████| 79/79 [00:03<00:00, 20.23it/s]\n",
      "Lin.Test Epoch: [23] Loss: 2.7042 ACC: 56.17% : 100%|██████████| 16/16 [00:01<00:00,  9.79it/s]\n",
      "Lin.Train Epoch: [24] Loss: 2.4947 ACC: 56.86: 100%|██████████| 79/79 [00:03<00:00, 20.00it/s]\n",
      "Lin.Test Epoch: [24] Loss: 2.2884 ACC: 57.55% : 100%|██████████| 16/16 [00:01<00:00, 10.46it/s]\n",
      "Lin.Train Epoch: [25] Loss: 2.5785 ACC: 55.34: 100%|██████████| 79/79 [00:04<00:00, 19.26it/s]\n",
      "Lin.Test Epoch: [25] Loss: 2.3060 ACC: 59.62% : 100%|██████████| 16/16 [00:01<00:00,  9.85it/s]\n",
      "Lin.Train Epoch: [26] Loss: 2.7937 ACC: 54.46: 100%|██████████| 79/79 [00:03<00:00, 20.01it/s]\n",
      "Lin.Test Epoch: [26] Loss: 2.3673 ACC: 58.20% : 100%|██████████| 16/16 [00:01<00:00, 10.30it/s]\n",
      "Lin.Train Epoch: [27] Loss: 2.5880 ACC: 55.68: 100%|██████████| 79/79 [00:04<00:00, 19.71it/s]\n",
      "Lin.Test Epoch: [27] Loss: 3.0452 ACC: 52.80% : 100%|██████████| 16/16 [00:01<00:00, 10.91it/s]\n",
      "Lin.Train Epoch: [28] Loss: 2.6299 ACC: 55.12: 100%|██████████| 79/79 [00:03<00:00, 20.45it/s]\n",
      "Lin.Test Epoch: [28] Loss: 2.6491 ACC: 57.63% : 100%|██████████| 16/16 [00:01<00:00, 11.02it/s]\n",
      "Lin.Train Epoch: [29] Loss: 2.5478 ACC: 56.28: 100%|██████████| 79/79 [00:03<00:00, 20.60it/s]\n",
      "Lin.Test Epoch: [29] Loss: 2.3334 ACC: 59.60% : 100%|██████████| 16/16 [00:01<00:00, 11.66it/s]\n",
      "Lin.Train Epoch: [30] Loss: 2.5084 ACC: 55.93: 100%|██████████| 79/79 [00:03<00:00, 20.53it/s]\n",
      "Lin.Test Epoch: [30] Loss: 2.1895 ACC: 58.67% : 100%|██████████| 16/16 [00:01<00:00, 10.86it/s]\n",
      "Lin.Train Epoch: [31] Loss: 2.4408 ACC: 56.25: 100%|██████████| 79/79 [00:03<00:00, 20.83it/s]\n",
      "Lin.Test Epoch: [31] Loss: 2.2360 ACC: 59.05% : 100%|██████████| 16/16 [00:01<00:00, 11.86it/s]\n",
      "Lin.Train Epoch: [32] Loss: 2.4860 ACC: 55.98: 100%|██████████| 79/79 [00:03<00:00, 19.88it/s]\n",
      "Lin.Test Epoch: [32] Loss: 2.8124 ACC: 56.33% : 100%|██████████| 16/16 [00:01<00:00, 10.33it/s]\n",
      "Lin.Train Epoch: [33] Loss: 2.5749 ACC: 55.54: 100%|██████████| 79/79 [00:03<00:00, 19.79it/s]\n",
      "Lin.Test Epoch: [33] Loss: 2.0885 ACC: 60.08% : 100%|██████████| 16/16 [00:01<00:00, 10.30it/s]\n",
      "Lin.Train Epoch: [34] Loss: 2.2972 ACC: 57.45: 100%|██████████| 79/79 [00:04<00:00, 19.31it/s]\n",
      "Lin.Test Epoch: [34] Loss: 2.1729 ACC: 58.70% : 100%|██████████| 16/16 [00:01<00:00, 10.59it/s]\n",
      "Lin.Train Epoch: [35] Loss: 2.2800 ACC: 56.86: 100%|██████████| 79/79 [00:04<00:00, 19.65it/s]\n",
      "Lin.Test Epoch: [35] Loss: 2.4323 ACC: 57.25% : 100%|██████████| 16/16 [00:01<00:00,  9.84it/s]\n",
      "Lin.Train Epoch: [36] Loss: 2.3483 ACC: 56.62: 100%|██████████| 79/79 [00:04<00:00, 19.31it/s]\n",
      "Lin.Test Epoch: [36] Loss: 2.4024 ACC: 56.73% : 100%|██████████| 16/16 [00:01<00:00, 11.08it/s]\n",
      "Lin.Train Epoch: [37] Loss: 2.3641 ACC: 56.46: 100%|██████████| 79/79 [00:03<00:00, 20.67it/s]\n",
      "Lin.Test Epoch: [37] Loss: 2.5040 ACC: 54.60% : 100%|██████████| 16/16 [00:01<00:00, 11.57it/s]\n",
      "Lin.Train Epoch: [38] Loss: 2.2672 ACC: 57.32: 100%|██████████| 79/79 [00:03<00:00, 20.57it/s]\n",
      "Lin.Test Epoch: [38] Loss: 2.1837 ACC: 57.98% : 100%|██████████| 16/16 [00:01<00:00, 10.61it/s]\n",
      "Lin.Train Epoch: [39] Loss: 2.1483 ACC: 58.02: 100%|██████████| 79/79 [00:03<00:00, 20.55it/s]\n",
      "Lin.Test Epoch: [39] Loss: 2.2281 ACC: 58.98% : 100%|██████████| 16/16 [00:01<00:00, 10.87it/s]\n",
      "Lin.Train Epoch: [40] Loss: 2.2246 ACC: 57.18: 100%|██████████| 79/79 [00:03<00:00, 20.31it/s]\n",
      "Lin.Test Epoch: [40] Loss: 2.0814 ACC: 57.80% : 100%|██████████| 16/16 [00:01<00:00, 10.94it/s]\n",
      "Lin.Train Epoch: [41] Loss: 2.1203 ACC: 57.69: 100%|██████████| 79/79 [00:03<00:00, 20.13it/s]\n",
      "Lin.Test Epoch: [41] Loss: 2.0177 ACC: 59.65% : 100%|██████████| 16/16 [00:01<00:00, 11.00it/s]\n",
      "Lin.Train Epoch: [42] Loss: 2.1446 ACC: 57.15: 100%|██████████| 79/79 [00:04<00:00, 19.70it/s]\n",
      "Lin.Test Epoch: [42] Loss: 2.1320 ACC: 57.45% : 100%|██████████| 16/16 [00:01<00:00, 10.54it/s]\n",
      "Lin.Train Epoch: [43] Loss: 2.1217 ACC: 56.85: 100%|██████████| 79/79 [00:04<00:00, 19.43it/s]\n",
      "Lin.Test Epoch: [43] Loss: 1.9255 ACC: 60.42% : 100%|██████████| 16/16 [00:01<00:00, 10.56it/s]\n",
      "Lin.Train Epoch: [44] Loss: 2.0660 ACC: 57.59: 100%|██████████| 79/79 [00:03<00:00, 20.19it/s]\n",
      "Lin.Test Epoch: [44] Loss: 1.9525 ACC: 58.13% : 100%|██████████| 16/16 [00:01<00:00, 11.07it/s]\n",
      "Lin.Train Epoch: [45] Loss: 1.9816 ACC: 58.44: 100%|██████████| 79/79 [00:03<00:00, 20.33it/s]\n",
      "Lin.Test Epoch: [45] Loss: 2.0045 ACC: 57.67% : 100%|██████████| 16/16 [00:01<00:00, 11.18it/s]\n",
      "Lin.Train Epoch: [46] Loss: 1.9789 ACC: 57.92: 100%|██████████| 79/79 [00:03<00:00, 21.06it/s]\n",
      "Lin.Test Epoch: [46] Loss: 2.1061 ACC: 57.20% : 100%|██████████| 16/16 [00:01<00:00, 10.99it/s]\n",
      "Lin.Train Epoch: [47] Loss: 1.9313 ACC: 59.00: 100%|██████████| 79/79 [00:03<00:00, 20.33it/s]\n",
      "Lin.Test Epoch: [47] Loss: 1.7971 ACC: 60.30% : 100%|██████████| 16/16 [00:01<00:00, 10.39it/s]\n",
      "Lin.Train Epoch: [48] Loss: 1.8893 ACC: 58.89: 100%|██████████| 79/79 [00:03<00:00, 19.89it/s]\n",
      "Lin.Test Epoch: [48] Loss: 1.7473 ACC: 60.72% : 100%|██████████| 16/16 [00:01<00:00, 10.25it/s]\n",
      "Lin.Train Epoch: [49] Loss: 1.8168 ACC: 59.05: 100%|██████████| 79/79 [00:03<00:00, 20.19it/s]\n",
      "Lin.Test Epoch: [49] Loss: 1.9264 ACC: 58.83% : 100%|██████████| 16/16 [00:01<00:00, 10.94it/s]\n",
      "Lin.Train Epoch: [50] Loss: 1.8068 ACC: 59.33: 100%|██████████| 79/79 [00:04<00:00, 19.47it/s]\n",
      "Lin.Test Epoch: [50] Loss: 1.7115 ACC: 61.00% : 100%|██████████| 16/16 [00:01<00:00, 10.85it/s]\n",
      "Lin.Train Epoch: [51] Loss: 1.8038 ACC: 59.10: 100%|██████████| 79/79 [00:03<00:00, 20.33it/s]\n",
      "Lin.Test Epoch: [51] Loss: 1.7562 ACC: 60.25% : 100%|██████████| 16/16 [00:01<00:00,  9.99it/s]\n",
      "Lin.Train Epoch: [52] Loss: 1.7625 ACC: 59.40: 100%|██████████| 79/79 [00:04<00:00, 19.53it/s]\n",
      "Lin.Test Epoch: [52] Loss: 1.6145 ACC: 62.42% : 100%|██████████| 16/16 [00:01<00:00, 10.59it/s]\n",
      "Lin.Train Epoch: [53] Loss: 1.7945 ACC: 59.15: 100%|██████████| 79/79 [00:03<00:00, 19.89it/s]\n",
      "Lin.Test Epoch: [53] Loss: 1.6145 ACC: 60.95% : 100%|██████████| 16/16 [00:01<00:00, 11.24it/s]\n",
      "Lin.Train Epoch: [54] Loss: 1.7554 ACC: 59.27: 100%|██████████| 79/79 [00:03<00:00, 20.76it/s]\n",
      "Lin.Test Epoch: [54] Loss: 1.7201 ACC: 59.70% : 100%|██████████| 16/16 [00:01<00:00, 11.83it/s]\n",
      "Lin.Train Epoch: [55] Loss: 1.7131 ACC: 59.59: 100%|██████████| 79/79 [00:03<00:00, 20.48it/s]\n",
      "Lin.Test Epoch: [55] Loss: 1.6460 ACC: 60.68% : 100%|██████████| 16/16 [00:01<00:00, 10.87it/s]\n",
      "Lin.Train Epoch: [56] Loss: 1.6602 ACC: 60.40: 100%|██████████| 79/79 [00:03<00:00, 20.62it/s]\n",
      "Lin.Test Epoch: [56] Loss: 1.5046 ACC: 63.12% : 100%|██████████| 16/16 [00:01<00:00, 11.19it/s]\n",
      "Lin.Train Epoch: [57] Loss: 1.6307 ACC: 60.25: 100%|██████████| 79/79 [00:03<00:00, 19.89it/s]\n",
      "Lin.Test Epoch: [57] Loss: 1.5718 ACC: 62.10% : 100%|██████████| 16/16 [00:01<00:00, 10.74it/s]\n",
      "Lin.Train Epoch: [58] Loss: 1.5893 ACC: 61.05: 100%|██████████| 79/79 [00:03<00:00, 20.09it/s]\n",
      "Lin.Test Epoch: [58] Loss: 1.5507 ACC: 61.82% : 100%|██████████| 16/16 [00:01<00:00, 11.13it/s]\n",
      "Lin.Train Epoch: [59] Loss: 1.6534 ACC: 60.05: 100%|██████████| 79/79 [00:04<00:00, 19.37it/s]\n",
      "Lin.Test Epoch: [59] Loss: 1.6344 ACC: 59.42% : 100%|██████████| 16/16 [00:01<00:00, 10.31it/s]\n",
      "Lin.Train Epoch: [60] Loss: 1.6225 ACC: 59.85: 100%|██████████| 79/79 [00:03<00:00, 19.89it/s]\n",
      "Lin.Test Epoch: [60] Loss: 1.5083 ACC: 61.82% : 100%|██████████| 16/16 [00:01<00:00, 10.69it/s]\n",
      "Lin.Train Epoch: [61] Loss: 1.5702 ACC: 61.06: 100%|██████████| 79/79 [00:04<00:00, 19.29it/s]\n",
      "Lin.Test Epoch: [61] Loss: 1.4761 ACC: 63.25% : 100%|██████████| 16/16 [00:01<00:00, 10.97it/s]\n",
      "Lin.Train Epoch: [62] Loss: 1.5447 ACC: 60.66: 100%|██████████| 79/79 [00:03<00:00, 20.24it/s]\n",
      "Lin.Test Epoch: [62] Loss: 1.3940 ACC: 63.12% : 100%|██████████| 16/16 [00:01<00:00, 11.28it/s]\n",
      "Lin.Train Epoch: [63] Loss: 1.5193 ACC: 61.17: 100%|██████████| 79/79 [00:03<00:00, 21.24it/s]\n",
      "Lin.Test Epoch: [63] Loss: 1.4805 ACC: 61.22% : 100%|██████████| 16/16 [00:01<00:00, 10.69it/s]\n",
      "Lin.Train Epoch: [64] Loss: 1.4854 ACC: 61.67: 100%|██████████| 79/79 [00:03<00:00, 20.44it/s]\n",
      "Lin.Test Epoch: [64] Loss: 1.4031 ACC: 63.52% : 100%|██████████| 16/16 [00:01<00:00, 10.64it/s]\n",
      "Lin.Train Epoch: [65] Loss: 1.4676 ACC: 61.45: 100%|██████████| 79/79 [00:03<00:00, 20.23it/s]\n",
      "Lin.Test Epoch: [65] Loss: 1.4314 ACC: 63.30% : 100%|██████████| 16/16 [00:01<00:00, 10.60it/s]\n",
      "Lin.Train Epoch: [66] Loss: 1.4669 ACC: 61.53: 100%|██████████| 79/79 [00:03<00:00, 19.77it/s]\n",
      "Lin.Test Epoch: [66] Loss: 1.3484 ACC: 63.78% : 100%|██████████| 16/16 [00:01<00:00, 10.75it/s]\n",
      "Lin.Train Epoch: [67] Loss: 1.4335 ACC: 61.97: 100%|██████████| 79/79 [00:04<00:00, 19.60it/s]\n",
      "Lin.Test Epoch: [67] Loss: 1.3762 ACC: 63.55% : 100%|██████████| 16/16 [00:01<00:00, 10.18it/s]\n",
      "Lin.Train Epoch: [68] Loss: 1.4544 ACC: 61.93: 100%|██████████| 79/79 [00:04<00:00, 19.50it/s]\n",
      "Lin.Test Epoch: [68] Loss: 1.2929 ACC: 65.60% : 100%|██████████| 16/16 [00:01<00:00,  9.81it/s]\n",
      "Lin.Train Epoch: [69] Loss: 1.4217 ACC: 62.07: 100%|██████████| 79/79 [00:03<00:00, 19.77it/s]\n",
      "Lin.Test Epoch: [69] Loss: 1.3585 ACC: 63.75% : 100%|██████████| 16/16 [00:01<00:00, 10.84it/s]\n",
      "Lin.Train Epoch: [70] Loss: 1.3805 ACC: 62.80: 100%|██████████| 79/79 [00:04<00:00, 19.57it/s]\n",
      "Lin.Test Epoch: [70] Loss: 1.2770 ACC: 65.25% : 100%|██████████| 16/16 [00:01<00:00, 11.18it/s]\n",
      "Lin.Train Epoch: [71] Loss: 1.3477 ACC: 63.19: 100%|██████████| 79/79 [00:03<00:00, 20.68it/s]\n",
      "Lin.Test Epoch: [71] Loss: 1.3331 ACC: 63.82% : 100%|██████████| 16/16 [00:01<00:00, 10.86it/s]\n",
      "Lin.Train Epoch: [72] Loss: 1.3364 ACC: 63.39: 100%|██████████| 79/79 [00:03<00:00, 20.49it/s]\n",
      "Lin.Test Epoch: [72] Loss: 1.2851 ACC: 64.45% : 100%|██████████| 16/16 [00:01<00:00, 11.37it/s]\n",
      "Lin.Train Epoch: [73] Loss: 1.3104 ACC: 63.68: 100%|██████████| 79/79 [00:03<00:00, 20.85it/s]\n",
      "Lin.Test Epoch: [73] Loss: 1.2440 ACC: 65.22% : 100%|██████████| 16/16 [00:01<00:00, 10.83it/s]\n",
      "Lin.Train Epoch: [74] Loss: 1.3002 ACC: 63.74: 100%|██████████| 79/79 [00:04<00:00, 19.43it/s]\n",
      "Lin.Test Epoch: [74] Loss: 1.2569 ACC: 65.35% : 100%|██████████| 16/16 [00:01<00:00, 10.34it/s]\n",
      "Lin.Train Epoch: [75] Loss: 1.3068 ACC: 63.79: 100%|██████████| 79/79 [00:03<00:00, 20.03it/s]\n",
      "Lin.Test Epoch: [75] Loss: 1.2487 ACC: 64.70% : 100%|██████████| 16/16 [00:01<00:00, 10.16it/s]\n",
      "Lin.Train Epoch: [76] Loss: 1.2939 ACC: 64.22: 100%|██████████| 79/79 [00:03<00:00, 20.06it/s]\n",
      "Lin.Test Epoch: [76] Loss: 1.2352 ACC: 65.38% : 100%|██████████| 16/16 [00:01<00:00, 10.52it/s]\n",
      "Lin.Train Epoch: [77] Loss: 1.2599 ACC: 64.63: 100%|██████████| 79/79 [00:04<00:00, 19.50it/s]\n",
      "Lin.Test Epoch: [77] Loss: 1.2129 ACC: 65.85% : 100%|██████████| 16/16 [00:01<00:00, 11.28it/s]\n",
      "Lin.Train Epoch: [78] Loss: 1.2603 ACC: 64.52: 100%|██████████| 79/79 [00:04<00:00, 19.67it/s]\n",
      "Lin.Test Epoch: [78] Loss: 1.2469 ACC: 65.12% : 100%|██████████| 16/16 [00:01<00:00, 11.18it/s]\n",
      "Lin.Train Epoch: [79] Loss: 1.2561 ACC: 64.44: 100%|██████████| 79/79 [00:03<00:00, 20.08it/s]\n",
      "Lin.Test Epoch: [79] Loss: 1.2008 ACC: 66.05% : 100%|██████████| 16/16 [00:01<00:00, 11.35it/s]\n",
      "Lin.Train Epoch: [80] Loss: 1.2435 ACC: 64.70: 100%|██████████| 79/79 [00:03<00:00, 20.54it/s]\n",
      "Lin.Test Epoch: [80] Loss: 1.1913 ACC: 66.42% : 100%|██████████| 16/16 [00:01<00:00, 10.63it/s]\n",
      "Lin.Train Epoch: [81] Loss: 1.2175 ACC: 65.42: 100%|██████████| 79/79 [00:03<00:00, 20.38it/s]\n",
      "Lin.Test Epoch: [81] Loss: 1.1807 ACC: 66.12% : 100%|██████████| 16/16 [00:01<00:00, 11.25it/s]\n",
      "Lin.Train Epoch: [82] Loss: 1.2117 ACC: 65.53: 100%|██████████| 79/79 [00:04<00:00, 19.60it/s]\n",
      "Lin.Test Epoch: [82] Loss: 1.1804 ACC: 66.22% : 100%|██████████| 16/16 [00:01<00:00,  9.43it/s]\n",
      "Lin.Train Epoch: [83] Loss: 1.2172 ACC: 65.76: 100%|██████████| 79/79 [00:04<00:00, 19.06it/s]\n",
      "Lin.Test Epoch: [83] Loss: 1.1650 ACC: 66.90% : 100%|██████████| 16/16 [00:01<00:00, 10.19it/s]\n",
      "Lin.Train Epoch: [84] Loss: 1.1934 ACC: 65.43: 100%|██████████| 79/79 [00:04<00:00, 19.65it/s]\n",
      "Lin.Test Epoch: [84] Loss: 1.1675 ACC: 66.57% : 100%|██████████| 16/16 [00:01<00:00, 11.64it/s]\n",
      "Lin.Train Epoch: [85] Loss: 1.1969 ACC: 65.88: 100%|██████████| 79/79 [00:03<00:00, 19.99it/s]\n",
      "Lin.Test Epoch: [85] Loss: 1.1527 ACC: 66.33% : 100%|██████████| 16/16 [00:01<00:00, 10.27it/s]\n",
      "Lin.Train Epoch: [86] Loss: 1.1941 ACC: 65.64: 100%|██████████| 79/79 [00:04<00:00, 19.40it/s]\n",
      "Lin.Test Epoch: [86] Loss: 1.1466 ACC: 66.77% : 100%|██████████| 16/16 [00:01<00:00, 11.18it/s]\n",
      "Lin.Train Epoch: [87] Loss: 1.1808 ACC: 66.29: 100%|██████████| 79/79 [00:03<00:00, 20.36it/s]\n",
      "Lin.Test Epoch: [87] Loss: 1.1542 ACC: 66.42% : 100%|██████████| 16/16 [00:01<00:00, 11.27it/s]\n",
      "Lin.Train Epoch: [88] Loss: 1.1576 ACC: 66.92: 100%|██████████| 79/79 [00:03<00:00, 21.15it/s]\n",
      "Lin.Test Epoch: [88] Loss: 1.1458 ACC: 66.77% : 100%|██████████| 16/16 [00:01<00:00, 10.08it/s]\n",
      "Lin.Train Epoch: [89] Loss: 1.1574 ACC: 66.72: 100%|██████████| 79/79 [00:03<00:00, 20.27it/s]\n",
      "Lin.Test Epoch: [89] Loss: 1.1470 ACC: 66.22% : 100%|██████████| 16/16 [00:01<00:00, 10.43it/s]\n",
      "Lin.Train Epoch: [90] Loss: 1.1618 ACC: 66.31: 100%|██████████| 79/79 [00:03<00:00, 20.31it/s]\n",
      "Lin.Test Epoch: [90] Loss: 1.1489 ACC: 66.72% : 100%|██████████| 16/16 [00:01<00:00, 10.55it/s]\n",
      "Lin.Train Epoch: [91] Loss: 1.1585 ACC: 66.66: 100%|██████████| 79/79 [00:04<00:00, 19.24it/s]\n",
      "Lin.Test Epoch: [91] Loss: 1.1460 ACC: 66.75% : 100%|██████████| 16/16 [00:01<00:00, 11.17it/s]\n",
      "Lin.Train Epoch: [92] Loss: 1.1633 ACC: 66.66: 100%|██████████| 79/79 [00:03<00:00, 20.08it/s]\n",
      "Lin.Test Epoch: [92] Loss: 1.1354 ACC: 67.05% : 100%|██████████| 16/16 [00:01<00:00, 10.48it/s]\n",
      "Lin.Train Epoch: [93] Loss: 1.1500 ACC: 67.24: 100%|██████████| 79/79 [00:04<00:00, 19.60it/s]\n",
      "Lin.Test Epoch: [93] Loss: 1.1340 ACC: 67.03% : 100%|██████████| 16/16 [00:01<00:00, 10.90it/s]\n",
      "Lin.Train Epoch: [94] Loss: 1.1455 ACC: 66.99: 100%|██████████| 79/79 [00:04<00:00, 19.57it/s]\n",
      "Lin.Test Epoch: [94] Loss: 1.1310 ACC: 67.60% : 100%|██████████| 16/16 [00:01<00:00, 10.26it/s]\n",
      "Lin.Train Epoch: [95] Loss: 1.1536 ACC: 66.81: 100%|██████████| 79/79 [00:03<00:00, 20.16it/s]\n",
      "Lin.Test Epoch: [95] Loss: 1.1274 ACC: 67.33% : 100%|██████████| 16/16 [00:01<00:00, 10.95it/s]\n",
      "Lin.Train Epoch: [96] Loss: 1.1595 ACC: 67.23: 100%|██████████| 79/79 [00:03<00:00, 20.86it/s]\n",
      "Lin.Test Epoch: [96] Loss: 1.1273 ACC: 67.58% : 100%|██████████| 16/16 [00:01<00:00, 11.08it/s]\n",
      "Lin.Train Epoch: [97] Loss: 1.1471 ACC: 66.97: 100%|██████████| 79/79 [00:03<00:00, 20.29it/s]\n",
      "Lin.Test Epoch: [97] Loss: 1.1265 ACC: 67.33% : 100%|██████████| 16/16 [00:01<00:00, 10.39it/s]\n",
      "Lin.Train Epoch: [98] Loss: 1.1350 ACC: 67.11: 100%|██████████| 79/79 [00:03<00:00, 20.52it/s]\n",
      "Lin.Test Epoch: [98] Loss: 1.1242 ACC: 67.27% : 100%|██████████| 16/16 [00:01<00:00, 11.28it/s]\n",
      "Lin.Train Epoch: [99] Loss: 1.1577 ACC: 66.88: 100%|██████████| 79/79 [00:03<00:00, 20.03it/s]\n",
      "Lin.Test Epoch: [99] Loss: 1.1259 ACC: 67.22% : 100%|██████████| 16/16 [00:01<00:00, 10.58it/s]\n",
      "Lin.Train Epoch: [100] Loss: 1.1409 ACC: 67.14: 100%|██████████| 79/79 [00:04<00:00, 19.21it/s]\n",
      "Lin.Test Epoch: [100] Loss: 1.1258 ACC: 67.40% : 100%|██████████| 16/16 [00:01<00:00, 10.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1258106384277344,\n",
       " 67.4,\n",
       " LinearClassifier(\n",
       "   (classifier): Linear(in_features=512, out_features=40, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_id = 1\n",
    "lin_epoch = 100\n",
    "num_class = np.sum(args.class_split[:task_id+1])\n",
    "classifier = LinearClassifier(num_classes = num_class).to(device)\n",
    "lin_optimizer = torch.optim.SGD(classifier.parameters(), 0.2, momentum=0.9, weight_decay=0) # Infomax: no weight decay, epoch 100, cosine scheduler\n",
    "lin_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(lin_optimizer, lin_epoch, eta_min=0.002) #scheduler + values ref: infomax paper\n",
    "linear_evaluation(model2, train_data_loaders_linear[:task_id+1], test_data_loaders[:task_id+1], lin_optimizer,classifier, lin_scheduler, lin_epoch, device, task_id)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_top_k(outputs, targets, top_k=(1,5)):\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.argsort(outputs, dim=-1, descending=True)\n",
    "        result= []\n",
    "        for k in top_k:\n",
    "            correct_k = torch.sum((prediction[:, 0:k] == targets.unsqueeze(dim=-1)).any(dim=-1).float()).item() \n",
    "            result.append(correct_k)\n",
    "        return result\n",
    "    \n",
    "def contrastive_train2(net, data_loader, task_id, optimizer, classifier, scheduler, epochs, device):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        net.eval() # for not update batchnorm \n",
    "        total_num, train_bar = 0, tqdm(data_loader)\n",
    "        linear_loss = 0.0\n",
    "        for data_tuple in train_bar:\n",
    "            # Forward prop of the model with single augmented batch\n",
    "            pos_1, targets = data_tuple\n",
    "            pos_1 = torch.cat((pos_1, -pos_1), dim=0)\n",
    "            # print(pos_1.shape)\n",
    "            pos_1 = pos_1.to(device)\n",
    "            features = net(pos_1)\n",
    "\n",
    "            # Batchsize\n",
    "            batchsize_bc = features.shape[0]\n",
    "            targets = torch.zeros(targets.shape[0],dtype=torch.long).to(device)\n",
    "            targets = torch.cat((targets, torch.ones(targets.shape[0],dtype=torch.long).to(device)), dim=0)\n",
    "            # print(targets.shape)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            logits = classifier(features.detach()) \n",
    "\n",
    "            # Cross Entropy Loss \n",
    "            linear_loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "            # Backpropagation part\n",
    "            optimizer.zero_grad()\n",
    "            linear_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulating number of examples, losses and correct predictions\n",
    "            total_num += batchsize_bc\n",
    "            linear_loss += linear_loss.item() * batchsize_bc\n",
    "\n",
    "            train_bar.set_description('Lin.Train Epoch: [{}] Loss: {:.4f}'.format(epoch, linear_loss / total_num))\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        # wandb.log({\" Linear Layer Train Loss \": linear_loss / total_num, \" Epoch \": epoch})\n",
    "    return linear_loss/total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedml_academic",
   "language": "python",
   "name": "fedml_academic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
