{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duygu/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"\")))\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "\n",
    "from dataloaders.dataloader_cifar10 import get_cifar10\n",
    "from dataloaders.dataloader_cifar100 import get_cifar100\n",
    "from utils.eval_metrics import linear_evaluation, get_t_SNE_plot\n",
    "from models.linear_classifer import LinearClassifier\n",
    "from models.ssl import  SimSiam, Siamese, Encoder, Predictor\n",
    "\n",
    "from trainers.train_simsiam import train_simsiam\n",
    "from trainers.train_infomax import train_infomax\n",
    "from trainers.train_barlow import train_barlow\n",
    "\n",
    "from trainers.train_PFR import train_PFR_simsiam\n",
    "from trainers.train_PFR_contrastive import train_PFR_contrastive_simsiam\n",
    "from trainers.train_contrastive import train_contrastive_simsiam\n",
    "from trainers.train_ering import train_ering_simsiam\n",
    "\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from utils.lr_schedulers import LinearWarmupCosineAnnealingLR, SimSiamScheduler\n",
    "from utils.eval_metrics import Knn_Validation_cont\n",
    "from copy import deepcopy\n",
    "from loss import invariance_loss,CovarianceLoss,ErrorCovarianceLoss\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    \"\"\"Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\"\"\"\n",
    "\n",
    "    def __init__(self, sigma=[0.1, 2.0]):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = torchvision.transforms.functional.gaussian_blur(x,kernel_size=[3,3],sigma=sigma)#kernel size and sigma are open problems but right now seems ok!\n",
    "        return x\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    normalization = 'group'\n",
    "    weight_standard = True\n",
    "    same_lr = False\n",
    "    pretrain_batch_size = 512\n",
    "    pretrain_warmup_epochs = 10\n",
    "    pretrain_warmup_lr = 3e-3\n",
    "    pretrain_base_lr = 0.03\n",
    "    pretrain_momentum = 0.9\n",
    "    pretrain_weight_decay = 5e-4\n",
    "    min_lr = 0.00\n",
    "    lambdap = 1.0\n",
    "    appr = 'barlow_PFR'\n",
    "    knn_report_freq = 10\n",
    "    cuda_device = 5\n",
    "    num_workers = 8\n",
    "    contrastive_ratio = 0.001\n",
    "    dataset = 'cifar100'\n",
    "    class_split = [25,25,25,25]\n",
    "    epochs = [500,500,500,500]\n",
    "    cov_loss_weight = 1.0\n",
    "    sim_loss_weight = 250.0\n",
    "    info_loss = 'invariance'\n",
    "    lambda_norm = 1.0\n",
    "    subspace_rate = 0.99\n",
    "    lambda_param = 5e-3\n",
    "    bsize = 32\n",
    "    msize = 150\n",
    "    proj_hidden = 2048\n",
    "    proj_out = 2048 #infomax\n",
    "    pred_hidden = 512\n",
    "    pred_out = 2048\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset == \"cifar10\":\n",
    "        get_dataloaders = get_cifar10\n",
    "        num_classes=10\n",
    "elif args.dataset == \"cifar100\":\n",
    "    get_dataloaders = get_cifar100\n",
    "    num_classes=100\n",
    "assert sum(args.class_split) == num_classes\n",
    "assert len(args.class_split) == len(args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "num_worker = args.num_workers\n",
    "#device\n",
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wandb init\n",
    "wandb.init(project=\"CSSL\",  entity=\"yavuz-team\",\n",
    "            mode=\"disabled\",\n",
    "            config=args,\n",
    "            name= str(args.dataset) + '-algo' + str(args.appr) + \"-e\" + str(args.epochs) + \"-b\" \n",
    "            + str(args.pretrain_batch_size) + \"-lr\" + str(args.pretrain_base_lr)+\"-CS\"+str(args.class_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    transform = T.Compose([\n",
    "            T.RandomResizedCrop(size=32, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)]), p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([GaussianBlur()], p=0.5), \n",
    "            T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])])\n",
    "\n",
    "    transform_prime = T.Compose([\n",
    "            T.RandomResizedCrop(size=32, scale=(0.2, 1.0)),\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomApply(torch.nn.ModuleList([T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)]), p=0.8),\n",
    "            T.RandomGrayscale(p=0.2),\n",
    "            T.RandomApply([GaussianBlur()], p=0.5), \n",
    "            T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.247, 0.243, 0.261])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Dataloaders..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Dataloaders\n",
    "print(\"Creating Dataloaders..\")\n",
    "#Class Based\n",
    "train_data_loaders, train_data_loaders_knn, test_data_loaders, _, train_data_loaders_linear, train_data_loaders_pure  = get_dataloaders(transform, transform_prime, \\\n",
    "                                    classes=args.class_split, valid_rate = 0.00, batch_size=args.pretrain_batch_size, seed = 0, num_worker= num_worker)\n",
    "_, train_data_loaders_knn_all, test_data_loaders_all, _, train_data_loaders_linear_all, train_data_loaders_pure_all = get_dataloaders(transform, transform_prime, \\\n",
    "                                        classes=[num_classes], valid_rate = 0.00, batch_size=args.pretrain_batch_size, seed = 0, num_worker= num_worker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:\" + str(args.cuda_device) if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    proj_hidden = args.proj_hidden\n",
    "    proj_out = args.proj_out\n",
    "    encoder = Encoder(hidden_dim=proj_hidden, output_dim=proj_out, normalization = args.normalization, weight_standard = args.weight_standard,appr_name =args.appr)\n",
    "    model = Siamese(encoder)\n",
    "    model.to(device) #automatically detects from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model here\n",
    "file_name = 'checkpoints/checkpoint_cifar100-algocassle_barlow-e[500, 500, 500, 500]-b256-lr0.06-CS[25, 25, 25, 25]acc_60.42999999999999.pth.tar'\n",
    "dict = torch.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['arch', 'lr', 'state_dict', 'optimizer', 'loss', 'encoder', 'classifier'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.temporal_projector = nn.Sequential(\n",
    "            nn.Linear(args.proj_out, args.proj_hidden, bias=False),\n",
    "            nn.BatchNorm1d(args.proj_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(args.proj_hidden, args.proj_out),\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_activations(net,train_data_loader, device, orth_set): #distributed\n",
    "\n",
    "    layer_names = ['0.weight', '3.0.conv1.weight', '3.0.conv2.weight', '3.1.conv1.weight', \\\n",
    "    '3.1.conv2.weight', '4.0.conv1.weight', '4.0.conv2.weight', '4.0.shortcut.0.weight', \\\n",
    "        '4.1.conv1.weight', '4.1.conv2.weight', '5.0.conv1.weight', '5.0.conv2.weight', \\\n",
    "            '5.0.shortcut.0.weight', '5.1.conv1.weight', '5.1.conv2.weight', '6.0.conv1.weight', \\\n",
    "                '6.0.conv2.weight', '6.0.shortcut.0.weight', '6.1.conv1.weight', '6.1.conv2.weight']\n",
    "                \n",
    "    stride_list = [1, 1,1,1,1, 2,1,2,1,1, 2,1,2,1,1, 2,1,2,1,1]\n",
    "    padding_list = []\n",
    "    kernel_list = []\n",
    "\n",
    "    net.eval()\n",
    "    activation = {}\n",
    "    for key in layer_names:\n",
    "        activation[key] = []\n",
    "        \n",
    "    for batch_index, (x, _) in enumerate(train_data_loader):\n",
    "        #print(x.shape)\n",
    "        if batch_index > 15: break\n",
    "        _ = net(x.to(device))\n",
    "        act_list = [x.to(device), \n",
    "            net[3][0].act['conv_0'], net[3][0].act['conv_1'], net[3][1].act['conv_0'], net[3][1].act['conv_1'],\n",
    "            net[4][0].act['conv_0'], net[4][0].act['conv_1'], net[4][0].act['conv_0'], net[4][1].act['conv_0'], net[4][1].act['conv_1'],\n",
    "            net[5][0].act['conv_0'], net[5][0].act['conv_1'], net[5][0].act['conv_0'], net[5][1].act['conv_0'], net[5][1].act['conv_1'],\n",
    "            net[6][0].act['conv_0'], net[6][0].act['conv_1'], net[6][0].act['conv_0'], net[6][1].act['conv_0'], net[6][1].act['conv_1']]\n",
    "        for j, key in enumerate(layer_names):\n",
    "            activation[key].append(act_list[j].detach().cpu())\n",
    "    for name in activation.keys():\n",
    "        activation[name] = torch.cat(activation[name],dim=0)\n",
    "        if \"shortcut\" not in name:\n",
    "            activation[name] = F.pad(activation[name], (1, 1, 1, 1), \"constant\", 0)\n",
    "            kernel_list.append(3)\n",
    "        else:\n",
    "            kernel_list.append(1)\n",
    "    \n",
    "    \n",
    "    device = 'cpu'\n",
    "    for i in range(len(stride_list)):\n",
    "        layer_name = layer_names[i]\n",
    "        print(i)\n",
    "       \n",
    "        st = stride_list[i]\n",
    "        #pad = padding_list[i]\n",
    "        kernel = kernel_list[i]\n",
    "\n",
    "        act = activation[layer_name]\n",
    "\n",
    "        unfolder = torch.nn.Unfold(kernel, dilation=1, padding=0, stride= st)\n",
    "\n",
    "        mat = unfolder(act)\n",
    "        mat = mat.permute(0,2,1)\n",
    "        mat = mat.reshape(-1, mat.shape[2])\n",
    "        \n",
    "        mat = mat.T.to(device)\n",
    "        ratio = 1\n",
    "        if orth_set[layer_name] is not None:\n",
    "            U = orth_set[layer_name].to(device)\n",
    "            projected = U @ U.T @ mat\n",
    "            remaining = mat - projected\n",
    "            rem_norm = torch.norm(remaining)\n",
    "            orj_norm = torch.norm(mat)\n",
    "            ratio = (rem_norm / orj_norm).cpu()\n",
    "            mat = remaining\n",
    "        activation[layer_name] = mat.cpu()\n",
    "    return activation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "def expand_orth_set(activations, orth_set, eps = 0.95):\n",
    "    #print(activations.keys())\n",
    "    for key in activations.keys():\n",
    "        if orth_set[key] == None:\n",
    "            projected = torch.zeros(1)\n",
    "        else:\n",
    "            projected = orth_set[key]  @ orth_set[key].T @ activations[key] \n",
    "\n",
    "        remaining = activations[key] - projected\n",
    "        ratio = torch.norm(projected)**2 / torch.norm(activations[key])**2\n",
    "        eps_new = eps - ratio\n",
    "        #tot = torch.norm(remaining)**2\n",
    "\n",
    "        #find svds of remaining\n",
    "        remaining = remaining / (remaining.shape[1]) @ remaining.T\n",
    "\n",
    "        U, S, V = torch.svd(remaining.cpu())\n",
    "        #if key == '3.1.conv2.weight':\n",
    "        #    return S\n",
    "\n",
    "        tot = torch.linalg.norm(S, ord = 1)#because already square\n",
    "        \n",
    "        \n",
    "        #S = torch.sqrt(S)\n",
    "        #U, S, V = randomized_svd(remaining.cpu().numpy(),n_components=remaining.shape[0])\n",
    "        U = torch.tensor(U)\n",
    "        S = torch.tensor(S)\n",
    "        #U = U.cuda()\n",
    "        #find how many singular vectors will be used\n",
    "        for i in range(len(S)):\n",
    "            hand = torch.linalg.norm(S[0:i+1], ord = 1)\n",
    "                #print(eps_new)\n",
    "            \n",
    "            if  hand / tot > eps_new:\n",
    "                break\n",
    "            \n",
    "        print(U[:,0:i+1].shape)\n",
    "        if orth_set[key] == None:\n",
    "            orth_set[key] = U[:,0:i+1]\n",
    "        else:\n",
    "            orth_set[key] = torch.cat((orth_set[key], U[:,0:i+1]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_children(model: torch.nn.Module):\n",
    "    # get children form model!\n",
    "    children = list(model.children())\n",
    "    flatt_children = []\n",
    "    if children == []:\n",
    "        # if model has no children; model is last child! :O\n",
    "        return model\n",
    "    else:\n",
    "       # look for children from children... to the last child!\n",
    "       for child in children:\n",
    "            try:\n",
    "                flatt_children.extend(get_children(child))\n",
    "            except TypeError:\n",
    "                flatt_children.append(get_children(child))\n",
    "    return flatt_children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_modules = get_children(model)\n",
    "# for m in all_modules:\n",
    "#     if isinstance(m, nn.GroupNorm):\n",
    "#         m.track_running_stats = False\n",
    "# for key,p in model.encoder.backbone.named_parameters():\n",
    "#     if 'bias' in key or 'bn' in key or 'shortcut.1' in key or '1.weight' == key:\n",
    "#         print(key)\n",
    "#         p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_memory(memory, dataloader, size):\n",
    "    indices = np.random.choice(len(dataloader.dataset), size=size, replace=False)\n",
    "    x, _ =  dataloader.dataset[indices]\n",
    "    memory = torch.cat((memory, x), dim=0)\n",
    "    return memory\n",
    "\n",
    "def train_LRD_replay_infomax(model, train_data_loaders, knn_train_data_loaders, train_data_loaders_pure, test_data_loaders, device, args, transform, transform_prime):#just for 2 tasks\n",
    "    \n",
    "    memory = torch.Tensor()\n",
    "\n",
    "    epoch_counter = 0\n",
    "    old_model = None\n",
    "    criterion = nn.CosineSimilarity(dim=1)\n",
    "    Q = None\n",
    "\n",
    "    for task_id, loader in enumerate(train_data_loaders):\n",
    "        # Optimizer and Scheduler\n",
    "        model.task_id = task_id\n",
    "        init_lr = args.pretrain_base_lr*args.pretrain_batch_size/256.\n",
    "        if task_id != 0 and args.same_lr != True:\n",
    "            init_lr = init_lr / 10\n",
    "\n",
    "        project_dim = args.proj_out\n",
    "        covarince_loss = CovarianceLoss(project_dim,device=device)\n",
    "\n",
    "            \n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=init_lr, momentum=args.pretrain_momentum, weight_decay= args.pretrain_weight_decay)\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=args.pretrain_warmup_epochs , max_epochs=args.epochs[task_id],warmup_start_lr=args.pretrain_warmup_lr,eta_min=args.min_lr) #eta_min=2e-4 is removed scheduler + values ref: infomax paper\n",
    "\n",
    "        loss_ = []\n",
    "        for epoch in range(args.epochs[task_id]):\n",
    "            start = time.time()\n",
    "            model.train()\n",
    "            epoch_loss_task = []\n",
    "            epoch_loss_kd = []\n",
    "            epoch_loss_norm = []\n",
    "            epoch_loss_norm_old = []\n",
    "            for x1, x2, y in loader:\n",
    "                x1,x2 = x1.to(device), x2.to(device)\n",
    "                f1 = model.encoder.backbone(x1).squeeze() # NxC\n",
    "                f2 = model.encoder.backbone(x2).squeeze() # NxC\n",
    "\n",
    "                if task_id > 0:\n",
    "                    indices = np.random.choice(len(memory), size=min(args.bsize, len(memory)), replace=False)\n",
    "                    x = memory[indices].to(device)\n",
    "                    x1_old, x2_old = transform(x), transform_prime(x)\n",
    "                    f1_old = model.encoder.backbone(x1_old).squeeze() # NxC\n",
    "                    f2_old = model.encoder.backbone(x2_old).squeeze() # NxC\n",
    "\n",
    "                if Q != None:#let's do projection\n",
    "                    f1_projected = f1 @ Q @ Q.T  \n",
    "                    f2_projected = f2 @ Q @ Q.T\n",
    "\n",
    "                    f1 = f1 - f1_projected\n",
    "                    f2 = f2 - f2_projected\n",
    "\n",
    "                    norm_loss_1 = torch.norm(f1_projected,dim =1) / (torch.norm(f1,dim =1) + 0.0000001) \n",
    "                    norm_loss_1 = torch.mean(norm_loss_1)\n",
    "\n",
    "                    norm_loss_2 = torch.norm(f2_projected,dim =1) / (torch.norm(f2,dim =1) + 0.0000001) \n",
    "                    norm_loss_2 = torch.mean(norm_loss_2)\n",
    "\n",
    "                    loss_norm = (norm_loss_1 + norm_loss_2) / 2\n",
    "\n",
    "                    f1_projected_old = f1_old @ Q @ Q.T  \n",
    "                    f2_projected_old = f2_old @ Q @ Q.T\n",
    "\n",
    "                    f1_rem_old = f1_old - f1_projected_old\n",
    "                    f2_rem_old = f2_old - f2_projected_old\n",
    "\n",
    "                    norm_loss_1 = torch.norm(f1_rem_old,dim =1) / (torch.norm(f1_projected_old,dim =1) + 0.0000001) \n",
    "                    norm_loss_1 = torch.mean(norm_loss_1)\n",
    "\n",
    "                    norm_loss_2 = torch.norm(f2_rem_old,dim =1) / (torch.norm(f2_projected_old,dim =1) + 0.0000001) \n",
    "                    norm_loss_2 = torch.mean(norm_loss_2)\n",
    "\n",
    "                    loss_norm_old = (norm_loss_1 + norm_loss_2) / 2\n",
    "                else:\n",
    "                    loss_norm = torch.tensor(0)\n",
    "                    loss_norm_old = torch.tensor(0)\n",
    "\n",
    "\n",
    "                z1 = model.encoder.projector(f1) # NxC\n",
    "                z2 = model.encoder.projector(f2) # NxC\n",
    "\n",
    "                z1 = F.normalize(z1, p=2)\n",
    "                z2 = F.normalize(z2, p=2)\n",
    "                cov_loss =  covarince_loss(z1, z2)\n",
    "                sim_loss =  invariance_loss(z1, z2)\n",
    "                \n",
    "\n",
    "                loss_task = (args.sim_loss_weight * sim_loss) + (args.cov_loss_weight * cov_loss) \n",
    "\n",
    "                if task_id != 0: #do Distillation\n",
    "                    f1Old = oldModel(x1).squeeze().detach()\n",
    "                    f2Old = oldModel(x2).squeeze().detach()\n",
    "\n",
    "                    lossKD = (-(criterion(f1_projected, f1Old).mean() * 0.5\n",
    "                                            + criterion(f2_projected, f2Old).mean() * 0.5) )\n",
    "                else:\n",
    "                    lossKD = torch.tensor(0)\n",
    "                \n",
    "\n",
    "\n",
    "                epoch_loss_task.append(loss_task.item())\n",
    "                epoch_loss_kd.append(lossKD.item())\n",
    "                epoch_loss_norm.append(loss_norm.item())\n",
    "                epoch_loss_norm_old.append(loss_norm_old.item())\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_task +  args.lambdap * lossKD + args.lambda_norm * loss_norm +  args.lambda_norm * loss_norm_old\n",
    "                loss.backward()\n",
    "            \n",
    "                optimizer.step() \n",
    "                    \n",
    "            epoch_counter += 1\n",
    "            scheduler.step()\n",
    "            loss_.append(np.mean(epoch_loss_task))\n",
    "            end = time.time()\n",
    "            print('epoch end')\n",
    "            if (epoch+1) % args.knn_report_freq == 0:\n",
    "                knn_acc, task_acc_arr = Knn_Validation_cont(model, knn_train_data_loaders[:task_id+1], test_data_loaders[:task_id+1], device=device, K=200, sigma=0.5) \n",
    "                wandb.log({\" Global Knn Accuracy \": knn_acc, \" Epoch \": epoch_counter})\n",
    "                for i, acc in enumerate(task_acc_arr):\n",
    "                    wandb.log({\" Knn Accuracy Task-\"+str(i): acc, \" Epoch \": epoch_counter})\n",
    "                    print(f\" Knn Accuracy Task- {str(i)} : {acc},  Epoch : {epoch_counter}\")\n",
    "                print(f'Task {task_id:2d} | Epoch {epoch:3d} | Time:  {end-start:.1f}s  | Loss: {np.mean(epoch_loss_task):.4f} | KDLoss: {np.mean(epoch_loss_kd):.4f} | Norm_Loss: {np.mean(epoch_loss_norm):.4f}  | Norm_Loss Old:  {np.mean(epoch_loss_norm_old):.4f}   | Knn:  {knn_acc*100:.2f}')\n",
    "                print(task_acc_arr)\n",
    "            else:\n",
    "                print(f'Task {task_id:2d} | Epoch {epoch:3d} | Time:  {end-start:.1f}s  | Loss: {np.mean(epoch_loss_task):.4f} | KDLoss: {np.mean(epoch_loss_kd):.4f} | Norm_Loss: {np.mean(epoch_loss_norm):.4f} | Norm_Loss Old:  {np.mean(epoch_loss_norm_old):.4f} ')\n",
    "        \n",
    "            wandb.log({\" Average Training Loss \": np.mean(epoch_loss_task), \" Epoch \": epoch_counter, \" Average KD Loss \": np.mean(epoch_loss_kd) , \" Average Norm Loss \": np.mean(epoch_loss_norm) , \" Average Norm Loss Old\": np.mean(epoch_loss_norm_old) })  \n",
    "            wandb.log({\" lr \": optimizer.param_groups[0]['lr'], \" Epoch \": epoch_counter})\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            oldModel = deepcopy(model.encoder.backbone.eval())  # save t-1 model\n",
    "        oldModel.to(device)\n",
    "        oldModel.eval()\n",
    "        for param in oldModel.parameters(): #Freeze old model\n",
    "            param.requires_grad = False\n",
    "\n",
    "        Q = extract_subspace(model, knn_train_data_loaders[task_id], rate= args.subspace_rate,device = device, Q_prev = Q)\n",
    "        Q = Q.to(device)\n",
    "\n",
    "        memory = update_memory(memory, train_data_loaders_pure[task_id], args.msize)\n",
    "\n",
    "        #file_name = './checkpoints/checkpoint_' + str(args.dataset) + '-algo' + str(args.appr) + \"-e\" + str(args.epochs) + \"-b\" + str(args.pretrain_batch_size) + \"-lr\" + str(args.pretrain_base_lr)\n",
    "        #+\"-CS\"+str(args.class_split) + 'task_' + str(task_id) + 'lambdap_' + str(args.lambdap) + 'lambda_norm_' + str(args.lambda_norm) + 'same_lr_' + str(args.same_lr) + 'norm_' + str(normalization) + 'ws_' + str(args.weight_standard) + '.pth.tar' \n",
    "        \n",
    "        file_name = './checkpoints/checkpoint_' + str(args.dataset) + '-algo' + str(args.appr) + \"-e\" + str(args.epochs) + \"-b\" + str(args.pretrain_batch_size) + \"-lr\" + str(args.pretrain_base_lr) + \"-CS\" + str(args.class_split) + '_task_' + str(task_id) + '_lambdap_' + str(args.lambdap) + '_lambda_norm_' + str(args.lambda_norm) + '_same_lr_' + str(args.same_lr) + '_norm_' + str(args.normalization) + '_ws_' + str(args.weight_standard) + '.pth.tar'\n",
    "\n",
    "        # save your encoder network\n",
    "        torch.save({\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'optimizer' : optimizer.state_dict(),\n",
    "                        'encoder': model.encoder.backbone.state_dict(),\n",
    "                    }, file_name)\n",
    "\n",
    "\n",
    "    return model, loss_, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    normalization = 'group'\n",
    "    weight_standard = True\n",
    "    same_lr = False\n",
    "    pretrain_batch_size = 512\n",
    "    pretrain_warmup_epochs = 10\n",
    "    pretrain_warmup_lr = 3e-3\n",
    "    pretrain_base_lr = 0.10\n",
    "    pretrain_momentum = 0.9\n",
    "    pretrain_weight_decay = 5e-4\n",
    "    min_lr = 0.00\n",
    "    lambdap = 100.0\n",
    "    appr = 'LRD_infomax'\n",
    "    knn_report_freq = 1\n",
    "    cuda_device = 2\n",
    "    num_workers = 8\n",
    "    contrastive_ratio = 0.001\n",
    "    dataset = 'cifar10'\n",
    "    class_split = [5,5]\n",
    "    epochs = [50,500]\n",
    "    cov_loss_weight = 1.0\n",
    "    sim_loss_weight = 250.0\n",
    "    info_loss = 'invariance'\n",
    "    lambda_norm = 1.0\n",
    "    subspace_rate = 0.98\n",
    "    lambda_param = 5e-3\n",
    "    bsize = 32\n",
    "    msize = 150\n",
    "    proj_hidden = 2048\n",
    "    proj_out = 64 #infomax\n",
    "    pred_hidden = 512\n",
    "    pred_out = 2048\n",
    "    m_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.lambda_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch end\n",
      " Knn Accuracy Task- 0 : 0.5298,  Epoch : 1\n",
      "Task  0 | Epoch   0 | Time:  49.0s  | Loss: 13.2724 | KDLoss: 0.0000 | Norm_Loss: 0.0000  | Norm_Loss Old:  0.0000   | Knn:  52.98\n",
      "[0.5298]\n",
      "epoch end\n",
      " Knn Accuracy Task- 0 : 0.5468,  Epoch : 2\n",
      "Task  0 | Epoch   1 | Time:  29.8s  | Loss: 13.9099 | KDLoss: 0.0000 | Norm_Loss: 0.0000  | Norm_Loss Old:  0.0000   | Knn:  54.68\n",
      "[0.5468]\n",
      "epoch end\n",
      " Knn Accuracy Task- 0 : 0.5546,  Epoch : 3\n",
      "Task  0 | Epoch   2 | Time:  30.4s  | Loss: 12.6586 | KDLoss: 0.0000 | Norm_Loss: 0.0000  | Norm_Loss Old:  0.0000   | Knn:  55.46\n",
      "[0.5546]\n",
      "epoch end\n",
      " Knn Accuracy Task- 0 : 0.576,  Epoch : 4\n",
      "Task  0 | Epoch   3 | Time:  30.8s  | Loss: 12.5329 | KDLoss: 0.0000 | Norm_Loss: 0.0000  | Norm_Loss Old:  0.0000   | Knn:  57.60\n",
      "[0.576]\n",
      "epoch end\n",
      " Knn Accuracy Task- 0 : 0.593,  Epoch : 5\n",
      "Task  0 | Epoch   4 | Time:  31.1s  | Loss: 12.3795 | KDLoss: 0.0000 | Norm_Loss: 0.0000  | Norm_Loss Old:  0.0000   | Knn:  59.30\n",
      "[0.593]\n",
      "epoch end\n",
      " Knn Accuracy Task- 0 : 0.6196,  Epoch : 6\n",
      "Task  0 | Epoch   5 | Time:  30.6s  | Loss: 12.2295 | KDLoss: 0.0000 | Norm_Loss: 0.0000  | Norm_Loss Old:  0.0000   | Knn:  61.96\n",
      "[0.6196]\n",
      "epoch end\n",
      " Knn Accuracy Task- 0 : 0.6334,  Epoch : 7\n",
      "Task  0 | Epoch   6 | Time:  30.8s  | Loss: 12.0478 | KDLoss: 0.0000 | Norm_Loss: 0.0000  | Norm_Loss Old:  0.0000   | Knn:  63.34\n",
      "[0.6334]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_87138/1677097508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_LRD_replay_infomax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loaders_knn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data_loaders_pure\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_data_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform_prime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_87138/3435796217.py\u001b[0m in \u001b[0;36mtrain_LRD_replay_infomax\u001b[0;34m(model, train_data_loaders, knn_train_data_loaders, train_data_loaders_pure, test_data_loaders, device, args, transform, transform_prime)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mepoch_loss_norm_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, loss, optimizer = train_LRD_replay_infomax(model, train_data_loaders, train_data_loaders_knn,train_data_loaders_pure , test_data_loaders, device, args,transform,transform_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'infomax' in args.appr or 'barlow' in args.appr:\n",
    "    proj_hidden = args.proj_hidden\n",
    "    proj_out = args.proj_out\n",
    "    encoder = Encoder(hidden_dim=proj_hidden, output_dim=proj_out, normalization = args.normalization, weight_standard = args.weight_standard,appr_name=args.appr)\n",
    "    old_model = Siamese(encoder)\n",
    "    old_model.to(device) #automatically detects from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_model.load_state_dict(dict['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:0! (when checking argument for argument weight in method wrapper__cudnn_convolution)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44370/1036516990.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_subspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_loaders_knn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubspace_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mold_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_44370/446098673.py\u001b[0m in \u001b[0;36mextract_subspace\u001b[0;34m(model, loader, rate, device, Q_prev)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;31m#out = model.encoder(x).squeeze().cpu().detach().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/Continual_Experiments/models/ssl.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mz1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/Continual_Experiments/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         return F.conv2d(x, weight, self.bias, self.stride,\n\u001b[0;32m---> 35\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:7 and cuda:0! (when checking argument for argument weight in method wrapper__cudnn_convolution)"
     ]
    }
   ],
   "source": [
    "Q = extract_subspace(old_model, train_data_loaders_knn[0], rate= args.subspace_rate,device = device,Q_prev = None)\n",
    "old_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9881, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11185/3644558521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/Continual_Experiments/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fedml_academic/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/Continual_Experiments/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         return F.conv2d(x, weight, self.bias, self.stride,\n\u001b[0;32m---> 35\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBasicBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "Q = Q.to(device)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "for x1, x2, y in train_data_loaders[1]:\n",
    "    x1,x2 = x1.to(device), x2.to(device)\n",
    "    f1 = model.encoder.backbone(x1).squeeze() # NxC\n",
    "    f2 = model.encoder.backbone(x2).squeeze() # NxC\n",
    "\n",
    "    if Q != None:#let's do projection\n",
    "        f1_projected = f1 @ Q @ Q.T  \n",
    "        f2_projected = f2 @ Q @ Q.T\n",
    "\n",
    "        f1 = f1 - f1_projected\n",
    "        f2 = f2 - f2_projected\n",
    "\n",
    "        norm_loss_1 = torch.norm(f1_projected,dim =1) / (torch.norm(f1,dim =1) + 0.0000001) \n",
    "        norm_loss_1 = torch.mean(norm_loss_1)\n",
    "\n",
    "        norm_loss_2 = torch.norm(f2_projected,dim =1) / (torch.norm(f2,dim =1) + 0.0000001) \n",
    "        norm_loss_2 = torch.mean(norm_loss_2)\n",
    "\n",
    "        loss_norm = (norm_loss_1 + norm_loss_2) / 2\n",
    "        print(loss_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_acc, task_acc_arr = Knn_Validation_cont(model, train_data_loaders_knn[:1], test_data_loaders[:task_id+1], device=device, K=200, sigma=0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Classifier Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lin.Train Epoch: [1] Loss: 1.9388 : 100%|| 98/98 [00:09<00:00, 10.29it/s]\n",
      "Lin.Test Epoch: [1] Loss: 1.7908 ACC@1: 63.40% ACC@5: 80.53% : 100%|| 20/20 [00:02<00:00,  6.98it/s]\n",
      "Lin.Train Epoch: [2] Loss: 1.6584 : 100%|| 98/98 [00:09<00:00, 10.25it/s]\n",
      "Lin.Test Epoch: [2] Loss: 1.6033 ACC@1: 69.04% ACC@5: 85.03% : 100%|| 20/20 [00:02<00:00,  7.09it/s]\n",
      "Lin.Train Epoch: [3] Loss: 1.4899 : 100%|| 98/98 [00:09<00:00, 10.60it/s]\n",
      "Lin.Test Epoch: [3] Loss: 1.4646 ACC@1: 71.50% ACC@5: 87.95% : 100%|| 20/20 [00:02<00:00,  6.97it/s]\n",
      "Lin.Train Epoch: [4] Loss: 1.3616 : 100%|| 98/98 [00:09<00:00, 10.46it/s]\n",
      "Lin.Test Epoch: [4] Loss: 1.3554 ACC@1: 72.52% ACC@5: 90.94% : 100%|| 20/20 [00:02<00:00,  7.05it/s]\n",
      "Lin.Train Epoch: [5] Loss: 1.2603 : 100%|| 98/98 [00:09<00:00, 10.20it/s]\n",
      "Lin.Test Epoch: [5] Loss: 1.2686 ACC@1: 73.70% ACC@5: 92.30% : 100%|| 20/20 [00:02<00:00,  6.93it/s]\n",
      "Lin.Train Epoch: [6] Loss: 1.1782 : 100%|| 98/98 [00:09<00:00, 10.29it/s]\n",
      "Lin.Test Epoch: [6] Loss: 1.1970 ACC@1: 73.93% ACC@5: 93.41% : 100%|| 20/20 [00:02<00:00,  7.09it/s]\n",
      "Lin.Train Epoch: [7] Loss: 1.1106 : 100%|| 98/98 [00:09<00:00, 10.54it/s]\n",
      "Lin.Test Epoch: [7] Loss: 1.1373 ACC@1: 74.77% ACC@5: 94.59% : 100%|| 20/20 [00:02<00:00,  6.90it/s]\n",
      "Lin.Train Epoch: [8] Loss: 1.0542 : 100%|| 98/98 [00:09<00:00, 10.21it/s]\n",
      "Lin.Test Epoch: [8] Loss: 1.0876 ACC@1: 75.28% ACC@5: 95.05% : 100%|| 20/20 [00:02<00:00,  6.88it/s]\n",
      "Lin.Train Epoch: [9] Loss: 1.0064 : 100%|| 98/98 [00:09<00:00, 10.12it/s]\n",
      "Lin.Test Epoch: [9] Loss: 1.0446 ACC@1: 75.87% ACC@5: 96.11% : 100%|| 20/20 [00:02<00:00,  6.86it/s]\n",
      "Lin.Train Epoch: [10] Loss: 0.9655 : 100%|| 98/98 [00:09<00:00, 10.54it/s]\n",
      "Lin.Test Epoch: [10] Loss: 1.0076 ACC@1: 76.36% ACC@5: 96.50% : 100%|| 20/20 [00:02<00:00,  6.97it/s]\n",
      "Lin.Train Epoch: [11] Loss: 0.9301 : 100%|| 98/98 [00:09<00:00, 10.12it/s]\n",
      "Lin.Test Epoch: [11] Loss: 0.9754 ACC@1: 76.62% ACC@5: 96.91% : 100%|| 20/20 [00:02<00:00,  6.97it/s]\n",
      "Lin.Train Epoch: [12] Loss: 0.8990 : 100%|| 98/98 [00:09<00:00, 10.22it/s]\n",
      "Lin.Test Epoch: [12] Loss: 0.9474 ACC@1: 77.07% ACC@5: 97.07% : 100%|| 20/20 [00:02<00:00,  6.70it/s]\n",
      "Lin.Train Epoch: [13] Loss: 0.8718 : 100%|| 98/98 [00:09<00:00, 10.42it/s]\n",
      "Lin.Test Epoch: [13] Loss: 0.9233 ACC@1: 77.22% ACC@5: 97.18% : 100%|| 20/20 [00:02<00:00,  6.98it/s]\n",
      "Lin.Train Epoch: [14] Loss: 0.8476 : 100%|| 98/98 [00:09<00:00, 10.50it/s]\n",
      "Lin.Test Epoch: [14] Loss: 0.9013 ACC@1: 77.56% ACC@5: 97.28% : 100%|| 20/20 [00:02<00:00,  6.92it/s]\n",
      "Lin.Train Epoch: [15] Loss: 0.8260 : 100%|| 98/98 [00:09<00:00, 10.21it/s]\n",
      "Lin.Test Epoch: [15] Loss: 0.8815 ACC@1: 77.87% ACC@5: 97.41% : 100%|| 20/20 [00:03<00:00,  6.47it/s]\n",
      "Lin.Train Epoch: [16] Loss: 0.8064 : 100%|| 98/98 [00:09<00:00, 10.28it/s]\n",
      "Lin.Test Epoch: [16] Loss: 0.8636 ACC@1: 78.11% ACC@5: 97.61% : 100%|| 20/20 [00:02<00:00,  7.07it/s]\n",
      "Lin.Train Epoch: [17] Loss: 0.7889 : 100%|| 98/98 [00:09<00:00, 10.38it/s]\n",
      "Lin.Test Epoch: [17] Loss: 0.8477 ACC@1: 78.16% ACC@5: 97.64% : 100%|| 20/20 [00:02<00:00,  6.89it/s]\n",
      "Lin.Train Epoch: [18] Loss: 0.7729 : 100%|| 98/98 [00:09<00:00, 10.08it/s]\n",
      "Lin.Test Epoch: [18] Loss: 0.8326 ACC@1: 78.54% ACC@5: 97.81% : 100%|| 20/20 [00:02<00:00,  6.87it/s]\n",
      "Lin.Train Epoch: [19] Loss: 0.7582 : 100%|| 98/98 [00:09<00:00, 10.18it/s]\n",
      "Lin.Test Epoch: [19] Loss: 0.8190 ACC@1: 78.80% ACC@5: 97.93% : 100%|| 20/20 [00:03<00:00,  6.63it/s]\n",
      "Lin.Train Epoch: [20] Loss: 0.7448 : 100%|| 98/98 [00:09<00:00, 10.52it/s]\n",
      "Lin.Test Epoch: [20] Loss: 0.8069 ACC@1: 78.95% ACC@5: 98.00% : 100%|| 20/20 [00:03<00:00,  6.58it/s]\n",
      "Lin.Train Epoch: [21] Loss: 0.7325 : 100%|| 98/98 [00:09<00:00, 10.26it/s]\n",
      "Lin.Test Epoch: [21] Loss: 0.7954 ACC@1: 79.06% ACC@5: 98.06% : 100%|| 20/20 [00:02<00:00,  6.69it/s]\n",
      "Lin.Train Epoch: [22] Loss: 0.7211 : 100%|| 98/98 [00:09<00:00, 10.22it/s]\n",
      "Lin.Test Epoch: [22] Loss: 0.7851 ACC@1: 79.19% ACC@5: 98.18% : 100%|| 20/20 [00:03<00:00,  6.49it/s]\n",
      "Lin.Train Epoch: [23] Loss: 0.7105 : 100%|| 98/98 [00:09<00:00, 10.45it/s]\n",
      "Lin.Test Epoch: [23] Loss: 0.7754 ACC@1: 79.43% ACC@5: 98.21% : 100%|| 20/20 [00:03<00:00,  6.64it/s]\n",
      "Lin.Train Epoch: [24] Loss: 0.7008 : 100%|| 98/98 [00:09<00:00, 10.46it/s]\n",
      "Lin.Test Epoch: [24] Loss: 0.7665 ACC@1: 79.46% ACC@5: 98.26% : 100%|| 20/20 [00:03<00:00,  6.44it/s]\n",
      "Lin.Train Epoch: [25] Loss: 0.6916 : 100%|| 98/98 [00:09<00:00,  9.94it/s]\n",
      "Lin.Test Epoch: [25] Loss: 0.7582 ACC@1: 79.68% ACC@5: 98.27% : 100%|| 20/20 [00:03<00:00,  6.11it/s]\n",
      "Lin.Train Epoch: [26] Loss: 0.6831 : 100%|| 98/98 [00:09<00:00, 10.25it/s]\n",
      "Lin.Test Epoch: [26] Loss: 0.7504 ACC@1: 79.81% ACC@5: 98.32% : 100%|| 20/20 [00:02<00:00,  6.79it/s]\n",
      "Lin.Train Epoch: [27] Loss: 0.6751 : 100%|| 98/98 [00:09<00:00, 10.06it/s]\n",
      "Lin.Test Epoch: [27] Loss: 0.7427 ACC@1: 80.15% ACC@5: 98.38% : 100%|| 20/20 [00:02<00:00,  6.95it/s]\n",
      "Lin.Train Epoch: [28] Loss: 0.6676 : 100%|| 98/98 [00:09<00:00, 10.17it/s]\n",
      "Lin.Test Epoch: [28] Loss: 0.7360 ACC@1: 80.17% ACC@5: 98.39% : 100%|| 20/20 [00:03<00:00,  6.39it/s]\n",
      "Lin.Train Epoch: [29] Loss: 0.6608 : 100%|| 98/98 [00:09<00:00,  9.85it/s]\n",
      "Lin.Test Epoch: [29] Loss: 0.7299 ACC@1: 80.21% ACC@5: 98.39% : 100%|| 20/20 [00:03<00:00,  6.40it/s]\n",
      "Lin.Train Epoch: [30] Loss: 0.6540 : 100%|| 98/98 [00:09<00:00, 10.28it/s]\n",
      "Lin.Test Epoch: [30] Loss: 0.7243 ACC@1: 80.22% ACC@5: 98.45% : 100%|| 20/20 [00:02<00:00,  6.82it/s]\n",
      "Lin.Train Epoch: [31] Loss: 0.6479 : 100%|| 98/98 [00:09<00:00, 10.08it/s]\n",
      "Lin.Test Epoch: [31] Loss: 0.7181 ACC@1: 80.31% ACC@5: 98.41% : 100%|| 20/20 [00:02<00:00,  6.77it/s]\n",
      "Lin.Train Epoch: [32] Loss: 0.6421 : 100%|| 98/98 [00:10<00:00,  9.75it/s]\n",
      "Lin.Test Epoch: [32] Loss: 0.7127 ACC@1: 80.45% ACC@5: 98.46% : 100%|| 20/20 [00:03<00:00,  6.00it/s]\n",
      "Lin.Train Epoch: [33] Loss: 0.6366 : 100%|| 98/98 [00:09<00:00,  9.90it/s]\n",
      "Lin.Test Epoch: [33] Loss: 0.7077 ACC@1: 80.51% ACC@5: 98.46% : 100%|| 20/20 [00:03<00:00,  6.41it/s]\n",
      "Lin.Train Epoch: [34] Loss: 0.6314 : 100%|| 98/98 [00:09<00:00,  9.85it/s]\n",
      "Lin.Test Epoch: [34] Loss: 0.7027 ACC@1: 80.56% ACC@5: 98.51% : 100%|| 20/20 [00:03<00:00,  6.57it/s]\n",
      "Lin.Train Epoch: [35] Loss: 0.6263 : 100%|| 98/98 [00:09<00:00, 10.15it/s]\n",
      "Lin.Test Epoch: [35] Loss: 0.6988 ACC@1: 80.67% ACC@5: 98.48% : 100%|| 20/20 [00:03<00:00,  6.61it/s]\n",
      "Lin.Train Epoch: [36] Loss: 0.6218 : 100%|| 98/98 [00:09<00:00, 10.27it/s]\n",
      "Lin.Test Epoch: [36] Loss: 0.6941 ACC@1: 80.67% ACC@5: 98.55% : 100%|| 20/20 [00:03<00:00,  6.36it/s]\n",
      "Lin.Train Epoch: [37] Loss: 0.6174 : 100%|| 98/98 [00:09<00:00,  9.95it/s]\n",
      "Lin.Test Epoch: [37] Loss: 0.6902 ACC@1: 80.72% ACC@5: 98.49% : 100%|| 20/20 [00:03<00:00,  6.23it/s]\n",
      "Lin.Train Epoch: [38] Loss: 0.6131 : 100%|| 98/98 [00:10<00:00,  9.67it/s]\n",
      "Lin.Test Epoch: [38] Loss: 0.6862 ACC@1: 80.71% ACC@5: 98.58% : 100%|| 20/20 [00:03<00:00,  5.89it/s]\n",
      "Lin.Train Epoch: [39] Loss: 0.6091 : 100%|| 98/98 [00:09<00:00, 10.19it/s]\n",
      "Lin.Test Epoch: [39] Loss: 0.6827 ACC@1: 80.85% ACC@5: 98.59% : 100%|| 20/20 [00:03<00:00,  6.00it/s]\n",
      "Lin.Train Epoch: [40] Loss: 0.6054 : 100%|| 98/98 [00:09<00:00,  9.84it/s]\n",
      "Lin.Test Epoch: [40] Loss: 0.6792 ACC@1: 80.97% ACC@5: 98.58% : 100%|| 20/20 [00:03<00:00,  5.93it/s]\n",
      "Lin.Train Epoch: [41] Loss: 0.6018 : 100%|| 98/98 [00:10<00:00,  9.79it/s]\n",
      "Lin.Test Epoch: [41] Loss: 0.6761 ACC@1: 80.95% ACC@5: 98.61% : 100%|| 20/20 [00:03<00:00,  5.76it/s]\n",
      "Lin.Train Epoch: [42] Loss: 0.6130 :   3%|         | 3/98 [00:02<01:07,  1.40it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44370/2438117765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m test_loss, test_acc1, test_acc5, classifier = linear_evaluation(model, train_data_loaders_knn_all[0],\n\u001b[1;32m     12\u001b[0m                                                                     \u001b[0mtest_data_loaders_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlin_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                                                     lin_scheduler, epochs=lin_epoch, device=device) \n\u001b[0m",
      "\u001b[0;32m~/CI_SSL/utils/eval_metrics.py\u001b[0m in \u001b[0;36mlinear_evaluation\u001b[0;34m(net, data_loader, test_data_loader, train_optimizer, classifier, scheduler, epochs, device)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlinear_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mlinear_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_acc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_acc5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;31m# Testing for linear evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CI_SSL/utils/eval_metrics.py\u001b[0m in \u001b[0;36mlinear_train\u001b[0;34m(net, data_loader, train_optimizer, classifier, scheduler, epoch, device)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatchsize_bc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test Linear classification acc\n",
    "print(\"Starting Classifier Training..\")\n",
    "lin_epoch = 100\n",
    "if args.dataset == 'cifar10':\n",
    "    classifier = LinearClassifier(num_classes = 10).to(device)\n",
    "elif args.dataset == 'cifar100':\n",
    "    classifier = LinearClassifier(num_classes = 100).to(device)\n",
    "\n",
    "lin_optimizer = torch.optim.SGD(classifier.parameters(), 0.1, momentum=0.9) # Infomax: no weight decay, epoch 100, cosine scheduler\n",
    "lin_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(lin_optimizer, lin_epoch, eta_min=2e-4) #scheduler + values ref: infomax paper\n",
    "test_loss, test_acc1, test_acc5, classifier = linear_evaluation(model, train_data_loaders_knn_all[0],\n",
    "                                                                    test_data_loaders_all[0],lin_optimizer, classifier, \n",
    "                                                                    lin_scheduler, epochs=lin_epoch, device=device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedml_academic",
   "language": "python",
   "name": "fedml_academic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
